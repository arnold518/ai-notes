
<!doctype html>
<html lang="ko" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../1/">
      
      
        <link rel="next" href="../3/">
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.12">
    
    
      
        <title>2. Gradient Descent - Artificial Intelligence Notes</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.2afb09e1.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.06af60db.min.css">
      
      
  
  
    
    
  
    
    
  
  
  <style>:root{--md-admonition-icon--note:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2024%2024%22%3E%3Cpath%20d%3D%22M12%206a6%206%200%200%201%206%206c0%202.22-1.21%204.16-3%205.2V19a1%201%200%200%201-1%201h-4a1%201%200%200%201-1-1v-1.8c-1.79-1.04-3-2.98-3-5.2a6%206%200%200%201%206-6m2%2015v1a1%201%200%200%201-1%201h-2a1%201%200%200%201-1-1v-1zm6-10h3v2h-3zM1%2011h3v2H1zM13%201v3h-2V1zM4.92%203.5l2.13%202.14-1.42%201.41L3.5%204.93zm12.03%202.13%202.12-2.13%201.43%201.43-2.13%202.12z%22/%3E%3C/svg%3E');--md-admonition-icon--tip:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%206.7.2%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202024%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M256%200a256%20256%200%201%201%200%20512%20256%20256%200%201%201%200-512m-24%20120v136c0%208%204%2015.5%2010.7%2020l96%2064c11%207.4%2025.9%204.4%2033.3-6.7s4.4-25.9-6.7-33.3L280%20243.2V120c0-13.3-10.7-24-24-24s-24%2010.7-24%2024%22/%3E%3C/svg%3E');}</style>



    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:300,300i,400,400i,700,700i%7CUbuntu+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Noto Sans";--md-code-font:"Ubuntu Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="deep-orange" data-md-color-accent="deep-orange">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#2-gradient-descent" class="md-skip">
          콘텐츠로 이동
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="상단/헤더">
    <a href="../../../.." title="Artificial Intelligence Notes" class="md-header__button md-logo" aria-label="Artificial Intelligence Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Artificial Intelligence Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              2. Gradient Descent
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="deep-orange" data-md-color-accent="deep-orange"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="deep-orange" data-md-color-accent="deep-orange"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="검색" placeholder="검색" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="검색">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="공유" aria-label="공유" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="지우기" aria-label="지우기" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            검색 초기화
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/arnold518/ai-notes" title="저장소로 이동" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    arnold518/ai-notes
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="탭" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../" class="md-tabs__link">
          
  
  
    
  
  Books & Courses

        </a>
      </li>
    
  

    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="네비게이션" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="Artificial Intelligence Notes" class="md-nav__button md-logo" aria-label="Artificial Intelligence Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Artificial Intelligence Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/arnold518/ai-notes" title="저장소로 이동" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    arnold518/ai-notes
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Books & Courses
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Books & Courses
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" checked>
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Mathematical Foundations of Deep Neural Networks
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Mathematical Foundations of Deep Neural Networks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2_1_2" id="__nav_2_1_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Ch 1. Optimization and Stochastic Gradient Descent
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_1_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_1_2">
            <span class="md-nav__icon md-icon"></span>
            Ch 1. Optimization and Stochastic Gradient Descent
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Optimization Problem
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    2. Gradient Descent
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    2. Gradient Descent
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="목차">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      목차
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#gradient-descent" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Descent
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#convergence-of-gradient-descent" class="md-nav__link">
    <span class="md-ellipsis">
      Convergence of Gradient Descent
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stochastic-gradient-descent" class="md-nav__link">
    <span class="md-ellipsis">
      Stochastic Gradient Descent
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#variants-of-stochastic-gradient-descent" class="md-nav__link">
    <span class="md-ellipsis">
      Variants of Stochastic Gradient Descent
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://drive.google.com/file/d/1PSpoeseUPIptYQOPuQpxNawxtZbVR_K6/view?usp=sharing" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chapter 1 Code
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1_3" >
        
          
          <label class="md-nav__link" for="__nav_2_1_3" id="__nav_2_1_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Ch 2. Shallow Neural Networks to Multilayer Perceptrons
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_1_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1_3">
            <span class="md-nav__icon md-icon"></span>
            Ch 2. Shallow Neural Networks to Multilayer Perceptrons
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Shallow Neural Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../4/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Deep Neural Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://drive.google.com/file/d/15kXC3cJUV63gZNfXK6JdPPuSrwQZBzI8/view?usp=sharing" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chapter 2 Code
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1_4" >
        
          
          <label class="md-nav__link" for="__nav_2_1_4" id="__nav_2_1_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Ch 3. Convolutional Neural Networks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_1_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1_4">
            <span class="md-nav__icon md-icon"></span>
            Ch 3. Convolutional Neural Networks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../5/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Convolutional Neural Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../6/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Foundations of Design and Training of Deep Neural Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../7/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. ImageNet Challenge
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://drive.google.com/file/d/12O9fLasWDA-kOKBD-lE-1UhCl-PoDf0z/view?usp=sharing" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chapter 3 Code
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1_5" >
        
          
          <label class="md-nav__link" for="__nav_2_1_5" id="__nav_2_1_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Ch 4. CNNs for Other Supervised Learning Tasks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_1_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1_5">
            <span class="md-nav__icon md-icon"></span>
            Ch 4. CNNs for Other Supervised Learning Tasks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../8/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. CNNs for Other Supervised Learning Tasks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://drive.google.com/file/d/16IOKVF6IiDMyUvL73pGKBaEMKyXvezLB/view?usp=sharing" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chapter 4 Code
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1_6" >
        
          
          <label class="md-nav__link" for="__nav_2_1_6" id="__nav_2_1_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Ch 5. Unsupervised Learning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_1_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1_6">
            <span class="md-nav__icon md-icon"></span>
            Ch 5. Unsupervised Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../9/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    9. Autoencoder
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../10/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    10. Flow Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    11. Variational Autoencoders
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../12/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    12. Generative Adversarial Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://drive.google.com/file/d/1JM5k-e6LkhZ0vXRlfROWpiuw4YC85Jv1/view?usp=sharing" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chapter 5 Code
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1_7" >
        
          
          <label class="md-nav__link" for="__nav_2_1_7" id="__nav_2_1_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Ch A. Appendix - Basics of Monte Carlo
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_1_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1_7">
            <span class="md-nav__icon md-icon"></span>
            Ch A. Appendix - Basics of Monte Carlo
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../13/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    13. Basics of Monte Carlo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://drive.google.com/file/d/18b01xoORd0LFQmpfc_4ou5Rv44MY1neg/view?usp=sharing" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chapter A Code
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1_8" >
        
          
          <label class="md-nav__link" for="__nav_2_1_8" id="__nav_2_1_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Python Basics
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_1_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1_8">
            <span class="md-nav__icon md-icon"></span>
            Python Basics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://drive.google.com/file/d/1-iY9XfDhWNRDq3z_wVVvOzU04aRQWAfW/view?usp=drive_link" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Python Lecture 1
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://drive.google.com/file/d/1BANbCC6jBjPEUFSRJMFkcLbc4oVmoQHm/view?usp=sharing" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Python Lecture 2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://drive.google.com/file/d/1-Bc11RZmno6yx37kfVLjsyY-XKpLK5Je/view?usp=drive_link" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Python Lecture 3
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="목차">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      목차
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#gradient-descent" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Descent
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#convergence-of-gradient-descent" class="md-nav__link">
    <span class="md-ellipsis">
      Convergence of Gradient Descent
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stochastic-gradient-descent" class="md-nav__link">
    <span class="md-ellipsis">
      Stochastic Gradient Descent
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#variants-of-stochastic-gradient-descent" class="md-nav__link">
    <span class="md-ellipsis">
      Variants of Stochastic Gradient Descent
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="2-gradient-descent">§ 2. Gradient Descent<a class="headerlink" href="#2-gradient-descent" title="Permanent link">&para;</a></h1>
<h2 id="gradient-descent">Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permanent link">&para;</a></h2>
<div class="admonition definition">
<p class="admonition-title">Definition 2.1 <a id="definition-2-1"></a>: Gradient Descent</p>
<p>Consider the unconstrained optimization problem</p>
<div class="arithmatex">\[
\underset{\theta \in \mathbb{R}^{P}}{\operatorname{minimize}} f(\theta)
\]</div>
<p>where <span class="arithmatex">\(f\)</span> is differentiable.</p>
<p><strong>Gradient Descent (GD)</strong> algorithm:</p>
<div class="arithmatex">\[
\theta^{k+1}=\theta^{k}-\alpha_{k} \nabla f\left(\theta^{k}\right) \quad \text { for } k=0,1, \ldots,
\]</div>
<p>where <span class="arithmatex">\(\theta^{0} \in \mathbb{R}^{p}\)</span> is the <strong>initial point</strong> and <span class="arithmatex">\(\alpha_{k}&gt;0\)</span> is the <strong>learning rate</strong> or the <strong>stepsize</strong>.</p>
<p>The terminology learning rate is common in the machine learning literature while stepsize is more common in the optimization literature.</p>
</div>
<p>In math, a function is "differentiable" if its derivative exists everywhere.</p>
<p>In deep learning (DL), a function is often said to be differentiable if its derivative exists almost everywhere and the function is nice.
ReLU activation functions are said to be differentiable.</p>
<div class="admonition concept">
<p class="admonition-title">Concept 2.2 <a id="concept-2-2"></a>: Efficiency of gradient descent can be expected using the first-order Taylor expansion of <span class="arithmatex">\(f\)</span>.</p>
<div class="arithmatex">\[
\theta^{k+1}=\theta^{k}-\alpha_{k} \nabla f\left(\theta^{k}\right)
\]</div>
<p>Taylor expansion of <span class="arithmatex">\(f\)</span> about <span class="arithmatex">\(\theta^{k}\)</span> :</p>
<div class="arithmatex">\[
f(\theta)=f\left(\theta^{k}\right)+\nabla f\left(\theta^{k}\right)^{\top}\left(\theta-\theta^{k}\right)+\mathcal{O}\left(\left\|\theta-\theta^{k}\right\|^{2}\right)
\]</div>
<p>Plug in <span class="arithmatex">\(\theta^{k+1}\)</span> :</p>
<div class="arithmatex">\[
f\left(\theta^{k+1}\right)=f\left(\theta^{k}\right)-\alpha_{k}\left\|\nabla f\left(\theta^{k}\right)\right\|^{2}+\mathcal{O}\left(\alpha_{k}^{2}\right)
\]</div>
<p><span class="arithmatex">\(-\nabla f\left(\theta^{k}\right)\)</span> is steepest descent direction. For small (cautious) <span class="arithmatex">\(\alpha_{k}\)</span>, GD step reduces function value.</p>
</div>
<p>However, note that a step of GD need not result in descent, i.e., <span class="arithmatex">\(f\left(\theta^{k+1}\right)&gt;f\left(\theta^{k}\right)\)</span> is possible.</p>
<center>
![](.././assets/2.1.png){: width="40%"}
</center>

<p>We need an assumption that ensures the first-order Taylor expansion is a good approximation within a sufficiently large neighborhood.</p>
<h2 id="convergence-of-gradient-descent">Convergence of Gradient Descent<a class="headerlink" href="#convergence-of-gradient-descent" title="Permanent link">&para;</a></h2>
<p>Without further assumptions, there is no hope of finding the global minimum.</p>
<p>We cannot prove the function value converges to global optimum. We instead prove <span class="arithmatex">\(\nabla f\left(\theta^{k}\right) \rightarrow 0\)</span>. Roughly speaking, this is similar, but weaker than proving that <span class="arithmatex">\(\theta^{k}\)</span> converges to a local minimum.</p>
<center>
![](.././assets/2.2.png){: width="70%"}
</center>

<p>Without further assumptions, we cannot show that <span class="arithmatex">\(\theta^{k}\)</span> converges to a limit, and even <span class="arithmatex">\(\theta^{k}\)</span> does converge to a limit, we cannot guarantee that that limit is not a saddle point or even a local maximum. Nevertheless, people commonly use the argument that <span class="arithmatex">\(\theta^{k}\)</span> usually converges and that it is unlikely that the limit is a local maximum or a saddle point.</p>
<div class="admonition definition">
<p class="admonition-title">Definition 2.3 <a id="definition-2-3"></a>: <span class="arithmatex">\(L\)</span>-Lipschitz</p>
<p>We say <span class="arithmatex">\(\nabla f: \mathbb{R}^{p} \rightarrow \mathbb{R}^{p}\)</span> is <span class="arithmatex">\(L\)</span>-Lipschitz if</p>
<div class="arithmatex">\[
\|\nabla f(x)-\nabla f(y)\| \leq L\|x-y\| \quad \forall x, y \in \mathbb{R}^{p} .
\]</div>
<p>Roughly, this means <span class="arithmatex">\(\nabla f\)</span> does not change rapidly. As a consequence, we can trust the first-order Taylor expansion on a non-infinitesimal neighborhood.</p>
</div>
<div class="admonition theorem">
<p class="admonition-title">Theorem 2.4 <a id="theorem-2-4"></a>: Lipschitz Gradient Lemma</p>
<p>Let <span class="arithmatex">\(f: \mathbb{R}^{p} \rightarrow \mathbb{R}\)</span> be differentiable and <span class="arithmatex">\(\nabla f: \mathbb{R}^{p} \rightarrow \mathbb{R}^{p}\)</span> be L-Lipschitz. Then</p>
<div class="arithmatex">\[
f(\theta+\delta) \leq f(\theta)+\nabla f(\theta)^{\top} \delta+\frac{L}{2}\|\delta\|^{2} \quad \forall \theta, \delta \in \mathbb{R}^{p}
\]</div>
<hr />
<p><span class="arithmatex">\(f(\theta)+\nabla f(\theta)^{\top} \delta-\frac{L}{2}\|\delta\|^{2} \leq f(\theta+\delta)\)</span> is also true, but we do not need this other direction. Together the inequalities imply</p>
<div class="arithmatex">\[
\left|f(\theta+\delta)-\left(f(\theta)+\nabla f(\theta)^{\top} \delta\right)\right| \leq \frac{L}{2}\|\delta\|^{2} \quad \forall \theta, \delta \in \mathbb{R}^{p}
\]</div>
<div class="admonition proof">
<p class="admonition-title">Proof</p>
<p>Define <span class="arithmatex">\(g: \mathbb{R} \rightarrow \mathbb{R}\)</span> as <span class="arithmatex">\(g(t)=f(\theta+t \delta)\)</span>. Then <span class="arithmatex">\(g\)</span> is differentiable and</p>
<div class="arithmatex">\[
g^{\prime}(t)=\nabla f(\theta+t \delta)^{\top} \delta
\]</div>
<p>Note <span class="arithmatex">\(g^{\prime}\)</span> is <span class="arithmatex">\(\left(L\|\delta\|^{2}\right)\)</span>-Lipschitz continuous since</p>
<div class="arithmatex">\[
\begin{gathered}
\left|g^{\prime}\left(t_{1}\right)-g^{\prime}\left(t_{0}\right)\right|=\left|\left(\nabla f\left(\theta+t_{1} \delta\right)-\nabla f\left(\theta+t_{0} \delta\right)\right)^{\top} \delta\right| \\
\leq\left\|\nabla f\left(\theta+t_{1} \delta\right)-\nabla f\left(\theta+t_{0} \delta\right)\right\|\| \| \delta \| \\
\leq L\left\|t_{1} \delta-t_{0} \delta\right\|\|\delta\| \\
=L\|\delta\|^{2}\left|t_{1}-t_{0}\right|
\end{gathered}
\]</div>
<p>Finally, we conclude with</p>
<div class="arithmatex">\[
\begin{gathered}
f(\theta+\delta)=g(1)=g(0)+\int_{0}^{1} g^{\prime}(t) \mathrm{d} t \\
\leq f(\theta)+\int_{0}^{1}\left(g^{\prime}(0)+L\|\delta\|^{2} t\right) \mathrm{d} t \\
=f(\theta)+\nabla f(\theta)^{\top} \delta+\frac{L}{2}\|\delta\|^{2}
\end{gathered}
\]</div>
</div>
</div>
<div class="admonition theorem">
<p class="admonition-title">Theorem 2.5 <a id="theorem-2-5"></a>: Summability Lemma</p>
<p>Let <span class="arithmatex">\(V^{0}, V^{1}, \ldots \in \mathbb{R}\)</span> and <span class="arithmatex">\(S^{0}, S^{1}, \ldots \in \mathbb{R}\)</span> be nonnegative sequences satisfying</p>
<div class="arithmatex">\[
V^{k+1} \leq V^{k}-S^{k}
\]</div>
<p>for <span class="arithmatex">\(k=0,1,2, \ldots\)</span> Then <span class="arithmatex">\(S^{k} \rightarrow 0\)</span>.</p>
<div class="admonition proof">
<p class="admonition-title">Proof</p>
<p>Key idea. <span class="arithmatex">\(S^{k}\)</span> measures progress (decrease) made in iteration <span class="arithmatex">\(k\)</span>. Since <span class="arithmatex">\(V^{k} \geq 0, V^{k}\)</span> cannot decrease forever, so the progress (magnitude of <span class="arithmatex">\(S^{k}\)</span> ) must diminish to 0.</p>
<p>Sum the inequality from <span class="arithmatex">\(i=0\)</span> to <span class="arithmatex">\(k\)</span></p>
<div class="arithmatex">\[
V^{k+1}+\sum_{i=0}^{k} S^{i} \leq V^{0}
\]</div>
<p>Let <span class="arithmatex">\(k \rightarrow \infty\)</span></p>
<div class="arithmatex">\[
\sum_{i=0}^{\infty} S^{i} \leq V^{0}-\lim _{k \rightarrow \infty} V^{k} \leq V^{0}
\]</div>
<p>Since <span class="arithmatex">\(\sum_{i=0}^{\infty} S^{i}&lt;\infty, S^{i} \rightarrow 0\)</span>.</p>
</div>
</div>
<div class="admonition theorem">
<p class="admonition-title">Theorem 2.6 <a id="theorem-2-6"></a>: Convergence of GD</p>
<p>Assume <span class="arithmatex">\(f: \mathbb{R}^{p} \rightarrow \mathbb{R}\)</span> is differentiable, <span class="arithmatex">\(\nabla f\)</span> is <span class="arithmatex">\(L\)</span>-Lipschitz continuous, and <span class="arithmatex">\(\inf _{\theta \in \mathbb{R}^{p}} f(\theta)&gt;-\infty\)</span>. Then</p>
<div class="arithmatex">\[
\theta^{k+1}=\theta^{k}-\alpha \nabla f\left(\theta^{k}\right)
\]</div>
<p>with <span class="arithmatex">\(\alpha \in\left(0, \frac{2}{L}\right)\)</span> satisfies <span class="arithmatex">\(\nabla f\left(\theta^{k}\right) \rightarrow 0\)</span>.</p>
<div class="admonition proof">
<p class="admonition-title">Proof</p>
<p>Use Lipschitz gradient lemma with <span class="arithmatex">\(\theta=\theta^{k}\)</span> and <span class="arithmatex">\(\delta=-\alpha \nabla f\left(\theta^{k}\right)\)</span> to get</p>
<div class="arithmatex">\[
f\left(\theta^{k+1}\right) \leq f\left(\theta^{k}\right)-\alpha\left(1-\frac{\alpha L}{2}\right)\left\|\nabla f\left(\theta^{k}\right)\right\|^{2}
\]</div>
<p>and</p>
<div class="arithmatex">\[
\left(f\left(\theta^{k+1}\right)-\inf _{\theta} f(\theta)\right) \leq\left(f\left(\theta^{k}\right)-\inf _{\theta} f(\theta)\right)-\alpha\left(1-\frac{\alpha L}{2}\right)\left\|\nabla f\left(\theta^{k}\right)\right\|^{2} \\
\]</div>
<div class="arithmatex">\[
\left(f\left(\theta^{k+1}\right)-\inf _{\theta} f(\theta)\right) \ge 0, 
\left(f\left(\theta^{k}\right)-\inf _{\theta} f(\theta)\right) \ge 0, 
\alpha\left(1-\frac{\alpha L}{2}\right)\left\|\nabla f\left(\theta^{k}\right)\right\|^{2} &gt; 0 \text{ for } \alpha \in\left(0, \frac{2}{L}\right)
\]</div>
<p>By the summability lemma, <span class="arithmatex">\(\left\|\nabla f\left(\theta^{k}\right)\right\|^{2} \rightarrow 0\)</span> and thus <span class="arithmatex">\(\nabla f\left(\theta^{k}\right) \rightarrow 0\)</span>.</p>
</div>
</div>
<p>In deep learning, the condition that <span class="arithmatex">\(\nabla f\)</span> is <span class="arithmatex">\(L\)</span>-Lipschitz is usually not true (due to the use of ReLU activation functions).</p>
<p>Rather, the purpose of these mathematical analyses is to obtain qualitative insights; this convergence proof are meant to provide you with intuition on the training dynamics of GD and SGD.</p>
<p>Because analyzing deep learning systems as is rigorously is usually difficult, people usually analyze modified (simplified) setups rigorously or analyze the full setup heuristically.</p>
<p>In both cases, the goal is to obtain qualitative insights, rather than theoretical guarantees.</p>
<h2 id="stochastic-gradient-descent">Stochastic Gradient Descent<a class="headerlink" href="#stochastic-gradient-descent" title="Permanent link">&para;</a></h2>
<div class="admonition definition">
<p class="admonition-title">Definition 2.7 <a id="definition-2-7"></a>: Finite-Sum Optimization Problem</p>
<p>A <strong>finite-sum optimization problem</strong> has the structure</p>
<div class="arithmatex">\[
\underset{\theta \in \mathbb{R}^{p}}{\operatorname{minimize}} \frac{1}{N} \sum_{i=1}^{N} f_{i}(\theta):=F(\theta)
\]</div>
<p>Finite-sum is ubiquitous in ML. <span class="arithmatex">\(N\)</span> usually corresponds to the number of data points.</p>
</div>
<p>In finite-sum problem, using GD</p>
<div class="arithmatex">\[
\theta^{k+1}=\theta^{k}-\frac{\alpha_{k}}{N} \sum_{i=1}^{N} \nabla f_{i}\left(\theta^{k}\right)
\]</div>
<p>is impractical when <span class="arithmatex">\(N\)</span> is large since <span class="arithmatex">\(\frac{1}{N} \sum_{i=1}^{N} \nabla f_{i}\left(\theta^{k}\right)\)</span> takes too long to compute.</p>
<div class="admonition concept">
<p class="admonition-title">Concept 2.8 <a id="concept-2-8"></a>: Finite-sum problem can be reformulated with expectation.</p>
<p>Although the finite-sum optimization problem has no inherent randomness, we can reformulate this problem with randomness:</p>
<div class="arithmatex">\[
\operatorname{minimize}_{\theta \in \mathbb{R}^{p}} \quad \mathbb{E}_{I}\left[f_{I}(\theta)\right]
\]</div>
<p>where <span class="arithmatex">\(I \sim\)</span> Uniform <span class="arithmatex">\(\{1, \ldots, N\}\)</span>. To see the equivalence,</p>
<div class="arithmatex">\[
\mathbb{E}_{I}\left[f_{I}(\theta)\right]=\sum_{i=1}^{N} f_{i}(\theta) \mathbb{P}(I=i)=\frac{1}{N} \sum_{i=1}^{N} f_{i}(\theta)=F(\theta)
\]</div>
</div>
<div class="admonition definition">
<p class="admonition-title">Definition 2.9 <a id="definition-2-9"></a>: Stochastic Gradient Descent</p>
<p><strong>Stochastic gradient descent (SGD)</strong></p>
<div class="arithmatex">\[
\begin{gathered}
i(k) \sim \operatorname{Uniform}\{1, \ldots, N\} \\
\theta^{k+1}=\theta^{k}-\alpha_{k} \nabla f_{i(k)}\left(\theta^{k}\right)
\end{gathered}
\]</div>
<p>for <span class="arithmatex">\(k=0,1, \ldots\)</span>, where <span class="arithmatex">\(\theta^{0} \in \mathbb{R}^{p}\)</span> is the <strong>initial point</strong> and <span class="arithmatex">\(\alpha_{k}&gt;0\)</span> is the <strong>learning rate</strong>.</p>
<p><span class="arithmatex">\(\nabla f_{i(k)}\left(\theta^{k}\right)\)</span> is a stochastic gradient of <span class="arithmatex">\(F\)</span> at <span class="arithmatex">\(\theta^{k}\)</span>, i.e.,</p>
<div class="arithmatex">\[
\mathbb{E}\left[\nabla f_{i(k)}\left(\theta^{k}\right)\right]=\nabla \mathbb{E}\left[f_{i(k)}\left(\theta^{k}\right)\right]=\nabla F\left(\theta^{k}\right)
\]</div>
</div>
<div class="admonition concept">
<p class="admonition-title">Concept 2.10 <a id="concept-2-10"></a>: SGD is more efficient than GD.</p>
<p>GD uses all indices <span class="arithmatex">\(i=1, \ldots, N\)</span> every iteration</p>
<div class="arithmatex">\[
\theta^{k+1}=\theta^{k}-\frac{\alpha_{k}}{N} \sum_{i=1}^{N} \nabla f_{i}\left(\theta^{k}\right)
\]</div>
<p>SGD uses only a single random index <span class="arithmatex">\(i(k)\)</span> every iteration</p>
<div class="arithmatex">\[
\begin{gathered}
i(k) \sim \text { Uniform }\{1, \ldots, N\} \\
\theta^{k+1}=\theta^{k}-\alpha_{k} \nabla f_{i(k)}\left(\theta^{k}\right)
\end{gathered}
\]</div>
<p>When size of the data <span class="arithmatex">\(N\)</span> is large, SGD is often more effective than GD.</p>
</div>
<div class="admonition concept">
<p class="admonition-title">Concept 2.11 <a id="concept-2-11"></a>: Efficiency of stochastic gradient descent can be expected using the first-order Taylor expansion of <span class="arithmatex">\(F\)</span>.</p>
<p>Plug <span class="arithmatex">\(\theta^{k+1}\)</span> into Taylor expansion of <span class="arithmatex">\(F\)</span> about <span class="arithmatex">\(\theta^{k}\)</span> :</p>
<div class="arithmatex">\[
F\left(\theta^{k+1}\right)=F\left(\theta^{k}\right)-\alpha_{k} \nabla F\left(\theta^{k}\right)^{\top} \nabla f_{i(k)}\left(\theta^{k}\right)+\mathcal{O}\left(\alpha_{k}^{2}\right)
\]</div>
<p>Take expectation on both sides:</p>
<div class="arithmatex">\[
\mathbb{E}_{k}\left[F\left(\theta^{k+1}\right)\right]=F\left(\theta^{k}\right)-\alpha_{k}\left\|\nabla F\left(\theta^{k}\right)\right\|^{2}+\mathcal{O}\left(\alpha_{k}^{2}\right)
\]</div>
<p>( <span class="arithmatex">\(\mathbb{E}_{k}\)</span> is expectation conditioned on <span class="arithmatex">\(\theta^{k}\)</span> )</p>
<p><span class="arithmatex">\(-\nabla f_{i(k)}\left(\theta^{k}\right)\)</span> is descent direction in expectation. For small (cautious) <span class="arithmatex">\(\alpha_{k}\)</span>, SGD step reduces function value in expectation.</p>
</div>
<h2 id="variants-of-stochastic-gradient-descent">Variants of Stochastic Gradient Descent<a class="headerlink" href="#variants-of-stochastic-gradient-descent" title="Permanent link">&para;</a></h2>
<p>Consider</p>
<div class="arithmatex">\[
\underset{\theta \in \mathbb{R}^{p}}{\operatorname{minimize}} \frac{1}{N} \sum_{i=1}^{N} f_{i}(\theta)
\]</div>
<p>SGD can be generalized to</p>
<div class="arithmatex">\[
\theta^{k+1}=\theta^{k}-\alpha_{k} g^{k}
\]</div>
<p>where <span class="arithmatex">\(g^{k}\)</span> is a stochastic gradient. The choice <span class="arithmatex">\(g^{k}=\nabla f_{i(k)}\left(\theta^{k}\right)\)</span> is just one option.</p>
<div class="admonition theorem">
<p class="admonition-title">Theorem 2.12 <a id="theorem-2-12"></a>: Sampling with Replacement Lemma</p>
<p>Let <span class="arithmatex">\(X_{1}, \ldots, X_{N} \in \mathbb{R}^{p}\)</span> be given (non-random) vectors. Let <span class="arithmatex">\(\frac{1}{N} \sum_{i=1}^{N} X_{i}=\mu\)</span>. Let <span class="arithmatex">\(i(1), \ldots, i(B) \subseteq\{1, \ldots, N\}\)</span> be random indices. Then</p>
<div class="arithmatex">\[
\mathbb{E} \frac{1}{B} \sum_{b=1}^{B} X_{i(b)}=\mu
\]</div>
<div class="admonition proof">
<p class="admonition-title">Proof</p>
<div class="arithmatex">\[
\mathbb{E} \frac{1}{B} \sum_{b=1}^{B} X_{i(b)}=\frac{1}{B} \sum_{b=1}^{B} \mathbb{E} X_{i(b)}=\frac{1}{B} \sum_{b=1}^{B} \mu=\mu
\]</div>
</div>
</div>
<div class="admonition definition">
<p class="admonition-title">Definition 2.13 <a id="definition-2-13"></a>: Minibatch SGD with Replacement</p>
<p><strong>Minibatch SGD with replacement</strong></p>
<div class="arithmatex">\[
\begin{gathered}
i(k, 1), \ldots, i(k, B) \sim \text { Uniform }\{1, \ldots, N\} \\
\theta^{k+1}=\theta^{k}-\frac{\alpha_{k}}{B} \sum_{b=1}^{B} \nabla f_{i(k, b)}\left(\theta^{k}\right)
\end{gathered}
\]</div>
<p>To clarify, we sample <span class="arithmatex">\(B\)</span> out of <span class="arithmatex">\(N\)</span> indices with replacement, i.e., the same index can be sampled multiple times.</p>
<p>By <a href="./#theorem-2-12">Theorem 2.12</a>, <span class="arithmatex">\(\frac{1}{B} \sum_{b=1}^{B} \nabla f_{i(k, b)}\left(\theta^{k}\right)\)</span> is a stochastic gradient of <span class="arithmatex">\(F\)</span> at <span class="arithmatex">\(\theta^{k}\)</span>.</p>
</div>
<div class="admonition theorem">
<p class="admonition-title">Theorem 2.14 <a id="theorem-2-14"></a>: Sampling without Replacement Lemma</p>
<p>Let <span class="arithmatex">\(X_{1}, \ldots, X_{N} \in \mathbb{R}^{p}\)</span> be given (non-random) vectors. Let <span class="arithmatex">\(\frac{1}{N} \sum_{i=1}^{N} X_{i}=\mu\)</span>. Let <span class="arithmatex">\(\sigma\)</span> be a random permutation. Then</p>
<div class="arithmatex">\[
\mathbb{E} \frac{1}{B} \sum_{b=1}^{B} X_{\sigma(b)}=\mu
\]</div>
<div class="admonition proof">
<p class="admonition-title">Proof</p>
<div class="arithmatex">\[
\mathbb{E} \frac{1}{B} \sum_{b=1}^{B} X_{\sigma(b)}=\frac{1}{B} \sum_{b=1}^{B} \mathbb{E} X_{\sigma(b)}=\frac{1}{B} \sum_{b=1}^{B} \mu=\mu
\]</div>
</div>
</div>
<div class="admonition definition">
<p class="admonition-title">Definition 2.15 <a id="definition-2-15"></a>: Minibatch SGD without Replacement</p>
<p><strong>Minibatch SGD without replacement</strong></p>
<div class="arithmatex">\[
\begin{gathered}
\sigma^{k} \sim \operatorname{permutation}(N) \\
\theta^{k+1}=\theta^{k}-\frac{\alpha_{k}}{B} \sum_{b=1}^{B} \nabla f_{\sigma^{k}(b)}\left(\theta^{k}\right)
\end{gathered}
\]</div>
<p>We assume <span class="arithmatex">\(B \leq N\)</span>. To clarify, we sample <span class="arithmatex">\(B\)</span> out of <span class="arithmatex">\(N\)</span> indices without replacement, i.e., the same index cannot be sampled multiple times.</p>
<p>By <a href="./#theorem-2-14">Theorem 2.14</a>, <span class="arithmatex">\(\frac{1}{B} \sum_{b=1}^{B} \nabla f_{\sigma^{k}(b)}\left(\theta^{k}\right)\)</span> is a stochastic gradient of <span class="arithmatex">\(F\)</span> at <span class="arithmatex">\(\theta^{k}\)</span>.</p>
</div>
<div class="admonition concept">
<p class="admonition-title">Concept 2.16 <a id="concept-2-16"></a>: How to choose batch size <span class="arithmatex">\(B\)</span>?</p>
<p>Note <span class="arithmatex">\(B=1\)</span> minibatch SGD becomes SGD.</p>
<p>Mathematically (measuring performance per iteration)</p>
<ul>
<li>Use large batch is when noise/randomness is large.</li>
<li>Use small batch is when noise/randomness is small.</li>
</ul>
<p>Practically (measuring performance per unit time)</p>
<ul>
<li>Large batch allows more efficient computation on GPUs.</li>
<li>Often best to increase batch size up to the GPU memory limit.</li>
</ul>
</div>
<p>In DL, SGD is applied to nice continuous but non-differentiable functions that are differentiable almost everywhere.</p>
<p>In this case, if we choose <span class="arithmatex">\(\theta^{0} \in \mathbb{R}^{n}\)</span> randomly and run</p>
<div class="arithmatex">\[
\theta^{k+1}=\theta^{k}-\alpha_{k} \nabla f\left(\theta^{k}\right)
\]</div>
<p>the algorithm is usually well-defined, i.e., <span class="arithmatex">\(\theta^{k}\)</span> never hits a point of non-differentiability.</p>
<p>With a proof or not, GD and SGD are applied to non-differentiable minimization in ML. The absence of differentiability does not seem to cause serious problems.</p>
<div class="admonition definition">
<p class="admonition-title">Definition 2.17 <a id="definition-2-17"></a>: Cyclic SGD</p>
<p>Consider the sequence of indices</p>
<div class="arithmatex">\[
\{\bmod (k, N)+1\}_{k=0,1, \ldots}=1,2, \ldots, N, 1,2, \ldots, N, \ldots
\]</div>
<p>Here, <span class="arithmatex">\(\bmod (k, N)\)</span> is the remainder of <span class="arithmatex">\(k\)</span> when divided by <span class="arithmatex">\(N\)</span>.</p>
<p><strong>Cyclic SGD</strong>:</p>
<div class="arithmatex">\[
\theta^{k+1}=\theta^{k}-\alpha_{\mathbf{k}} \nabla f_{\bmod (k, N)+1}\left(\theta^{k}\right)
\]</div>
<p>To clarify, this samples the indices in a (deterministic) cyclic order.</p>
</div>
<div class="admonition concept">
<p class="admonition-title">Concept 2.18 <a id="concept-2-18"></a>: Pros and Cons of Cyclic SGD</p>
<p>Strictly speaking, cyclic SGD is not an instance of SGD as unbiased estimation property lost.</p>
<p>Advantage:</p>
<ul>
<li>Uses all indices (data) every <span class="arithmatex">\(N\)</span> iterations.</li>
</ul>
<p>Disadvantage:</p>
<ul>
<li>Worse than SGD in some cases, theoretically and empirically.</li>
<li>In DL, neural networks can learn to anticipate cyclic order.</li>
</ul>
</div>
<div class="admonition definition">
<p class="admonition-title">Definition 2.19 <a id="definition-2-19"></a>: Shuffled Cyclic SGD</p>
<p><strong>Shuffled Cyclic SGD</strong>:</p>
<div class="arithmatex">\[
\theta^{k+1}=\theta^{k}-\alpha_{k} \nabla f_{\sigma^{\left \lfloor \frac{k}{N} \right \rfloor} (\bmod (k, N)+1)}\left(\theta^{k}\right)
\]</div>
<p>where <span class="arithmatex">\(\sigma^{0}, \sigma^{1}, \ldots\)</span> is a sequence of random permutations, i.e., we shuffle the order every cycle.</p>
</div>
<div class="admonition concept">
<p class="admonition-title">Concept 2.19 <a id="concept-2-19"></a>: Pros and Cons of Shuffled Cyclic SGD</p>
<p>Again, strictly speaking, shuffled cyclic SGD is not an instance of SGD as unbiased estimation property lost.</p>
<p>Advantages :</p>
<ul>
<li>Uses all indices (data) every <span class="arithmatex">\(N\)</span> iterations.</li>
<li>Neural network cannot learn to anticipate data order.</li>
<li>Empirically best performance.</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>Theory not as strong as regular SGD.</li>
</ul>
</div>
<div class="admonition concept">
<p class="admonition-title">Concept 2.20 <a id="concept-2-20"></a>: Which variant of SGD to use?</p>
<p>Theoretical comparison of SGD variants:</p>
<ul>
<li>Not that easy.</li>
<li>Result does not strongly correlate with practical performance in DL.</li>
</ul>
<p>In DL, the most common choice is</p>
<ul>
<li>shuffled cyclic minibatch SGD (without replacement) and</li>
<li>batchsize <span class="arithmatex">\(B\)</span> is as large as possible within the GPU memory limit.</li>
</ul>
<p>One can generally consider this to be the default option.</p>
</div>
<div class="admonition definition">
<p class="admonition-title">Definition 2.21 <a id="definition-2-21"></a>: Epoch in finite-sum optimization and machine learning training</p>
<p>An <strong>epoch</strong> is loosely defined as the unit of optimization or training progress of processing all indices or data once.</p>
<ul>
<li>1 iteration of GD constitutes an epoch.</li>
<li><span class="arithmatex">\(N\)</span> iterations of SGD, cyclic SGD, or shuffled cyclic SGD constitute an epoch.</li>
<li><span class="arithmatex">\(N / B\)</span> iterations of minibatch SGD constitute an epoch.</li>
</ul>
<p>Epoch is often a convenient unit for counting iterations compared to directly counting the iteration number.</p>
</div>
<div class="admonition concept">
<p class="admonition-title">Concept 2.22 <a id="concept-2-22"></a>: SGD with General Expectation</p>
<p>Consider an optimization problem with its objective defined with a general expectation</p>
<div class="arithmatex">\[
\operatorname{minimize}_{\theta \in \mathbb{R}^{p}} \quad \mathbb{E}_{\omega}\left[f_{\omega}(\theta)\right]:=F(\theta)
\]</div>
<p>Here, <span class="arithmatex">\(\omega\)</span> is a random variable. We will encounter these expectations (non-finite sum) when we talk about generative models.</p>
<p>For this setup, the SGD algorithm is</p>
<div class="arithmatex">\[
\theta^{k+1}=\theta^{k}-\alpha_{k} \nabla f_{\omega^{k}}\left(\theta^{k}\right)
\]</div>
<p>where <span class="arithmatex">\(\omega^{0}, \omega^{1}, \ldots\)</span> are IID random samples of <span class="arithmatex">\(\omega\)</span>. If <span class="arithmatex">\(\nabla_{\theta} \mathbb{E}_{\omega}\left[f_{\omega}(\theta)\right]=\mathbb{E}_{\omega}\left[\nabla_{\theta} f_{\omega}(\theta)\right]\)</span>, then <span class="arithmatex">\(\nabla f_{\omega^{k}}\left(\theta^{k}\right)\)</span> is a stochastic gradient of <span class="arithmatex">\(F(\theta)\)</span> at <span class="arithmatex">\(\theta^{k}\)</span>. (Make sure you understand why the previous SGD for the finite-sum setup is a special case of this.)</p>
<p>GD for this setup is</p>
<div class="arithmatex">\[
\theta^{k+1}=\theta^{k}-\alpha_{k} \mathbb{E}_{\omega}\left[\nabla_{\theta} f_{\omega}\left(\theta^{k}\right)\right]
\]</div>
<p>However, if the expectation is difficult to compute GD is impractical and SGD is preferred.</p>
</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  맨위로
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/arnold518" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../../..", "features": ["content.code.copy", "header.autohide", "navigation.instant", "navigation.tracking", "navigation.tabs", "toc.follow", "navigation.top", "search.suggest", "search.highlight", "search.share", "navigation.indexes"], "search": "../../../../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "\ud074\ub9bd\ubcf4\ub4dc\uc5d0 \ubcf5\uc0ac\ub428", "clipboard.copy": "\ud074\ub9bd\ubcf4\ub4dc\ub85c \ubcf5\uc0ac", "search.result.more.one": "\uc774 \ubb38\uc11c\uc5d0\uc11c 1\uac1c\uc758 \uac80\uc0c9 \uacb0\uacfc \ub354 \ubcf4\uae30", "search.result.more.other": "\uc774 \ubb38\uc11c\uc5d0\uc11c #\uac1c\uc758 \uac80\uc0c9 \uacb0\uacfc \ub354 \ubcf4\uae30", "search.result.none": "\uac80\uc0c9\uc5b4\uc640 \uc77c\uce58\ud558\ub294 \ubb38\uc11c\uac00 \uc5c6\uc2b5\ub2c8\ub2e4", "search.result.one": "1\uac1c\uc758 \uc77c\uce58\ud558\ub294 \ubb38\uc11c", "search.result.other": "#\uac1c\uc758 \uc77c\uce58\ud558\ub294 \ubb38\uc11c", "search.result.placeholder": "\uac80\uc0c9\uc5b4\ub97c \uc785\ub825\ud558\uc138\uc694", "search.result.term.missing": "\ud3ec\ud568\ub418\uc9c0 \uc54a\uc740 \uac80\uc0c9\uc5b4", "select.version": "\ubc84\uc804 \uc120\ud0dd"}, "version": null}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="../../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>