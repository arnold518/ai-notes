{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":[" "]},"docs":[{"location":"","title":"Home","text":""},{"location":"tags/","title":"Tags","text":""},{"location":"books-and-courses/mfdnn/","title":"Mathematical Foundations of Deep Neural Networks","text":""},{"location":"books-and-courses/mfdnn/#course-information","title":"Course Information","text":"<p>Mathematical Foundations of Deep Neural Networks, Spring 2024 (Ernest K. Ryu)</p>"},{"location":"books-and-courses/mfdnn/#table-of-contents","title":"Table of Contents","text":""},{"location":"books-and-courses/mfdnn/#ch-1-optimization-and-stochastic-gradient-descent","title":"Ch 1. Optimization and Stochastic Gradient Descent","text":"<ul> <li>1. Optimization Problem</li> <li>2. Gradient Descent</li> <li>Chapter 1 Code</li> </ul>"},{"location":"books-and-courses/mfdnn/#ch-2-shallow-neural-networks-to-multilayer-perceptrons","title":"Ch 2. Shallow Neural Networks to Multilayer Perceptrons","text":"<ul> <li>3. Shallow Neural Networks</li> <li>4. Deep Neural Networks</li> <li>Chapter 2 Code</li> </ul>"},{"location":"books-and-courses/mfdnn/#ch-3-convolutional-neural-networks","title":"Ch 3. Convolutional Neural Networks","text":"<ul> <li>5. Convolutional Neural Networks</li> <li>6. Foundations of Design and Training of Deep Neural Networks</li> <li>7. ImageNet Challenge</li> <li>Chapter 3 Code</li> </ul>"},{"location":"books-and-courses/mfdnn/#ch-4-cnns-for-other-supervised-learning-tasks","title":"Ch 4. CNNs for Other Supervised Learning Tasks","text":"<ul> <li>8. CNNs for Other Supervised Learning Tasks</li> <li>Chapter 4 Code</li> </ul>"},{"location":"books-and-courses/mfdnn/#ch-5-unsupervised-learning","title":"Ch 5. Unsupervised Learning","text":"<ul> <li>9. Autoencoder</li> <li>10. Flow Models</li> <li>11. Variational Autoencoders</li> <li>12. Generative Adversarial Networks</li> <li>Chapter 5 Code</li> </ul>"},{"location":"books-and-courses/mfdnn/#ch-a-appendix-basics-of-monte-carlo","title":"Ch A. Appendix - Basics of Monte Carlo","text":"<ul> <li>13. Basics of Monte Carlo</li> <li>Chapter A Code</li> </ul>"},{"location":"books-and-courses/mfdnn/#python-basics","title":"Python Basics","text":"<ul> <li>Python Lecture 1</li> <li>Python Lecture 2</li> <li>Python Lecture 3</li> </ul>"},{"location":"books-and-courses/mfdnn/#materials","title":"Materials","text":"<ul> <li>Latex PDF</li> </ul>"},{"location":"books-and-courses/mfdnn/1/","title":"\u00a7 1. Optimization Problem","text":""},{"location":"books-and-courses/mfdnn/1/#definition-of-optimization-problem","title":"Definition of Optimization Problem","text":"<p>Definition 1.1 : Optimization Problem</p> <p>In an optimization problem, we minimize or maximize a function value, possibly subject to constraints.</p> \\[ \\begin{array}{ll} \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} &amp; f(\\theta) \\\\ \\text { subject to } &amp; h_{1}(\\theta)=0, &amp; \\cdots &amp; , \\ h_{m}(\\theta)=0, \\\\ &amp; g_{1}(\\theta) \\le 0, &amp; \\cdots &amp; , \\ g_{n}(\\theta) \\le 0 \\end{array} \\] <ul> <li>Decision variable: \\(\\theta\\)</li> <li>Objective function: \\(f\\)</li> <li>Equality constraint: \\(h_{i}(\\theta)=0\\) for \\(i=1, \\ldots, m\\)</li> <li>Inequality constraint: \\(g_{j}(\\theta) \\leq 0\\) for \\(j=1, \\ldots, n\\)</li> </ul> <p>In machine learning (ML), we often minimize a \"loss\", but sometimes we maximize the \"likelihood\". In any case, minimization and maximization are equivalent since</p> \\[ \\text { maximize } f(\\theta) \\quad \\Leftrightarrow \\quad \\text { minimize }-f(\\theta). \\] <p>Definition 1.2 : Feasible Point and Constraints</p> <p>\\(\\theta \\in \\mathbb{R}^{p}\\) is a feasible point if it satisfies all constraints:</p> \\[ \\begin{array}{cc} h_{1}(\\theta)=0 &amp; g_{1}(\\theta) \\leq 0 \\\\ \\vdots &amp; \\vdots \\\\ h_{m}(\\theta)=0 &amp; g_{n}(\\theta) \\leq 0 \\end{array} \\] <p>Optimization problem is infeasible if there is no feasible point.</p> <p>An optimization problem with no constraint is called an unconstrained optimization problem. Optimization problems with constraints is called a constrained optimization problem.</p> <p>Definition 1.3 : Optimal Value and Solution</p> <p>Optimal value of an optimization problem is</p> \\[ p^{\\star}=\\inf \\left\\{f(\\theta) \\mid \\theta \\in \\mathbb{R}^{n}, \\theta \\text { feasible }\\right\\} \\] <ul> <li>\\(p^{\\star}=\\infty\\) if problem is infeasible</li> <li>\\(p^{\\star}=-\\infty\\) is possible</li> <li>In ML, it is often a priori clear that \\(0 \\leq p^{\\star}&lt;\\infty\\).</li> </ul> <p>If \\(f\\left(\\theta^{\\star}\\right)=p^{\\star}\\), we say \\(\\theta^{\\star}\\) is a solution or \\(\\theta^{\\star}\\) is optimal. A solution may or may not exist, and a solution may or may not be unique.</p>"},{"location":"books-and-courses/mfdnn/1/#examples-of-optimization-problem","title":"Examples of Optimization Problem","text":"<p>Example 1.4 : Curve Fitting</p> <p>Consider setup with data \\(X_{1}, \\ldots, X_{N}\\) and corresponding labels \\(Y_{1}, \\ldots, Y_{N}\\) satisfying the relationship</p> \\[ Y_{i}=f_{\\star}\\left(X_{i}\\right)+\\text { error } \\] <p>for \\(i=1, \\ldots, N\\). Hopefully, \"error\" is small. True function \\(f_{\\star}\\) is unknown.</p> <p>Goal is to find a function (curve) \\(f\\) such that \\(f \\approx f_{\\star}\\).</p> <p>Example 1.5 : Least-Squares Minimization</p> <ul> <li> <p>Problem</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\quad \\frac{1}{2}\\|X \\theta-Y\\|^{2} \\] <p>where \\(X \\in \\mathbb{R}^{N \\times p}\\) and \\(Y \\in \\mathbb{R}^{N}\\). Equivalent to</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\frac{1}{2} \\sum_{i=1}^{N}\\left(X_{i}^{\\top} \\theta-Y_{i}\\right)^{2} \\] <p>where \\(X=\\left[\\begin{array}{c}X_{1}^{\\top} \\\\ \\vdots \\\\ X_{N}^{\\top}\\end{array}\\right]\\) and \\(Y=\\left[\\begin{array}{c}Y_{1} \\\\ \\vdots \\\\ Y_{N}\\end{array}\\right]\\).</p> </li> <li> <p>Solution</p> <p>To solve</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\frac{1}{2}\\|X \\theta-Y\\|^{2} \\] <p>take gradient and set it to \\(0\\).</p> \\[ \\nabla_{\\theta} \\frac{1}{2}\\|X \\theta-Y\\|^{2}=X^{\\top}(X \\theta-Y) \\] \\[ \\begin{gathered} X^{\\top}\\left(X \\theta^{\\star}-Y\\right)=0 \\\\ \\theta^{\\star}=\\left(X^{\\top} X\\right)^{-1} X^{\\top} Y \\end{gathered} \\] <p>Here, we assume \\(X^{\\top} X\\) is invertible.</p> </li> </ul> <p>Concept 1.6 : Least squares is an instance of curve fitting.</p> <p>Define \\(f_{\\theta}(x)=x^{\\top} \\theta\\). Then LS becomes</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\frac{1}{2} \\sum_{i=1}^{N}\\left(f_{\\theta}\\left(X_{i}\\right)-Y_{i}\\right)^{2} \\] <p>and the solution hopefully satisfies</p> \\[ Y_{i}=f_{\\theta}\\left(X_{i}\\right)+\\text { small. } \\] <p>Since \\(X_{i}\\) and \\(Y_{i}\\) is assumed to satisfy</p> \\[ Y_{i}=f_{\\star}\\left(X_{i}\\right)+\\text { error } \\] <p>we are searching over linear functions (linear curves) \\(f_{\\theta}\\) that best fit (approximate) \\(f_{\\star}\\).</p>"},{"location":"books-and-courses/mfdnn/1/#local-and-global-minimum","title":"Local and Global Minimum","text":"<p>Definition 1.7 : Local vs Global Minima</p> <p>\\(\\theta^{\\star}\\) is a local minimum if \\(f(\\theta) \\geq f\\left(\\theta^{\\star}\\right)\\) for all feasible \\(\\theta\\) within a small neighborhood.</p> <p>\\(\\theta^{\\star}\\) is a global minimum if \\(f(\\theta) \\geq f\\left(\\theta^{\\star}\\right)\\) for all feasible \\(\\theta\\).</p> <p> </p> <p>In the worst case, finding the global minimum of an optimization problem is difficult. However, in deep learning, optimization problems are often \"solved\" without any guarantee of global optimality.</p>"},{"location":"books-and-courses/mfdnn/10/","title":"\u00a7 10. Flow Models","text":""},{"location":"books-and-courses/mfdnn/10/#probabilistic-generative-models","title":"Probabilistic Generative Models","text":"<p>Definition 10.1 : Probabilistic generative models</p> <p>A probabilistic generative model learns a distribution \\(p_{\\theta}\\) from \\(X_{1}, \\ldots, X_{N} \\sim p_{\\text {true }}\\) such that \\(p_{\\theta} \\approx p_{\\text {true }}\\) and such that we can generate new samples \\(X \\sim p_{\\theta}\\).</p> <p>The ability to generate new synthetic data is interesting, but by itself not very useful. (Generating fake images to use in fake social media accounts is the only direct application that I can think of.)</p> <p>The structure of the data learned through the unsupervised learning is of higher value. However, we won't talk about the downstream applications in this course.</p> <p>In this class, we will talk about flow models, VAEs, and GANs.</p> <p>Fit a probability density function \\(p_{\\theta}(x)\\) with continuous data \\(X_{1}, \\ldots, X_{N} \\sim p_{\\text {true }}(x)\\).</p> <ul> <li>We want to fit the data \\(X_{1}, \\ldots, X_{N}\\) (or really the underlying distribution \\(p_{\\text {true}}\\)) well.</li> <li>We want to be able to sample from \\(p_{\\theta}\\).</li> <li>(We want to get a good latent representation.)</li> </ul> <p>We first develop the mathematical discussion with 1D flows, and then generalize the discussion to high dimensions.</p> <p>Concept 10.2 : Example Density Model : Gaussian Mixture Model</p> \\[ p_{\\theta}(x)=\\sum_{i=1}^{k} \\pi_{i} \\mathcal{N}\\left(x ; \\mu_{i}, \\sigma_{i}^{2}\\right) \\] <p>Parameters: means and variances of components, mixture weights</p> \\[ \\theta=\\left(\\pi_{1}, \\ldots, \\pi_{k}, \\mu_{1}, \\ldots, \\mu_{k}, \\sigma_{1}, \\ldots, \\sigma_{k}\\right) \\] <p>Problems with GMM:</p> <ul> <li>Highly non-convex optimization problem. Can easily get stuck in local minima.</li> <li>It is does not have the representation power to express high-dimensional data.</li> </ul> <p> </p> <p>GMM doesn't work with high-dimensional data. The sampling process is:</p> <ol> <li>Pick a cluster center</li> <li>Add Gaussian noise</li> </ol> <p>If this is done with natural images, a realistic image can be generated only if it is a cluster center, i.e., the clusters must already be realistic images.</p> <p> </p> <p>So then how do we fit a general (complex) density model?</p>"},{"location":"books-and-courses/mfdnn/10/#1d-flow-models","title":"1D Flow Models","text":"<p>Concept 10.3 : Math Review : 1D Continuous RV</p> <p>A random variable \\(X\\) is continuous if there exists a probability density function (PDF) \\(p_{X}(x) \\geq 0\\) such that</p> \\[ \\mathbb{P}(a \\leq X \\leq b)=\\int_{a}^{b} p_{X}(x) d x \\] <p>In this case, we write \\(X \\sim p_{X}\\).</p> <p> </p> <p>The cumulative distribution function (CDF) of \\(X\\) is defined as</p> \\[ F_{X}(t)=\\mathbb{P}(X \\leq t)=\\int_{-\\infty}^{t} p_{X}(x) d x \\] <ul> <li>\\(F_{X}(t)\\) is a nondecreasing function.  </li> <li>\\(F_{X}(t)\\) is a continuous function if \\(X\\) is a continuous random variable.</li> </ul> <p> </p> <p>Concept 10.4 : Na\u00efve Approach : Parameterize \\(p_{\\theta}\\) as DNN</p> <p>Na\u00efve approach for fitting a density model. Represent \\(p_{\\theta}(x)\\) with DNN.  </p> <p>There are some challenges:</p> <ol> <li> <p>How to ensure proper distribution?</p> \\[\\int_{-\\infty}^{+\\infty} p_{\\theta}(x) d x=1, \\quad p_{\\theta}(x) \\geq 0, \\quad x \\in \\mathbb{R}\\] </li> <li> <p>How to sample?</p> </li> </ol> <p>Normalization of \\(p_{\\theta}\\)</p> <p>For discrete random variables, one can use the soft-max function \\(\\mu: \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{k}\\) defined as</p> \\[ \\mu_{i}(z)_{i}=\\frac{e^{z_{i}}}{\\sum_{j=1}^{k} e^{z_{j}}} \\] <p>to normalize probabilities.</p> <p>For continuous random variables, we can ensure \\(p_{\\theta} \\geq 0\\) with \\(p_{\\theta}(x)=e^{f_{\\theta}(x)}\\), where \\(f_{\\theta}\\) is the output of the neural network. However, ensuring the normalization</p> \\[ \\int_{-\\infty}^{+\\infty} p_{\\theta}(x) d x=1 \\] <p>is not a simple matter. (Any Bayesian statistician can tell you how difficult this is.)</p> <p>What happens if we ignore normalization?</p> <p>Do we really need this normalization thing? Yes, we do.</p> <p>Without normalization, one can just assign arbitrarily large probabilities everywhere when we perform maximum likelihood estimation:</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right) \\] <p>The solution is to set \\(p_{\\theta}(x)=M\\) with \\(M \\rightarrow \\infty\\).</p> <p>We want model to place large probability on data \\(X_{1}, \\ldots, X_{N}\\) while placing small probability elsewhere. Normalization forces model to place small probability where data doesn't reside.</p> <p>Definition 10.5 : Flow Model : Parameterize \\(Z=f_{\\theta}(X)\\) with DNN</p> <p>Key insight of normalizing flow: DNN outputs random variable \\(Z\\), rather than \\(p_{\\theta}(X)\\). We choose \\( Z \\) from a known distribution that is easy to sample from, such as \\( \\mathcal{N}(0,1) \\) or \\( \\operatorname{Uniform}([0,1]) \\).</p> <p> </p> <p>In normalizing flow, find \\(\\theta\\) such that the flow \\(f_{\\theta}\\) normalizes the random variable \\(X \\sim p_{X}\\) into \\(Z \\sim \\mathcal{N}(0,1)\\). Generally, we can consider \\(Z \\sim pZ\\). The choice of \\(pZ\\), however, does not seem to make a significant difference.</p> <p>Important questions to resolve:</p> <ol> <li>How to train? (How to evaluate \\(p_{\\theta}(x)\\) ? DNN outputs \\(f_{\\theta}\\), not \\(p_{\\theta}\\).) (Concept 10.7)</li> <li>How to sample \\(X\\) ? (Concept 10.8)</li> </ol> <p>Concept 10.6 : Math Review : 1D Change of Variable Formula for RV</p> <p>Assume \\(f\\) is invertible, \\(f\\) is differentiable, and \\(f^{-1}\\) is differentiable.</p> <p>If \\(X \\sim p_{X}\\), then \\(Z=f(X)\\) has pdf</p> \\[ p_{Z}(z)=p_{X}\\left(f^{-1}(z)\\right)\\left|\\frac{d x}{d z}\\right| \\] <p>If \\(Z \\sim p_{Z}\\), then \\(X=f^{-1}(Z)\\) has pdf</p> \\[ p_{X}(x)=p_{Z}(f(x))\\left|\\frac{d f(x)}{d x}\\right| \\] <p>Since \\(Z=f(X)\\), one might think \\(p_{X}(x)=p_{Z}(z)=p_{Z}(f(x))\\). \\(\\leftarrow\\) This is wrong.</p> <p>Invertibility of \\(f\\) is essential; it is not a minor technical issue.</p> <p>Definition 10.7 : Traning Flow Models</p> <p>Train model with MLE</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{Z}\\left(f_{\\theta}\\left(X_{i}\\right)\\right)+\\log \\left|\\frac{\\partial f_{\\theta}}{\\partial x}\\left(X_{i}\\right)\\right| \\] <p>where \\(f_{\\theta}\\) is invertible and differentiable, and \\(X=f_{\\theta}^{-1}(Z)\\) with \\(Z \\sim p_{Z}\\) so</p> \\[ p_{X}(x)=p_{Z}\\left(f_{\\theta}(x)\\right)\\left|\\frac{\\partial f_{\\theta}}{\\partial x}(x)\\right| \\] <p>Can optimize with SGD, if we know how to perform backprop on \\(\\left|\\frac{\\partial f_{\\theta}}{\\partial x}\\left(X_{i}\\right)\\right|\\). More on this later.</p> <p>Definition 10.8 : Sampling from Flow Models</p> <p> </p> <ol> <li>Sample \\(Z \\sim p_{Z}\\)</li> <li>Compute \\(X=f_{\\theta}^{-1}(Z)\\)</li> </ol> <p>Concept 10.9 : Requirements of Flow \\(f_{\\theta}\\)</p> <p>Theoretical requirement:</p> <ul> <li>\\(f_{\\theta}(x)\\) invertible and differentiable.</li> </ul> <p>Computational requirements:</p> <ul> <li>\\(f_{\\theta}(x)\\) and \\(\\nabla_{\\theta} f_{\\theta}(x)\\) efficient to evaluate (for training)</li> <li>\\(\\left|\\frac{\\partial f_{\\theta}}{\\partial x}(x)\\right|\\) and \\(\\nabla_{\\theta}\\left|\\frac{\\partial f_{\\theta}}{\\partial x}(x)\\right|\\) efficient to evaluate (for training)</li> <li>\\(f_{\\theta}^{-1}\\) efficient to evaluate (for sampling)</li> </ul> <p>Example 10.10 : Example of Trained Flow Models</p> <ul> <li>Flow to \\(Z\\) ~ \\(\\operatorname{Uniform}([0,1])\\) </li> <li>Flow to \\(Z \\sim \\operatorname{Beta}(5,5)\\) </li> <li>Flow to \\(Z \\sim \\mathcal{N}(0,1)\\) </li> </ul> <p>Concept 10.11 : Universality of Flows</p> <p>Are flows universal, i.e., can \\(f_{\\theta}^{-1}(Z) \\sim p_{X}\\) for any \\(X\\) provided that \\(f_{\\theta}\\) can represent any invertible function?</p> <p>Yes, 1D flows are universal due to the inverse CDF sampling technique. (Some basic conditions are being omitted.)</p> <p>Higher dimensional flows are also universal as shown by Huang et al.\\(^{\\star}\\) or earlier by the general theory of optimal transport. (link)</p> <p>\\(^{\\star}\\) C.-W. Huang, D. Krueger, A. Lacoste, and A. Courville, Neural Autoregressive Flows, ICML, 2018.</p> <p>Concept 10.12 : Math Review : Sampling via Inverse CDF</p> <p>Inverse CDF sampling is a technique for sampling \\(X \\sim p_{X}\\). If \\(F_{X}(t)\\) is furthermore a strictly increasing function, then \\(F_{X}\\) is invertible, i.e., \\(F_{X}^{-1}\\) exists.</p> <p>Generate a random number \\(U \\sim \\operatorname{Uniform}([0,1])\\) and compute \\(F_{X}^{-1}(U)\\). Then</p> \\[ F_{X}^{-1}(U) \\sim p_{X} \\] <p>since</p> \\[ \\mathbb{P}\\left(F_{X}^{-1}(U) \\leq t\\right)=\\mathbb{P}\\left(U \\leq F_{X}(t)\\right)=F_{X}(t) \\] <p>Technique can be generalized to when \\(F_{X}\\) is not invertible.</p> <p>Inverse CDF can be seen as flow model from \\(X \\sim p_X\\) to \\(U \\sim \\operatorname{Uniform}([0,1])\\). As seen above, the flow is \\(F_X\\), so when we do not know the true distribution \\(p_X\\), we can train the flow model to learn \\(F_{X}^{-1}(U) \\sim p_{X}\\).</p> <p>Concept 10.13 : Universality of 1D Flows</p> <p>Composition of flows is a flow, and inverse of a flow is a flow.</p> <p>Universality of 1D flows:</p> <ul> <li>Use inverse CDF as flow to transform \\(X \\sim p_{X}\\) into \\(U \\sim \\operatorname{Uniform}([0,1])\\) and \\(Z \\sim \\mathcal{N}(0,1)\\) into \\(U \\sim \\operatorname{Uniform}([0,1])\\).</li> <li>Compose flow \\(X \\rightarrow U\\) and inverse flow \\(U \\rightarrow Z\\).</li> </ul> <p> </p>"},{"location":"books-and-courses/mfdnn/10/#high-dimensional-flow-models","title":"High Dimensional Flow Models","text":"<p>Concept 10.14 : Math Review : Jacobian Notation</p> <p>Let \\(f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}\\), such that</p> \\[ f(x)=\\left[\\begin{array}{c} f_{1}(x) \\\\ f_{2}(x) \\\\ \\vdots \\\\ f_{n}(x) \\end{array}\\right] \\] <p>The Jacobian matrix is</p> \\[ \\frac{\\partial f}{\\partial x}(x)=\\left[\\begin{array}{cccc} \\frac{\\partial f_{1}}{\\partial x_{1}}(x) &amp; \\frac{\\partial f_{1}}{\\partial x_{2}}(x) &amp; \\cdots &amp; \\frac{\\partial f_{1}}{\\partial x_{n}}(x) \\\\ \\frac{\\partial f_{2}}{\\partial x_{1}}(x) &amp; \\frac{\\partial f_{2}}{\\partial x_{2}}(x) &amp; \\cdots &amp; \\frac{\\partial f_{2}}{\\partial x_{n}}(x) \\\\ \\vdots &amp; &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial f_{n}}{\\partial x_{1}}(x) &amp; \\frac{\\partial f_{n}}{\\partial x_{2}}(x) &amp; \\cdots &amp; \\frac{\\partial f_{n}}{\\partial x_{n}}(x) \\end{array}\\right]=\\left[\\begin{array}{c} \\left(\\nabla f_{1}(x)\\right)^{\\top} \\\\ \\left(\\nabla f_{2}(x)\\right)^{\\top} \\\\ \\vdots \\\\ \\left(\\nabla f_{n}(x)\\right)^{\\top} \\end{array}\\right] \\] <p>The Jacobian determinant is \\(\\operatorname{det}\\left(\\frac{\\partial f}{\\partial x}\\right)\\). We use the notation</p> \\[ \\left|\\frac{\\partial f}{\\partial x}(x)\\right|=\\left|\\operatorname{det}\\left(\\frac{\\partial f}{\\partial x}(x)\\right)\\right| \\] <p>where the second \\(|\\cdot|\\) is the absolute value of the determinant. (This notation is not completely standard.)</p> <p>Concept 10.15 : Math Review : Multivariate Change of Variables</p> <p>Let \\(f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}\\) be an invertible function such that both \\(f\\) and \\(f^{-1}\\) are differentiable. Let \\(U \\subseteq \\mathbb{R}^{n}\\). Then</p> \\[ \\int_{f(U)} h(v) d v=\\int_{U} h(f(u))\\left|\\frac{\\partial f}{\\partial u}(u)\\right| d u \\] <p>for any \\(h: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}\\). (Change of variable from \\(v=f(u)\\) to \\(u=f^{-1}(v)\\).)</p> <p>(The conditions for this change of variable formula can be further generalized.)</p> <p>Concept 10.16 : Math Review : Multivariate Continuous RV</p> <p>A multivariate random variable \\(X \\in \\mathbb{R}^{n}\\) is continuous if there exists a probability density function \\(p_{X}(x)\\) such that</p> \\[ \\mathbb{P}(X \\in A)=\\int_{A} p_{X}(x) d x \\] <p>where the integral is over the volume \\(A \\subseteq \\mathbb{R}^{n}\\). In this case, we write \\(X \\sim p_{X}\\).</p> <p>The joint cumulative distribution function (the copula) does not seem to be useful in the context of high-dimensional flow models.</p> <p>Concept 10.17 : Math Review : Multivariate Change of Variable Formula for RV</p> <p>Let \\(f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}\\) be an invertible function such that both \\(f\\) and \\(f^{-1}\\) are differentiable. Let \\(X\\) be a continuous random variable with probability density function \\(p_{X}\\) and let \\(Y=f(X)\\) have density \\(p_{Y}\\). Then</p> \\[ p_{X}(x)=p_{Y}(f(x))\\left|\\frac{\\partial f}{\\partial x}(x)\\right| \\] <p>Proof</p> \\[ \\mathbb{P}\\left(f^{-1}(Y) \\in A\\right)=\\mathbb{P}(Y \\in f(A))=\\int_{f(A)} p_{Y}(y) d y=\\int_{A} p_{Y}(f(x))\\left|\\frac{\\partial f}{\\partial x}(x)\\right| d x=\\mathbb{P}(X \\in A) \\] <p>Invertibility of \\(f\\) is essential; it is not a minor technical issue.</p> <p>Concept 10.18 : Math Review : Determinant</p> <p>Fact: Determinant definitions in undergraduate linear algebra textbooks require exponentially many operations to compute:</p> \\[ \\operatorname{det}(A)=\\sum_{\\sigma \\in S_{n}}\\left(\\operatorname{sgn}(\\sigma) \\prod_{i=1}^{n} a_{i, \\sigma_{i}}\\right) \\] <p>Efficient computation of determinant for general matrices and performing backprop through the computation is difficult. Therefore, high-dimensional flow model are designed to compute determinants only on simple matrices.</p> <ul> <li>Product formula: if \\(A\\) and \\(B\\) are square, then</li> </ul> \\[ \\operatorname{det}(A B)=\\operatorname{det}(A) \\operatorname{det}(B) \\] <ul> <li>Block lower triangular formula: if \\(A \\in \\mathbb{R}^{n \\times n}\\) and \\(C \\in \\mathbb{R}^{m \\times m}\\), then</li> </ul> \\[ \\operatorname{det}\\left(\\begin{array}{ll} A &amp; 0 \\\\ B &amp; C \\end{array}\\right)=\\operatorname{det}(A) \\operatorname{det}(C) \\] <ul> <li>Lower triangular formula: if \\(a_{1}, \\ldots, a_{n} \\in \\mathbb{R}\\) and \\(*\\) represents arbitrary values, then</li> </ul> \\[ \\operatorname{det}\\left(\\begin{array}{cccc} a_{1} &amp; 0 &amp; \\cdots &amp; 0 \\\\ * &amp; a_{2} &amp; &amp; \\vdots \\\\ * &amp; * &amp; \\ddots &amp; 0 \\\\ * &amp; * &amp; * &amp; a_{n} \\end{array}\\right)=\\prod_{i=1}^{n} a_{i} \\] <ul> <li>Upper triangular formula: same as for lower triangular matrices.</li> </ul> <p>Definition 10.19 : Training High Dimensional Flow Models</p> <p>Train model with MLE</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{Z}\\left(f_{\\theta}\\left(X_{i}\\right)\\right)+\\log \\left|\\frac{\\partial f_{\\theta}}{\\partial x}\\left(X_{i}\\right)\\right| \\] <p>where \\(f_{\\theta}(z)\\) is invertible and differentiable, and \\(X=f^{-1}(Z)\\) with \\(Z \\sim p_{Z}\\) so</p> \\[ p_{X}(x)=p_{Z}\\left(f_{\\theta}(x)\\right)\\left|\\frac{\\partial f_{\\theta}}{\\partial x}(x)\\right| \\] <p>(Exactly the same formula as with 1D flow.)</p> <p>Can optimize with SGD, if we know how to perform backprop on \\(\\left|\\frac{\\partial f_{\\theta}}{\\partial x}\\left(X_{i}\\right)\\right|\\).</p>"},{"location":"books-and-courses/mfdnn/10/#coupling-flows","title":"Coupling Flows","text":"<p>Concept 10.20 : Composing Flows</p> <p>Flows can be composed to increase expressiveness. (Deep NN more expressive.) Consider composition of \\(k\\) flows</p> \\[ \\begin{aligned} &amp; x \\rightarrow f_{1} \\rightarrow f_{2} \\rightarrow \\cdots \\rightarrow f_{k} \\rightarrow z \\\\ &amp; z=f_{k} \\circ \\cdots \\circ f_{1}(x) \\\\ &amp; x=f_{1}^{-1} \\circ \\cdots \\circ f_{k}^{-1}(z) \\end{aligned} \\] <p>Determinant computation splits nicely due to chain rule and product formula</p> \\[ \\begin{aligned} &amp; \\operatorname{det}\\left(\\frac{\\partial z}{\\partial x}\\right)=\\operatorname{det}\\left(\\frac{\\partial f_{k}}{\\partial f_{k-1}} \\cdots \\frac{\\partial f_{1}}{\\partial f_{0}}\\right)=\\operatorname{det}\\left(\\frac{\\partial f_{k}}{\\partial f_{k-1}}\\right) \\cdots \\operatorname{det}\\left(\\frac{\\partial f_{1}}{\\partial f_{0}}\\right) \\\\ &amp; \\log p_{\\theta}(x)=\\log p_{\\theta}(z)+\\sum_{i=1}^{k} \\log \\left|\\frac{\\partial f_{i}}{\\partial f_{i-1}}\\right| \\end{aligned} \\] <p>Definition 10.21 : Affine Flows</p> <p>An affine (linear) transformation</p> \\[ f_{A, b}(x)=A^{-1}(x-b) \\] <p>is a flow if matrix \\(A\\) is invertible. Then</p> \\[ \\frac{\\partial f_{A, b}}{\\partial x}=A^{-1} \\] <p>and</p> \\[ \\left|\\frac{\\partial f_{A, b}}{\\partial x}\\right|=\\left|\\operatorname{det}\\left(A^{-1}\\right)\\right|=\\frac{1}{|\\operatorname{det}(A)|} \\] <p>Sampling: \\(X=A Z+b\\), where \\(Z \\sim \\mathcal{N}(0, I)\\).</p> <p>Problem with affine flows:</p> <ul> <li>Computing \\(|\\operatorname{det}(A)|\\) is expensive and performing backprop over it is difficult. We want \\(\\frac{\\partial f_{A, b}}{\\partial x}\\) to be further structured so that determinant is easy to compute.</li> <li>One affine flow is insufficient to generate complex data. However, composing multiple affine flows yields an affine flow and therefore is pointless. We need to introduce nonlinearities.</li> </ul> <p>Definition 10.22 : Coupling Flows</p> <p>A coupling flow is a general and practical approach for constructing non-linear flows.</p> <p>Partition input into two disjoint subsets \\(x=\\left(x^{A}, x^{B}\\right)\\). Then</p> \\[ f(x)=\\left(x^{A}, \\hat{f}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right)\\right) \\] <p>where \\(\\psi_{\\theta}\\) is a neural network and \\(\\hat{f}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right)\\) is another flow whose parameters depend on \\(x^{A}\\).</p> <p>Definition 10.23 : Evaluation of Coupling Flows</p> <ul> <li>Forward Evaluation</li> </ul> <p> </p> <ul> <li>Inverse Evaluation</li> </ul> <p> </p> <p>Concept 10.24 : Jacobian of Coupling Flows</p> <p>The Jacobian of a coupling flow has a nice block structure</p> \\[ \\frac{\\partial f_{\\theta}}{\\partial x}(x)=\\left[\\begin{array}{cc} I &amp; 0 \\\\ \\frac{\\partial \\hat{f}}{\\partial x^{A}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right) &amp; \\frac{\\partial \\hat{f}}{\\partial x^{B}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right) \\end{array}\\right] \\] <p>which leads to the simplified determinant formula</p> \\[ \\operatorname{det}\\left(\\frac{\\partial f_{\\theta}}{\\partial x}(x)\\right)=\\operatorname{det}\\left(\\frac{\\partial \\hat{f}}{\\partial x^{B}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right)\\right) \\] <p>Note \\(\\frac{\\partial \\hat{f}}{\\partial x^{A}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right)\\), which will be very complicated, does not appear in the determinant.</p> <p>Definition 10.25 : Coupling transformation \\(\\hat{f}(x \\mid \\psi)\\)</p> <ul> <li> <p>Additive transformations (NICE)</p> \\[ \\hat{f}(x \\mid \\psi)=x+t \\] <p>where \\(\\psi=t\\).</p> </li> <li> <p>Affine transformations (Real NVP)</p> \\[ \\hat{f}(x \\mid \\psi)=e^{s} \\odot x+t \\] <p>where \\(\\psi=(s, t)\\).</p> </li> </ul> <p>Other transformations studied throughout the literature.</p> <p>Definition 10.26 : NICE (Non-linear Independent Components Estimation)</p> <p>NICE uses additive coupling layers: Split variables in half: \\(x_{1: n / 2}, x_{n / 2: n}\\)</p> \\[ \\begin{aligned} &amp; z_{1: n / 2}=x_{1: n / 2} \\\\ &amp; z_{n / 2: n}=x_{n / 2: n}+t_{\\theta}\\left(x_{1: n / 2}\\right) \\end{aligned} \\] <p>Easily invertible:</p> \\[ \\begin{aligned} &amp; x_{1: n / 2}=z_{1: n / 2} \\\\ &amp; x_{n / 2: n}=z_{n / 2: n}-t_{\\theta}\\left(x_{1: n / 2}\\right) \\end{aligned} \\] <p>Jacobian determinant is easy to compute:</p> \\[ \\operatorname{det} \\frac{\\partial f_{\\theta}}{\\partial x}(x)=\\operatorname{det}\\left[\\begin{array}{cc}I &amp; 0 \\\\ \\frac{\\partial \\hat{f}}{\\partial x^{A}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right) &amp; \\frac{\\partial \\hat{f}}{\\partial x^{B}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right)\\end{array}\\right]=\\operatorname{det}\\left[\\begin{array}{cc}I &amp; 0 \\\\ \\frac{\\partial \\hat{f}}{\\partial x^{A}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right) &amp; I\\end{array}\\right]=1 \\] <p>(L. Dinh, D. Krueger, and Y. Bengio, NICE: Non-linear independent components estimation, ICLR Workshop, 2015.)</p> <p>Definition 10.27 : Real NVP (Real-valued Non-Volume Preserving)</p> <p>Real NVP uses affine coupling layers:</p> \\[ \\begin{aligned} &amp; z_{1: n / 2}=x_{1: n / 2} \\\\ &amp; z_{n / 2: n}=e^{s_{\\theta}\\left(x_{1: n / 2}\\right)} \\odot x_{n / 2: n}+t_{\\theta}\\left(x_{1: n / 2}\\right) \\end{aligned} \\] <p>Easily invertible:</p> \\[ \\begin{aligned} &amp; x_{1: n / 2}=z_{1: n / 2} \\\\ &amp; x_{n / 2: n}=\\left(z_{n / 2: n}-t_{\\theta}\\left(x_{1: n / 2}\\right)\\right) \\odot e^{-s_{\\theta}\\left(x_{1: n / 2}\\right)} \\end{aligned} \\] <p>Jacobian determinant is easy to compute:</p> \\[ \\begin{aligned} \\operatorname{det} \\frac{\\partial f_{\\theta}}{\\partial x}(x) &amp; =\\operatorname{det}\\left[\\begin{array}{cc} I &amp; 0 \\\\ \\frac{\\partial \\hat{f}}{\\partial x^{A}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right) &amp; \\frac{\\partial \\hat{f}}{\\partial x^{B}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right) \\end{array}\\right] \\\\ &amp; =\\operatorname{det}\\left[\\begin{array}{cc} I &amp; 0 \\\\ \\frac{\\partial \\hat{f}}{\\partial x^{A}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right) &amp; \\operatorname{diag}\\left(e^{s_{\\theta}\\left(x_{1: n / 2}\\right)}\\right) \\end{array}\\right]=\\exp \\left(\\mathbf{1}_{n / 2}^{\\top} s_{\\theta}\\left(x_{1: n / 2}\\right)\\right) \\end{aligned} \\] <p>(L. Dinh, J. Sohl-Dickstein, and S. Bengio, Density estimation using Real NVP, ICLR, 2017.)</p> <ul> <li>Results of Real NVP</li> </ul>  ![](./assets/10.14.png){: width=\"100%\"}  <p>Concept 10.28 : How to partition variables?</p> <p>Note that the additive and affine coupling layers of NICE and Real NVP are nonlinear mappings from \\(x_{1: n}\\) to \\(z_{1: n}\\), since \\(s_{\\theta}\\left(x_{1: n / 2}\\right)\\) and \\(t_{\\theta}\\left(x_{1: n / 2}\\right)\\) are nonlinear.</p> <p>Flow models compose multiple nonlinear flows. But if \\(x_{1: n / 2}\\) is always unchanged, then the full composition will leave it unchanged. Therefore, we change the partitioning for every coupling layer.</p> <p>Concept 10.29 : Real NVP Variable Partitioning</p> <p>Two partition strategies:</p> <ol> <li>Partition with checkerboard pattern.</li> <li>Reshape tensor and then partition channelwise.</li> </ol> <p> </p> <p>(L. Dinh, J. Sohl-Dickstein, and S. Bengio, Density estimation using Real NVP, ICLR, 2017.)</p> <p>Definition 10.30 : Real NVP Architecture</p> <p> </p> <p>Input \\(X\\) : \\(c \\times 32 \\times 32\\) image with \\(c=3\\)</p> <p>Layer 1: Input \\(X: c \\times 32 \\times 32\\)</p> <ul> <li>Checkerboard \\(\\times 3\\), channel reshape into \\(4 c \\times 16 \\times 16\\), channel \\(\\times 3\\)</li> <li>Output: Split result to get \\(X_{1}: 2 c \\times 16 \\times 16\\) and \\(Z_{1}: 2 c \\times 16 \\times 16\\) (fine-grained latents)</li> </ul> <p>Layer 2: Input \\(X_{1}: 2 c \\times 16 \\times 16\\) from layer 1</p> <ul> <li>Checkerboard \\(\\times 3\\), channel reshape into \\(8 c \\times 8 \\times 8\\), channel \\(\\times 3\\)</li> <li>Split result to get \\(X_{2}: 4 c \\times 8 \\times 8\\) and \\(Z_{2}: 4 c \\times 8 \\times 8\\) (coarser latents)</li> </ul> <p>Layer 3: Input \\(X_{2}: 4 c \\times 8 \\times 8\\) from layer 2</p> <ul> <li>Checkerboard \\(\\times 3\\), channel reshape into \\(16 c \\times 4 \\times 4\\), channel \\(\\times 3\\)</li> <li>Get \\(Z_{3}: 16 c \\times 4 \\times 4\\) (latents for highest-level details)</li> </ul> <p>Output \\(Z = (Z_1, Z_2, Z_3) \\in \\mathbb{R}^{c \\dot 32^2}\\)</p> <p>Concept 10.31 : Batch Normalization in Deep Flows</p> <p>To train deep flows, BN is helpful. However, the large model size forces the use of small batch sizes, and BN is not robust with small batch sizes. RealNVP uses a modified form of BN</p> \\[ x \\mapsto \\frac{x-\\tilde{\\mu}}{\\sqrt{\\tilde{\\sigma}^{2}+\\varepsilon}} \\] <p>(No \\(\\beta\\) and \\(\\gamma\\) parameters.) This layer has the log Jacobian determinant</p> \\[ -\\frac{1}{2} \\sum_{i} \\log \\left(\\tilde{\\sigma}_{i}^{2}+\\varepsilon\\right) \\] <p>The mean and variance parameters are updated with</p> \\[ \\begin{aligned} \\tilde{\\mu}_{k+1} &amp; =\\rho \\tilde{\\mu}_{k}+(1-\\rho) \\hat{\\mu}_{k} \\\\ \\tilde{\\sigma}_{k+1}^{2} &amp; =\\rho \\tilde{\\sigma}_{k}^{2}+(1-\\rho) \\hat{\\sigma}_{k}^{2} \\end{aligned} \\] <p>where \\(\\rho\\) is the momentum. During gradient computation, only backprop through the current batch statistics \\(\\hat{\\mu}_{k}\\) and \\(\\hat{\\sigma}_{k}^{2}\\).</p> <p>Concept 10.32 : \\(s_{\\theta}\\) and \\(t_{\\theta}\\) networks</p> <p>The \\(s_{\\theta}\\) and \\(t_{\\theta}\\) do not need to be invertible. The original RealNVP paper does not describe its construction.</p> <p>We let \\(\\left(s_{\\theta}, t_{\\theta}\\right)\\) be a deep (20-layer) convolutional neural network using residual connections and standard batch normalization.</p>"},{"location":"books-and-courses/mfdnn/10/#researches","title":"Researches","text":"<p>Definition 10.33 : Glow Paper</p> <p>The authors of the Glow paper also released a blog post. link</p> <p>(D. P. Kingma and P. Dhariwal, Glow: Generative flow with invertible 1x1 convolutions, NeurIPS, 2018.)</p> <p>Definition 10.34 : FFJORD</p> <p>Instead of a discrete composition of flows, what if we have a continuous-time flow?</p> \\[ \\begin{aligned} z_{0} &amp; =x \\\\ z_{t} &amp; =z_{0}+\\int_{0}^{t} h\\left(t, z_{t}\\right) d t \\\\ f(x) &amp; =z_{1} \\end{aligned} \\] <p>Inverse:</p> \\[ \\begin{aligned} z_{1} &amp; =z \\\\ z_{t} &amp; =z_{1}-\\int_{t}^{1} h\\left(t, z_{t}\\right) d t \\\\ f^{-1}(z) &amp; =z_{0} \\end{aligned} \\] <p>(R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud, Neural ordinary differential equations, NeurIPS, 2018. W. Grathwohl, R. T. Q. Chen, J. Bettencourt, I. Sutskever, and D. Duvenaud, FFJORD: Free-form continuous dynamics for scalable reversible generative models, ICLR, 2019.)</p>"},{"location":"books-and-courses/mfdnn/11/","title":"\u00a7 11. Variational Autoencoders","text":"<p>Prerequisites : Ch A. Appendix - Basics of Monte Carlo</p> <p>Concept 11.1 : Math Review : Conditional Probabilities</p> <p>Let \\(A\\) and \\(B\\) be probabilistic events. Assume \\(A\\) has nonzero probability.</p> <p>Conditional probability satisfies</p> \\[ \\mathbb{P}(B \\mid A) \\mathbb{P}(A)=\\mathbb{P}(A \\cap B) \\] <p>Bayes' theorem is an application of conditional probability:</p> \\[ \\mathbb{P}(B \\mid A)=\\frac{\\mathbb{P}(A \\mid B) \\mathbb{P}(B)}{\\mathbb{P}(A)} \\] <p>Concept 11.2 : Math Review: Conditional Densities</p> <p>Let \\(X \\in \\mathbb{R}^{m}\\) and \\(Z \\in \\mathbb{R}^{n}\\) be continuous random variables with joint density \\(p(x, z)\\).</p> <p>The marginal densities are defined by</p> \\[ p_{X}(x)=\\int_{\\mathbb{R}^{n}} p(x, z) d z, \\quad p_{Z}(z)=\\int_{\\mathbb{R}^{m}} p(x, z) d x \\] <p>The conditional density function \\(p(z \\mid x)\\) has the following properties</p> \\[ \\begin{gathered} \\mathbb{P}(Z \\in S \\mid X=x)=\\int_{S} p(z \\mid x) d z \\\\ p(z \\mid x) p_{X}(x)=p(x, z), \\quad p(z \\mid x)=\\frac{p(x \\mid z) p_{Z}(z)}{p_{X}(x)} \\end{gathered} \\] <p>Concept 11.3 : Introduction for Variational Autoencoders (VAE)</p> <p>Key idea of VAE:</p> <ul> <li>Latent variable model with conditional probability distribution represented by \\(p_{\\theta}(x \\mid z)\\).</li> <li>Efficiently estimate \\(p_{\\theta}(x)=\\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}(x \\mid Z)\\right]\\) by importance sampling with \\(Z \\sim q_{\\phi}(z \\mid x)\\).</li> </ul> <p>We can interpret \\(q_{\\phi}(z \\mid x)\\) as an encoder and \\(p_{\\theta}(x \\mid z)\\) as a decoder.</p> <p>VAEs differ from autoencoders as follows:</p> <ul> <li>Derivations (latent variable model vs. dimensionality reduction)</li> <li>VAE regularizes/controls latent distribution, while AE does not.</li> </ul>  ![](./assets/11.1.png){: width=\"80%\"}  <p>These are synthetic (fake) images made with VAE.</p> <p>(A. Vahdat and J. Kautz, NVAE: A deep hierarchical variational autoencoder, NeurIPS, 2020.)</p>"},{"location":"books-and-courses/mfdnn/11/#latent-variable-model","title":"Latent Variable Model","text":"<ul> <li> <p>Assumption on data \\(X_{1}, \\ldots, X_{N}\\)</p> <p>Assumes there is an underlying latent variable \\(Z\\) representing the \"essential structure\" of the data and an observable variable \\(X\\) which generation is conditioned on \\(Z\\). Implicitly assumes the conditional randomness of \\(X \\sim p_{X \\mid Z}\\) is significantly smaller than the overall randomness \\(X \\sim p_{X}\\).</p> </li> <li> <p>Example</p> <p>\\(X\\) is a cat picture. \\(Z\\) encodes information about the body position, fur color, and facial expression of a cat. Latent variable \\(Z\\) encodes the overall content of the image, but \\(X\\) does contain details not specified in \\(Z\\).</p> </li> </ul> <p>Definition 11.4 : Latent Variable Model</p> <p>VAEs implements a latent variable model with a NN that generates \\(X\\) given \\(Z\\). More precisely, NN is a deterministic function that outputs the conditional distribution \\(p_{\\theta}(x \\mid Z)\\), and \\(X\\) is randomly generated according to this distribution. This structure may effectively learn the latent structure from data if the assumption on data is accurate.</p> <p> </p> <p>Sampling process:</p> \\[ X \\sim p_{\\theta}(x \\mid Z), \\quad Z \\sim p_{Z}(z) \\] <p>Usually \\(p_{Z}\\) is a Gaussian (fixed) and \\(p_{\\theta}(x \\mid z)\\) is a NN parameterized by \\(\\theta\\).</p> <p>Evaluating density (likelihood):</p> \\[ p_{\\theta}\\left(X_{i}\\right)=\\int_{z} p_{Z}(z) p_{\\theta}\\left(X_{i} \\mid z\\right) d z=\\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\] <p>Training via MLE:</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\] <p>When \\(p_{Z}\\) is a discrete:</p> \\[ p_{\\theta}(x)=\\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}(x \\mid Z)\\right]=\\sum_{z} p_{Z}(z) p_{\\theta}(x \\mid Z) \\] <p>When \\(p_{Z}\\) is a continuous:</p> \\[ p_{\\theta}(x)=\\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}(x \\mid Z)\\right]=\\int_{z} p_{Z}(z) p_{\\theta}(x \\mid z) d z \\] <p>To clarify, specification of \\(p_{Z}(z)\\) and \\(p_{\\theta}(x \\mid z)\\) fully determines \\(p_{\\theta}(x)\\) (as above) and</p> \\[ p_{\\theta}(z \\mid x)=\\frac{p_{\\theta}(x \\mid z) p_{Z}(z)}{p_{\\theta}(x)} \\] <p>Training</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\] <p>requires evaluation \\(\\mathbb{E}_{Z}\\).</p> <ul> <li>Scenario 1: If \\(Z\\) is discrete and takes a few of values, then compute \\(\\sum_{z}\\) exactly.</li> <li>Scenario 2: If \\(Z\\) takes many values or if it is a continuous, then \\(\\sum_{z}\\) or \\(\\mathbb{E}_{Z}\\) is impractical to compute. In this case, approximate expectation with Monte Carlo and importance sampling.</li> </ul> <p>Example 11.5 : Example Latent Variable Model: Mixture of Gaussians</p> <p>Mixture of 3 Gaussians in \\(\\mathbb{R}^{2}\\), uniform prior over components. (We can make the mixture weights a trainable parameter.)</p> \\[ \\begin{gathered} p_{Z}(Z=A)=p_{Z}(Z=B)=p_{Z}(Z=C)=\\frac{1}{3} \\\\ p_{\\theta}(x \\mid Z=k)=\\frac{1}{2 \\pi\\left|\\Sigma_{k}\\right|^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2}\\left(x-\\mu_{k}\\right)^{\\top} \\Sigma_{k}^{-1}\\left(x-\\mu_{k}\\right)\\right) \\end{gathered} \\] <p>Training objective:</p> \\[ \\begin{aligned} \\underset{\\mu, \\Sigma}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\mu, \\Sigma}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log &amp; \\left[ \\frac{1}{3} \\frac{1}{2 \\pi\\left|\\Sigma_{A}\\right|^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2}\\left(X_{i}-\\mu_{A}\\right)^{\\top} \\Sigma_{A}^{-1}\\left(X_{i}-\\mu_{A}\\right)\\right) \\right.\\\\ &amp; +\\frac{1}{3} \\frac{1}{2 \\pi\\left|\\Sigma_{B}\\right|^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2}\\left(X_{i}-\\mu_{B}\\right)^{\\top} \\Sigma_{B}^{-1}\\left(X_{i}-\\mu_{B}\\right)\\right) \\\\ &amp; \\left.+\\frac{1}{3} \\frac{1}{2 \\pi\\left|\\Sigma_{C}\\right|^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2}\\left(X_{i}-\\mu_{C}\\right)^{\\top} \\Sigma_{C}^{-1}\\left(X_{i}-\\mu_{C}\\right)\\right)\\right] \\end{aligned} \\] <p> </p>"},{"location":"books-and-courses/mfdnn/11/#training-latent-variable-model-with-importance-sampling","title":"Training Latent Variable Model with Importance Sampling","text":"<p>From now on, we will focus on HOW to train latent variable model with MLE,</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\] <p>Concept 11.6 : VAE Outline</p> <p>Outline of variational autoencoder (VAE):</p> <ol> <li> <p>(Choice 1) Approximate intractable objective with a single \\(Z\\) sample</p> \\[ \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\approx \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i} \\mid Z_{i}\\right), \\quad Z_{i} \\sim p_{Z} \\] </li> <li> <p>(Choice 2) Improve accuracy of approximation by sampling \\(Z_{i}\\) with importance sampling</p> \\[ \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\approx \\sum_{i=1}^{N} \\log \\frac{p_{\\theta}\\left(X_{i} \\mid Z_{i}\\right) p_{Z}\\left(Z_{i}\\right)}{q_{i}\\left(Z_{i}\\right)}, \\quad Z_{i} \\sim q_{i} \\] </li> <li> <p>Optimize approximate objective with SGD.</p> </li> </ol> <p>(D. Kingma and M. Welling, VAE: Auto-encoding variational Bayes, ICLR, 2014.)</p> <p>Concept 11.7 : IWAE Outline</p> <p>Importance weighted autoencoders (IWAE) approximates intractable with \\(K\\) samples of \\(Z\\) :</p> \\[ \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\approx \\sum_{i=1}^{N} \\log \\frac{1}{K} \\sum_{k=1}^{K} \\frac{p_{\\theta}\\left(X_{i} \\mid Z_{i, k}\\right) p_{Z}\\left(Z_{i, k}\\right)}{q_{i}\\left(Z_{i, k}\\right)}, \\quad Z_{i, 1}, \\ldots, Z_{i, K} \\sim q_{i} \\] <p>(Y. Burda, R. Grosse, and R. Salakhutdinov, Importance weighted autoencoders, ICLR, 2016.)</p> <p>Concept 11.8 : Why does VAE need IS?</p> <p>Among the two choices given in Concept 11.6, VAEs improve the accuracy of latent variable model with IS (Choice 2).</p> <p>Sampling \\(Z_{i} \\sim p_{Z}\\) (Choice 1) results in a high-variance estimator:</p> \\[ \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\approx p_{\\theta}\\left(X_{i} \\mid Z_{i}\\right), \\] <p>In the Gaussian mixture example (Example 11.5), only \\(1 / 3\\) of the \\(Z\\) samples meaningfully contribute to the estimate. More specifically, if \\(X_{i}\\) is near \\(\\mu_{A}\\) but is far from \\(\\mu_{B}\\) and \\(\\mu_{C}\\), then \\(p_{\\theta}\\left(X_{i} \\mid Z=A\\right) \\gg 0\\) but \\(p_{\\theta}\\left(X_{i} \\mid Z=B\\right) \\approx 0\\) and \\(p_{\\theta}\\left(X_{i} \\mid Z=C\\right) \\approx 0\\).</p> <p>The issue worsens as the observable and latent variable dimension increases.</p> <p>Concept 11.9 : Na\u00efve Approach : Na\u00efvely using IS for each \\(X_{i}\\)</p> <p>To improve estimation of \\(\\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]\\), consider importance sampling (IS) with sampling distribution \\(Z_{i} \\sim q_{i}(z)\\) :</p> \\[ \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\approx p_{\\theta}\\left(X_{i} \\mid Z_{i}\\right) \\frac{p_{Z}\\left(Z_{i}\\right)}{q_{i}\\left(Z_{i}\\right)} \\] <p>Optimal IS sampling distribution</p> \\[ q_{i}^{\\star}(z)=\\frac{p_{\\theta}\\left(X_{i} \\mid z\\right) p_{Z}(z)}{\\textcolor{red}{p_{\\theta}\\left(X_{i}\\right)}}=\\textcolor{red}{p_{\\theta}\\left(z \\mid X_{i}\\right)} \\] <p>To clarify, optimal sampling distribution depends on \\(X_{i}\\). To clarify, \\(\\textcolor{red}{p_{\\theta}\\left(X_{i}\\right)}\\) is the unkown normalizing factor so \\(\\textcolor{red}{p_{\\theta}\\left(z \\mid X_{i}\\right)}\\) is also unkown. We call \\(q_{i}^{\\star}(z)=p_{\\theta}\\left(z \\mid X_{i}\\right)\\) the true posterior distribution and we will soon consider the approximation \\(q_{\\phi}(z \\mid x) \\approx p_{\\theta}(z \\mid x)\\), which we call the approximate posterior.</p> <p>For each \\(X_{i}\\), let \\(q_i(z)\\) be the optimal approximate posterior dependent on \\(X_i\\), and consider</p> \\[ \\begin{gathered} \\underset{q_{i}}{\\operatorname{minimize}} D_{\\mathrm{KL}}\\left(q_{i}(\\cdot) \\| \\textcolor{red}{p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)} \\right)\\\\ =\\underset{q_{i}}{\\operatorname{minimize}} \\mathbb{E}_{Z \\sim q_{i}} \\log \\left(\\frac{q_{i}(Z)}{\\textcolor{red}{p_{\\theta}\\left(Z \\mid X_{i}\\right)}}\\right) \\\\ =\\underset{q_{i}}{\\operatorname{minimize}} \\mathbb{E}_{Z \\sim q_{i}} \\log \\left(\\frac{q_{i}(Z)}{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z) / \\textcolor{red}{p_{\\theta}\\left(X_{i}\\right)}}\\right) \\\\ =\\underset{Z \\sim q_{i}}{\\operatorname{minimize}}\\left[\\log q_{i}(Z)-\\log p_{Z}(Z)-\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]+\\textcolor{red}{\\log p_{\\theta}\\left(X_{i}\\right)} \\end{gathered} \\] <p>Note, \\(q_{i}(z), p_{Z}(z)\\), and \\(p_{\\theta}(x \\mid z)\\) are tractable/known while \\(\\textcolor{red}{p_{\\theta}\\left(X_{i}\\right)}\\) and \\(\\textcolor{red}{p_{\\theta}\\left(z \\mid X_{i}\\right)}\\) are intractable/unknown. Since \\(\\textcolor{red}{\\log p_{\\theta}\\left(X_{i}\\right)}\\) does not depend on \\(q_{i}\\), all quantities needed in the optimization problems are tractable. However, solving this minimization problem to obtain each \\(q_{i}\\) for each data point \\(X_{i}\\) is computationally too expensive.</p> <p>Individual inference (not amortized): For each \\(X_{1}, \\ldots, X_{N}\\), find corresponding optimal \\(q_{1}, \\ldots, q_{N}\\) by solving</p> \\[ \\underset{q_{i}}{\\operatorname{minimize}} \\quad D_{\\mathrm{KL}}\\left(q_{i}(\\cdot) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right) \\] <p>This is expensive as it requires solving \\(N\\) separate optimization problems.</p> <p>We need variational approach and amortized inference.</p> <p>Concept 11.10 : Variational Approach and Amortized Inference</p> <p>General principle of variational approach: We can't directly use the \\(q\\) we want. So, instead, we propose a parameterized distribution \\(q_{\\phi}\\) that we can work with easily (in this case, sample from easily), and find a parameter setting that makes it as good as possible.</p> <p>Parametrization of VAE:</p> \\[ q_{\\phi}\\left(z \\mid X_{i}\\right) \\approx q_{i}^{\\star}(z)=p_{\\theta}\\left(z \\mid X_{i}\\right) \\quad \\text { for all } i=1, \\ldots, N \\] <p>Amortized inference: Train a neural network \\(q_{\\phi}(\\cdot \\mid x)\\) such that \\(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right)\\) approximates the optimal \\(q_{i}(\\cdot)\\).</p> \\[ \\underset{\\phi \\in \\Phi}{\\operatorname{minimize}} \\sum_{i=1}^{N} D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right) \\] <p>Approximation \\(q_{\\phi}\\left(z \\mid X_{i}\\right) \\approx p_{\\theta}\\left(z \\mid X_{i}\\right)\\) is often less precise than that of individual inference \\(q_{i}(z) \\approx\\) \\(p_{\\theta}\\left(z \\mid X_{i}\\right)\\), but amortized inference is often significantly faster.</p> <p>Concept 11.11 : Encoder \\(q_{\\phi}\\) Optimization</p> <p>In analogy with autoencoders, we call \\(q_{\\phi}\\) the encoder.</p> <p>Optimization problem for encoder (derived from Concept 11.9) :</p> \\[ \\begin{aligned} &amp; \\underset{\\phi \\in \\Phi}{\\operatorname{minimize}} \\sum_{i=1}^{N} D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right) \\\\ = &amp; \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right)\\right]+\\text { constant independent of } \\phi \\\\ = &amp; \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]-D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right) \\end{aligned} \\] <p>Concept 11.12 : Decoder \\(p_{\\theta}\\) Optimization</p> <p>In analogy with autoencoders, we call \\(p_{\\theta}\\) the decoder. Perform approximate MLE (derived from IS, Choice 2 of Concept 11.6) :</p> \\[ \\begin{aligned} &amp; \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\\\ \\stackrel{(a)}{\\approx} &amp; \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z_{i}\\right) p_{Z}\\left(Z_{i}\\right)}{q_{\\phi}\\left(Z_{i} \\mid X_{i}\\right)}\\right), \\quad Z_{i} \\sim q_{\\phi}\\left(z \\mid X_{i}\\right) \\\\ \\stackrel{(b)}{\\approx} &amp; \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right)\\right] \\\\ = &amp; \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]-D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right) \\end{aligned} \\] <p>The \\(\\stackrel{(a)}{\\approx}\\) step replaces expectation inside the log with an estimate with \\(Z_{i}\\). The \\(\\stackrel{(b)}{\\approx}\\) step replaces the random variable with the expectation. These steps take \\(\\mathbb{E}_{Z}\\) outside of the log (which can not be normally done). More on this later (Concept 11.14).</p>"},{"location":"books-and-courses/mfdnn/11/#definition-of-vae","title":"Definition of VAE","text":"<p>Definition 11.13 : Variational Lower Bound (VLB)</p> <p>The optimization objectives for the encoder (Concept 11.11) and decoder (Concept 11.12) are the same!</p> <p>Simultaneously train \\(p_{\\theta}\\) and \\(q_{\\phi}\\) by solving</p> \\[ \\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\underbrace{\\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]-D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right)}_{\\stackrel{\\text { def }}{=} \\mathrm{VLB}_{\\theta, \\phi}\\left(X_{i}\\right)} \\] <p>We refer to the optimization objective as the variational lower bound (VLB) or evidence lower bound (ELBO) for reasons that will be explained soon (Concept 11.14).</p> <p>Concept 11.14 : How tight lower bound is the VLB?</p> <p>How accurate is the approximation?</p> \\[ \\begin{aligned} &amp; \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\frac{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right] \\\\ &amp; \\stackrel{?}{\\approx} \\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right)\\right] \\\\ &amp;=\\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right) \\end{aligned} \\] <p>This turns out that </p> \\[ \\log p_{\\theta}\\left(X_{i}\\right) \\geq \\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right) \\] <p>So we are maximizing a lower bound of the log likelihood. How large is the gap?</p> <ul> <li>Log-likelihood \\(\\geq\\) VLB: Derivation 1</li> </ul> <p>Proof</p> <p>Derivation via Jensen inequality:</p> \\[ \\begin{aligned} \\log p_{\\theta}\\left(X_{i}\\right) &amp; =\\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\\\ &amp; =\\log \\left(\\mathbb{E}_{Z \\sim q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right) \\frac{p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right]\\right) \\\\ &amp; \\geq \\mathbb{E}_{Z \\sim q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\left[\\log \\left(p_{\\theta}\\left(X_{i} \\mid Z\\right) \\frac{p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right)\\right] \\\\ &amp; \\stackrel{\\text { def }}{=} \\mathrm{VLB}_{\\theta, \\phi}\\left(X_{i}\\right) \\end{aligned} \\] <p>Does not explicitly characterize gap.</p> <ul> <li>Log-likelihood \\(\\geq\\) VLB: Derivation 2</li> </ul> <p>Proof</p> <p>Derivation via KL divergence:</p> \\[ \\begin{aligned} D_{\\mathrm{KL}}\\left[q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right] &amp; =\\mathbb{E}_{Z \\sim q_{\\theta}\\left(z \\mid X_{i}\\right)}\\left[\\log q_{\\theta}\\left(Z \\mid X_{i}\\right)-\\log p_{\\theta}\\left(Z \\mid X_{i}\\right)\\right] \\\\ &amp; =\\underbrace{\\mathbb{E}_{Z \\sim q_{\\theta}\\left(z \\mid X_{i}\\right)}\\left[\\log q_{\\theta}\\left(Z \\mid X_{i}\\right)-\\log p_{Z}(Z)-\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]}_{=-\\mathrm{VLB}_{\\theta, \\phi}\\left(X_{i}\\right)}+\\log p_{\\theta}\\left(X_{i}\\right) \\end{aligned} \\] <p>and</p> \\[ \\begin{aligned} \\log p_{\\theta}\\left(X_{i}\\right) &amp;= \\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right)+\\underbrace{D_{\\mathrm{KL}}\\left[q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right]}_{\\geq 0} \\\\ &amp; \\geq \\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right) \\end{aligned} \\] <p>This derivation explicitly characterizes the gap as \\(D_{\\mathrm{KL}}\\left[q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right]\\).</p> \\[ \\log p_{\\theta}\\left(X_{i}\\right) - \\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right) = D_{\\mathrm{KL}}\\left[q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right] \\] <p>Concept 11.15 : VLB is tight if encoder is infinitely powerful.</p> <p>If the encoder \\(q_{\\phi}\\) is powerful enough such that there is a \\(\\phi^{\\star}\\) achieving</p> \\[ q_{\\phi^{\\star}}\\left(\\cdot \\mid X_{i}\\right)=p_{\\theta}\\left(\\cdot \\mid X_{i}\\right) \\] <p>or equivalently</p> \\[ D_{\\mathrm{KL}}\\left[q_{\\phi^{\\star}}\\left(\\cdot \\mid X_{i}\\right) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right]=0 \\] <p>Then</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right) \\] <p>Definition 11.16 : Variational Autoencoder (VAE) Terminology</p> <p> </p> <ul> <li>Likelihood : \\(\\textcolor{red}{p_{\\theta}(x)}\\) (exact evaluation intractable)</li> <li>Prior : \\(p_{Z}(z)\\)</li> <li>Conditional distribution (decoder) : \\(p_{\\theta}(x \\mid z)\\)</li> <li>True posterior : \\(\\textcolor{red}{p_{\\theta}(z \\mid x)}\\) (exact evaluation intractable)</li> <li>Approximate posterior (encoder) : \\(q_{\\phi}(z \\mid x)\\)</li> </ul> <p>Conditional distribution \\(p_{\\theta}(x \\mid z)\\) and prior \\(p_{Z}(z)\\) determines the posterior \\(p_{\\theta}(z \\mid x)\\).</p> <p>There is no easy way to evaluate \\(p_{\\theta}(x)\\), but we can sample \\(X \\sim p_{\\theta}(x)\\) easily: \\(Z \\sim p_{Z}(z)\\) then \\(X \\sim p_{\\theta}(x \\mid Z)\\).</p> <p>NN in VAE do not directly generate random output. NN outputs parameters for random sampling.</p>"},{"location":"books-and-courses/mfdnn/11/#vae-standard-instance","title":"VAE Standard Instance","text":"<p>Definition 11.17 : VAE Standard Instance</p> <p>A standard VAE setup:</p> \\[ \\begin{aligned} &amp; p_{Z}=\\mathcal{N}(0, I) \\\\ &amp; q_{\\phi}(z \\mid x)=\\mathcal{N}\\left(\\mu_{\\phi}(x), \\Sigma_{\\phi}(x)\\right) \\text { with diagonal } \\Sigma_{\\phi} \\\\ &amp; p_{\\theta}(x \\mid z)=\\mathcal{N}\\left(f_{\\theta}(z), \\sigma^{2} I\\right) \\end{aligned} \\] <p>\\(\\mu_{\\phi}(x), \\Sigma_{\\phi}^{2}(x)\\), and \\(f_{\\theta}(z)\\) are deterministic NN.</p> <p>Using the following equation,</p> \\[ \\begin{aligned} &amp; D_{\\mathrm{KL}}\\left(\\mathcal{N}\\left(\\mu_{\\phi}(X), \\Sigma_{\\phi}(X)\\right) \\| \\mathcal{N}(0, I)\\right) \\\\ = &amp; \\frac{1}{2}\\left(\\operatorname{tr}\\left(\\Sigma_{\\phi}(X)\\right)+\\left\\|\\mu_{\\phi}(X)\\right\\|^{2}-d-\\log \\operatorname{det}\\left(\\Sigma_{\\phi}(X)\\right)\\right) \\\\ \\end{aligned} \\] <p>the training objective</p> \\[ \\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]-D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right) \\] <p>becomes</p> \\[ \\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{minimize}} \\sum_{i=1}^{N} \\frac{1}{\\sigma^{2}} \\mathbb{E}_{Z \\sim \\mathcal{N}\\left(\\mu_{\\phi}\\left(X_{i}\\right), \\Sigma_{\\phi}\\left(X_{i}\\right)\\right)}\\left\\|X_{i}-f_{\\theta}(Z)\\right\\|^{2}+\\operatorname{tr}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right)+\\left\\|\\mu_{\\phi}\\left(X_{i}\\right)\\right\\|^{2}-\\log \\operatorname{det}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right) \\] <p>Concept 11.18 : VAE Standard Instance with Reparameterization Trick</p> <p>The standard instance of VAE</p> \\[ \\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{minimize}} \\sum_{i=1}^{N} \\frac{1}{\\sigma^{2}} \\mathbb{E}_{Z \\sim \\mathcal{N}\\left(\\mu_{\\phi}\\left(X_{i}\\right), \\Sigma_{\\phi}\\left(X_{i}\\right)\\right)}\\left\\|X_{i}-f_{\\theta}(Z)\\right\\|^{2}+\\operatorname{tr}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right)+\\left\\|\\mu_{\\phi}\\left(X_{i}\\right)\\right\\|^{2}-\\log \\operatorname{det}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right) \\] <p>can be equivalently written with the reparameterization trick</p> \\[ \\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{minimize}} \\sum_{i=1}^{N} \\frac{1}{\\sigma^{2}} \\mathbb{E}_{\\varepsilon \\sim \\mathcal{N}(0, I)}\\left\\|X_{i}-f_{\\theta}\\left(\\mu_{\\phi}\\left(X_{i}\\right)+\\Sigma_{\\phi}^{1 / 2}\\left(X_{i}\\right) \\varepsilon\\right)\\right\\|^{2}+\\operatorname{tr}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right)+\\left\\|\\mu_{\\phi}\\left(X_{i}\\right)\\right\\|^{2}-\\log \\operatorname{det}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right) \\] <p>where \\(\\Sigma_{\\phi}^{1 / 2}\\) is diagonal with \\(\\sqrt{\\cdot}\\) of the diagonal elements of \\(\\Sigma_{\\phi}\\). (Remember, \\(\\Sigma_{\\phi}\\) is diagonal.)</p> <p>To clarify \\(Z \\stackrel{\\mathcal{D}}{=} \\mu_{\\phi}\\left(X_{i}\\right)+\\Sigma_{\\phi}^{1 / 2}\\left(X_{i}\\right) \\varepsilon\\), where \\(\\stackrel{\\mathcal{D}}{=}\\) denotes equality in distribution.</p> <p>We now have an objective amenable to stochastic optimization.</p> <p>Concept 11.19 : VAE Standard Instance Architecture</p> <ul> <li>Training (Without reparameterization trick)</li> </ul> <p> </p> <ul> <li>Training (With reparameterization trick)</li> </ul> <p> </p> <ul> <li> <p>Sampling</p> <p>During sampling, only the decoder network is used.</p> </li> </ul> <p> </p> <p>Concept 11.20 : Why variational \"autoencoder\"?</p> <p>VAE loss (VLB) contains a reconstruction loss resembling that of an autoencoder.</p> \\[ \\begin{aligned} \\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right) &amp; =\\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]-D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right) \\\\ &amp; =-\\frac{1}{2 \\sigma^{2}} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\left\\|X_{i}-f_{\\theta}(Z)\\right\\|^{2}\\right]-D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right) \\\\ &amp; =-\\underbrace{\\frac{1}{2 \\sigma^{2}} \\mathbb{E}_{\\varepsilon \\sim \\mathcal{N}(0, I)}\\left\\|X_{i}-f_{\\theta}\\left(\\mu_{\\phi}\\left(X_{i}\\right)+\\Sigma_{\\phi}^{1 / 2}\\left(X_{i}\\right) \\varepsilon\\right)\\right\\|^{2}}_{\\text {Reconstruction loss }}-\\underbrace{D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right)}_{\\text {Regularization }} \\end{aligned} \\] <p>VLB also contains a regularization term on the output of the encoder, which is not present in standard autoencoder losses.</p> <p>The choice of \\(\\sigma\\) determines the relative weight between the reconstruction loss and the regularization.</p>"},{"location":"books-and-courses/mfdnn/11/#training-vae","title":"Training VAE","text":"<p>Concept 11.21 : Training VAE with RT</p> <p>To obtain stochastic gradients of the VAE standard instance </p> \\[ \\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{minimize}} \\sum_{i=1}^{N} \\frac{1}{\\sigma^{2}} \\mathbb{E}_{\\varepsilon \\sim \\mathcal{N}(0, I)}\\left\\|X_{i}-f_{\\theta}\\left(\\mu_{\\phi}\\left(X_{i}\\right)+\\Sigma_{\\phi}^{1 / 2}\\left(X_{i}\\right) \\varepsilon\\right)\\right\\|^{2}+\\operatorname{tr}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right)+\\left\\|\\mu_{\\phi}\\left(X_{i}\\right)\\right\\|^{2}-\\log \\operatorname{det}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right) \\] <p>select a data \\(X_{i}\\), sample \\(\\varepsilon_{i} \\sim \\mathcal{N}(0, I)\\), evaluate</p> \\[ -\\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}, \\varepsilon_{i}\\right) \\stackrel{\\text { def }}{=} \\frac{1}{\\sigma^{2}}\\left\\|X_{i}-f_{\\theta}\\left(\\mu_{\\phi}\\left(X_{i}\\right)+\\Sigma_{\\phi}^{1 / 2}\\left(X_{i}\\right) \\varepsilon_{i}\\right)\\right\\|^{2}+\\operatorname{tr}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right)+\\left\\|\\mu_{\\phi}\\left(X_{i}\\right)\\right\\|^{2}-\\log \\operatorname{det}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right) \\] <p>and backprop on \\(\\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}, \\varepsilon_{i}\\right)\\).</p> <p>Usually, batch of \\(X_{i}\\) is selected. One can sample multiple \\(Z_{i, 1}, \\ldots, Z_{i, K}\\) (equivalently \\(\\varepsilon_{i, 1}, \\ldots, \\varepsilon_{i, K}\\) ) for each \\(X_{i}\\).</p> <p>Concept 11.22 : Traning VAE with Log-Derivative Trick</p> <p>Computing stochastic gradients without the reparameterization trick.</p> \\[ \\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\underbrace{\\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right)\\right]}_{\\stackrel{\\text { def }}{=} \\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right)} \\] <p>To obtain unbiased estimates of \\(\\nabla_{\\theta}\\), compute</p> \\[ \\frac{1}{K} \\sum_{k=1}^{K} \\log p_{\\theta}\\left(X_{i} \\mid Z_{i, k}\\right), \\quad Z_{i, 1}, \\ldots, Z_{i, K} \\sim q_{\\phi}\\left(z \\mid X_{i}\\right) \\] <p>and backprop with respect to \\(\\theta\\).</p> <p>We differentiate the VLB objectives</p> \\[ \\begin{aligned} \\nabla_{\\phi} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right)\\right] &amp; =\\nabla_{\\phi} \\int \\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid z\\right) p_{Z}(z)}{q_{\\phi}\\left(z \\mid X_{i}\\right)}\\right) q_{\\phi}\\left(z \\mid X_{i}\\right) d z \\\\ &amp; =\\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\left(\\nabla_{\\phi} \\log q_{\\phi}\\left(Z \\mid X_{i}\\right)\\right) \\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right)\\right] \\end{aligned} \\] <p>To obtain unbiased estimates of \\(\\nabla_{\\phi}\\), compute</p> \\[ \\frac{1}{K} \\sum_{k=1}^{K}\\left(\\nabla_{\\phi} \\log q_{\\phi}\\left(Z_{i, k} \\mid X_{i}\\right)\\right) \\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z_{i, k}\\right) p_{Z}\\left(Z_{i, k}\\right)}{q_{\\phi}\\left(Z_{i, k} \\mid X_{i}\\right)}\\right), \\quad Z_{i, 1}, \\ldots, Z_{i, K} \\sim q_{\\phi}\\left(z \\mid X_{i}\\right) \\]"},{"location":"books-and-courses/mfdnn/11/#researches","title":"Researches","text":"<p>Concept 11.23 : VQ-VAE</p> <p> </p> <p>(A. van den Oord, O. Vinyals, and K. Kavukcuoglu, Neural discrete representation learning, NeurIPS, 2017.)</p> <p>Concept 11.24 : VQ-VAE-2</p> <p> </p> <p>(A. Razavi, A. van den Oord, and O. Vinyals, Generating diverse high-fidelity images with VQ-VAE-2, NeurIPS, 2019.)</p> <p>Concept 11.25 : \\(\\beta\\)-VAE</p> <p>Uses the loss</p> \\[ \\ell_{\\theta, \\phi}\\left(X_{i}\\right)=\\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]-\\beta D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right) \\] <p>when \\(\\beta=1, \\ell_{\\theta, \\phi}\\left(X_{i}\\right)=\\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right)\\), i.e., \\(\\beta\\)-VAE coincides with VAE when \\(\\beta=1\\).</p> <p>With \\(\\beta&gt;1\\), authors observed better feature disentanglement.</p> <p>(I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner, \u03b2-VAE: Learning basic visual concepts with a constrained variational framework, ICLR, 2017.)</p>"},{"location":"books-and-courses/mfdnn/12/","title":"\u00a7 12. Generative Adversarial Networks","text":""},{"location":"books-and-courses/mfdnn/12/#minimax-optimization","title":"Minimax Optimization","text":"<p>Definition 12.1 : Minimax Optimization Problem</p> <p>In a minimax optimization problem we minimize with respect to one variable and maximize with respect to another:</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathcal{L}(\\theta, \\phi) \\] <p>We say \\(\\left(\\theta^{\\star}, \\phi^{\\star}\\right)\\) is a solution to the minimax problem if \\(\\theta^{\\star} \\in \\Theta, \\phi^{\\star} \\in \\Phi\\), and</p> \\[ \\mathcal{L}\\left(\\theta^{\\star}, \\phi\\right) \\leq \\mathcal{L}\\left(\\theta^{\\star}, \\phi^{\\star}\\right) \\leq \\mathcal{L}\\left(\\theta, \\phi^{\\star}\\right), \\quad \\forall \\theta \\in \\Theta, \\phi \\in \\Phi . \\] <p>In other words, unilaterally deviating from \\(\\theta^{\\star} \\in \\Theta\\) increases the value of \\(\\mathcal{L}(\\theta, \\phi)\\) while unilaterally deviating from \\(\\phi^{\\star} \\in \\Phi\\) decreases the value of \\(\\mathcal{L}(\\theta, \\phi)\\). In yet other words, the solution is defined as a Nash equilibrium in a 2-player zero-sum game.</p> <p>(There are other broader definitions of a \u201csolution\u201d in minimax optimization problems. Our definition is, in a sense, the strictest definition.)</p> <p>Concept 12.2 : Minimax vs. Maximin</p> <p>When a solution (as we defined in Definition 12.1) does not exist, then min-max is not the same as max-min:</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathcal{L}(\\theta, \\phi) \\neq \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\mathcal{L}(\\theta, \\phi) \\] <p>This is a technical distinction that we will not explore in this class.</p> <p>Concept 12.3 : Minimax Optimization</p> <p>So far, we trained NN by solving minimization problems.</p> <p>However, GANs are trained by solving minimax problems. Since the advent of GANs, minimax training has become more widely used in all areas of deep learning.</p> <p>Examples:</p> <ul> <li>Adversarial training to make NN robust against adversarial attacks.</li> <li>Domain adversarial networks to train NN to make fair decisions (e.g. not base its decision on a persons race or gender).</li> </ul> <p>Definition 12.4 : Minimax Optimization Algorithm</p> <p>First, consider deterministic gradient setup. Let \\(\\alpha\\) and \\(\\beta\\) be the stepsizes (learning rates) for the descent and ascent steps respectively.</p> <ul> <li> <p>Simultaneous gradient ascent-descent</p> \\[ \\begin{aligned} \\phi^{k+1} &amp; =\\phi^{k}+\\beta \\nabla_{\\phi} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k}\\right) \\\\ \\theta^{k+1} &amp; =\\theta^{k}-\\alpha \\nabla_{\\theta} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k}\\right) \\end{aligned} \\] </li> <li> <p>Alternating gradient ascent-descent</p> \\[ \\begin{aligned} \\phi^{k+1} &amp; =\\phi^{k}+\\beta \\nabla_{\\phi} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k}\\right) \\\\ \\theta^{k+1} &amp; =\\theta^{k}-\\alpha \\nabla_{\\theta} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k+1}\\right) \\end{aligned} \\] </li> <li> <p>Gradient multi-ascent-single-descent</p> \\[ \\begin{aligned} \\phi_{0}^{k+1} &amp; =\\phi_{n_{\\mathrm{dis}}}^{k} \\\\ \\phi_{i+1}^{k+1} &amp; =\\phi_{i}^{k+1}+\\beta \\nabla_{\\phi} \\mathcal{L}\\left(\\theta^{k}, \\phi_{i}^{k+1}\\right), \\quad \\text { for } i=0, \\ldots, n_{\\mathrm{dis}}-1 \\\\ \\theta^{k+1} &amp; =\\theta^{k}-\\alpha \\nabla_{\\theta} \\mathcal{L}\\left(\\theta^{k}, \\phi_{n_{\\mathrm{dis}}}^{k+1}\\right) \\end{aligned} \\] <p>(\\(n_{\\text {dis }}\\) stands for number of discriminator updates.) When \\(n_{\\text {dis }}=1\\), this algorithm reduces to alternating ascent-descent.</p> </li> </ul> <p>Definition 12.5 : Stochastic Minimax Optimization</p> <p>In deep learning, however, we have access to stochastic gradients.</p> <ul> <li>Stochastic gradient simultaneous ascent-descent</li> </ul> \\[ \\begin{aligned} \\phi^{k+1} &amp; =\\phi^{k}+\\beta g_{\\phi}^{k}, &amp; \\mathbb{E}\\left[g_{\\phi}^{k}\\right]=\\nabla_{\\phi} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k}\\right) \\\\ \\theta^{k+1} &amp; =\\theta^{k}-\\alpha g_{\\theta}^{k}, &amp; \\mathbb{E}\\left[g_{\\theta}^{k}\\right]=\\nabla_{\\theta} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k}\\right) \\end{aligned} \\] <ul> <li>Stochastic gradient alternating ascent-descent</li> </ul> \\[ \\begin{aligned} \\phi^{k+1} &amp; =\\phi^{k}+\\beta g_{\\phi}^{k}, &amp; \\mathbb{E}\\left[g_{\\phi}^{k}\\right]=\\nabla_{\\phi} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k}\\right) \\\\ \\theta^{k+1} &amp; =\\theta^{k}-\\alpha g_{\\theta}^{k}, &amp; \\mathbb{E}\\left[g_{\\theta}^{k}\\right]=\\nabla_{\\theta} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k+1}\\right) \\end{aligned} \\] <ul> <li>Stochastic gradient multi-ascent-single-descent</li> </ul> \\[ \\begin{aligned} \\phi_{0}^{k+1} &amp; =\\phi_{n_{\\text {dis }}}^{k} \\\\ \\phi_{i+1}^{k+1} &amp; =\\phi_{i}^{k+1}+\\beta \\nabla_{\\phi} g_{\\phi}^{k, i}, \\quad \\mathbb{E}\\left[g_{\\phi}^{k, i}\\right]=\\nabla_{\\phi} \\mathcal{L}\\left(\\theta^{k}, \\phi_{i}^{k+1}\\right), \\quad \\text { for } i=0, \\ldots, n_{\\text {dis }}-1 \\\\ \\theta^{k+1} &amp; =\\theta^{k}-\\alpha g_{\\theta}^{k}, \\quad \\mathbb{E}\\left[g_{\\theta}^{k}\\right]=\\nabla_{\\theta} \\mathcal{L}\\left(\\theta^{k}, \\phi_{n_{\\text {dis }}^{k}}^{k+1}\\right) \\end{aligned} \\] <p>Concept 12.6 : Minimax Optimization in Pytorch</p> <p>To perform minimax optimization in PyTorch, we maintain two separate optimizers, one for the ascent, one for the descent. The <code>OPTIMIZER</code> can be anything like <code>SGD</code> or <code>Adam</code>.</p> <pre><code>G = Generator(...).to(device)\nD = Discriminator(...).to(device)\n\nD_optimizer = optim.OPTIMIZER(D.parameters(), lr = beta)\nG_optimizer = optim.OPTIMIZER(G.parameters(), lr = alpha)\n</code></pre> <ul> <li>Simultaneous ascent-descent:     <pre><code>Evaluate D_loss\nD_loss.backward()\nEvaluate G_loss\nG_loss.backward()\n\nD_optimizer.step()\nG_optimizer.step()\n</code></pre></li> <li>Alternating ascent-descent     <pre><code>Evaluate D_loss\nD_loss.backward()\nD_optimizer.step()\n\nEvaluate G_loss\nG_loss.backward()\nG_optimizer.step()\n</code></pre></li> <li>Multi-ascent-single-descent     <pre><code>for _ in range(ndis) :\n    Evaluate D loss\n    D_loss.backward()\n    D_optimizer.step()\n\nEvaluate G_loss\nG_loss.backward()\nG_optimizer.step()\n</code></pre></li> </ul>"},{"location":"books-and-courses/mfdnn/12/#definition-of-gan","title":"Definition of GAN","text":"![](./assets/12.1.png){: width=\"100%\"}  <p>These are synthetic (fake) images made with GAN.</p> <p>(A. Brock, J. Donahue, and K. Simonyan, Large scale GAN training for high fidelity natural image synthesis, ICLR, 2019.)</p> <p>Definition 12.7 : Generative Adversarial Networks (GAN)</p> <p>In generative adversarial networks (GAN) a generator network and a discriminator network compete adversarially.</p> <p> </p> <p>Given data \\(X_{1}, \\ldots, X_{N} \\sim p_{\\text {true }}\\), GAN aims to learn \\(p_{\\theta} \\approx p_{\\text {true }}\\).</p> <p>Generator aims to generate fake data similar to training data.</p> <p>Discriminator aims to distinguish the training data from fake data.</p> <p>Analogy: Criminal creating fake money vs. police distinguishing fake money from real.</p> <p>(I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, Generative adversarial networks, NeurIPS, 2014.)</p> <p>Definition 12.8 : Generator Network</p> <p>The generator \\(G_{\\theta}: \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{n}\\) is a neural network parameterized by \\(\\theta \\in \\Theta\\). The generator takes a random latent vector \\(Z \\sim p_{Z}\\) as input and outputs generated (fake) data \\(\\tilde{X}=G_{\\theta}(Z)\\). The latent distribution is usually \\(p_{Z}=\\mathcal{N}(0, I)\\).</p> <p>Write \\(p_{\\theta}\\) for the probability distribution of \\(\\tilde{X}=G_{\\theta}(Z)\\). Although we can't evaluate the density \\(p_{\\theta}(x)\\), neither exactly nor approximately, we can sample from \\(\\tilde{X} \\sim p_{\\theta}\\).</p> <p>Definition 12.9 : Discriminator Network</p> <p>The discriminator \\(D_{\\phi}: \\mathbb{R}^{n} \\rightarrow(0,1)\\) is a neural network parameterized by \\(\\phi \\in \\Phi\\). The discriminator takes an image \\(X\\) as input and outputs whether \\(X\\) is a real or fake. (Real : \\(X\\) comes from a data set, i.e., \\(X \\sim p_{\\text{true}}\\). Fake : generated by \\(G_{\\theta}\\), i.e., \\(X \\sim p_{\\theta}\\).)</p> <ul> <li>\\(D_{\\phi}(X) \\approx 1\\) : discriminator confidently predicts \\(X\\) is real.</li> <li>\\(D_{\\phi}(X) \\approx 0\\) : discriminator confidently predicts \\(X\\) is fake.</li> <li>\\(D_{\\phi}(X) \\approx 0.5\\) : discriminator is unsure whether \\(X\\) is real or fake.</li> </ul> <p>Concept 12.10 : Discriminator Loss</p> <p>Cost of incorrectly classifying real as fake (type I error):</p> \\[ \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[-\\log D_{\\phi}(X)\\right] \\] <p>Cost of incorrectly classifying fake as real (type II error):</p> \\[ \\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}\\left[-\\log \\left(1-D_{\\phi}(\\tilde{X})\\right)\\right]=\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[-\\log \\left(1-D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\] <p>Discriminator solves</p> \\[ \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\quad \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log D_{\\phi}(X)\\right]+\\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}\\left[\\log \\left(1-D_{\\phi}(\\tilde{X})\\right)\\right] \\] <p>which is equivalent to</p> \\[ \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\quad \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log D_{\\phi}(X)\\right]+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[\\log \\left(1-D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\] <p>We can view</p> \\[ \\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}\\left[\\log \\left(1-D_{\\phi}(\\tilde{X})\\right)\\right]=\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[\\log \\left(1-D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\] <p>as an instance of the reparameterization technique.</p> <p>The loss</p> \\[ \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log D_{\\phi}(X)\\right]+\\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}\\left[\\log \\left(1-D_{\\phi}(\\tilde{X})\\right)\\right] \\] <p>puts equal weight on type I and type II errors. Alternatively, one can use the loss</p> \\[ \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log D_{\\phi}(X)\\right]+\\lambda \\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}\\left[\\log \\left(1-D_{\\phi}(\\tilde{X})\\right)\\right] \\] <p>where \\(\\lambda&gt;0\\) represents the relative significance of a type II error over a type I error.</p> <p>Concept 12.11 : Generator Loss</p> <p>Since the goal of the generator is to deceive the discriminator, the generator minimizes the same loss as Concept 12.10.</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log D_{\\phi}(X)\\right]+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[\\log \\left(1-D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\] <p>(The generator and discriminator operate under a zero-sum game.)</p> <p>Note, only the second term depend on \\(\\theta\\), while the both terms depend on \\(\\phi\\).</p> <p>Concept 12.12 : Empirical Risk Minimization for Discriminator / Generator</p> <p>In practice, we have finite samples \\(X_{1}, \\ldots, X_{N}\\), so we instead use the loss</p> \\[ \\frac{1}{N} \\sum_{i=1}^{N} \\log D_{\\phi}\\left(X_{i}\\right)+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[\\log \\left(1-D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\] <p>Since \\(\\tilde{X}=G_{\\theta}(Z)\\) is generated with \\(Z \\sim p_{Z}\\), we have unlimited \\(\\tilde{X}\\) samples. So we replace \\(\\mathbb{E}_{X} \\approx \\frac{1}{N} \\sum\\) while leaving \\(\\mathbb{E}_{Z}\\) as is.</p> <p>Definition 12.13 : Minimax Training (Zero-Sum Game) for GAN</p> <p>Train generator and discriminator simultaneously by solving</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathcal{L}(\\theta, \\phi) \\] <p>where</p> \\[ \\mathcal{L}(\\theta, \\phi)=\\frac{1}{N} \\sum_{i=1}^{N} \\log D_{\\phi}\\left(X_{i}\\right)+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[\\log \\left(1-D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\] <p>It remains to specify the architectures for \\(G_{\\theta}\\) and \\(D_{\\phi}\\).</p> <p>Definition 12.14 : DCGAN</p> <p>The original GAN was also deep and convolutional. However, Radford et al.'s Deep Convolutional Generative Adversarial Networks (DCGAN) paper proposed the following architectures, which crucially utilize batchnorm.</p> <p>Use batchnorm in both the generator and the discriminator after transposed conv and conv layers.</p> <p> </p> <p>(A. Radford, L. Metz, and S. Chintala, Unsupervised representation learning with deep convolutional generative adversarial networks, ICLR, 2016.)</p>"},{"location":"books-and-courses/mfdnn/12/#f-gan","title":"f-GAN","text":"<p>Definition 12.15 : f-Divergence</p> <p>The f-divergence of \\(p\\) from \\(q\\), where \\(f\\) is a convex function such that \\(f(1)=0\\), is</p> \\[ D_{f}(p \\| q)=\\int f\\left(\\frac{p(x)}{q(x)}\\right) q(x) d x, \\] <p>This includes the KL divergence:</p> <ul> <li>If \\(f(u)=u \\log u\\), then \\(D_{f}(p \\| q)=D_{\\mathrm{KL}}(p \\| q)\\).</li> <li>If \\(f(u)=-\\log u\\), then \\(D_{f}(p \\| q)=D_{\\text {KL }}(q \\| p)\\).</li> </ul> <p>Definition 12.16 : JS-Divergence</p> <p>Jensen-Shannon-divergence (JS-divergence) is</p> \\[ D_{\\mathrm{JS}}(p, q)=\\frac{1}{2} D_{\\mathrm{KL}}\\left(p \\| \\frac{1}{2}(p+q)\\right)+\\frac{1}{2} D_{\\mathrm{KL}}\\left(q \\| \\frac{1}{2}(p+q)\\right) \\] <p>With, \\(f(u)=\\left\\{\\begin{array}{ll}\\frac{1}{2}\\left(u \\log u-(u+1) \\log \\frac{u+1}{2}\\right) &amp; \\text { for } u \\geq 0 \\\\ \\infty &amp; \\text { otherwise }\\end{array}\\right.\\) we have \\(D_{f}=D_{\\mathrm{JS}}\\).</p> <p>With, \\(f(u)=\\left\\{\\begin{array}{ll}u \\log u-(u+1) \\log (u+1)+\\log 4 &amp; \\text { for } u \\geq 0 \\\\ \\infty &amp; \\text { otherwise }\\end{array}\\right.\\) we have \\(D_{f}=2 D_{\\mathrm{JS}}\\).</p> <p>Concept 12.17 : GAN \\(\\approx\\) JSD minimization</p> <p>Let us understand the minimax problem</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathcal{L}(\\theta, \\phi) \\] <p>via the minimization problem</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\mathcal{J}(\\theta) \\] <p>where</p> \\[ \\mathcal{J}(\\theta)=\\sup _{\\phi \\in \\Phi} \\mathcal{L}(\\theta, \\phi) \\] <p>For simplicity, assume the discriminator is infinitely powerful, i.e., \\(D_{\\phi}(x)\\) can represent any arbitrary function.</p> <p>Note</p> \\[ \\begin{aligned} \\mathcal{L}(\\theta, \\phi) &amp; =\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log D_{\\phi}(X)\\right]+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[\\log \\left(1-D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\\\ &amp; =\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log D_{\\phi}(X)\\right]+\\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}\\left[\\log \\left(1-D_{\\phi}(\\tilde{X})\\right)\\right] \\\\ &amp; =\\int p_{\\text {true }}(x) \\log D_{\\phi}(x)+p_{\\theta}(x) \\log \\left(1-D_{\\phi}(x)\\right) d x \\end{aligned} \\] <p>Since</p> \\[ \\frac{d}{d y}(a \\log y+b \\log (1-y))=0 \\quad \\Rightarrow \\quad y^{\\star}=\\frac{a}{a+b} \\] <p>The integral is maximized by</p> \\[ D_{\\phi^{\\star}}(x)=\\frac{p_{\\text {true }}(x)}{p_{\\text {true }}(x)+p_{\\theta}(x)} \\] <p>If we plug in the optimal discriminator,</p> \\[ D_{\\phi^{\\star}}(x)=\\frac{p_{\\text {true }}(x)}{p_{\\text {true }}(x)+p_{\\theta}(x)} \\] <p>we get</p> \\[ \\begin{aligned} \\mathcal{L}\\left(\\theta, \\phi^{\\star}\\right) &amp; =\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log \\frac{p_{\\text {true }}(X)}{p_{\\text {true }}(X)+p_{\\theta}(X)}\\right]+\\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}\\left[\\log \\frac{p_{\\theta}(\\tilde{X})}{p_{\\text {true }}(\\tilde{X})+p_{\\theta}(\\tilde{X})}\\right] \\\\ &amp; =2 D_{\\mathrm{JS}}\\left(p_{\\text {true }}, p_{\\theta}\\right)-\\log (4) \\end{aligned} \\] <p>Therefore,</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathcal{L}(\\theta, \\phi) \\approx \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} D_{\\mathrm{JS}}\\left(p_{\\text {true }}, p_{\\theta}\\right) \\] <p>Concept 12.18 : Motivation for f-GAN</p> <p>With GANs, we started from a minimax formulation and later reinterpreted it as minimizing the JS-divergence. (Concept 12.17)</p> <p>Let us instead the start from an f-divergence minimization</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad D_{f}\\left(p_{\\text {true }} \\| p_{\\theta}\\right) \\] <p>and then variationally approximate \\(D_{f}\\) to obtain a minimax formulation.</p> <p>Variational approach: Evaluating \\(D_{f}\\) directly is difficult, so we pose it as a maximization problem and parameterize the maximizing function as a \"discriminator\" neural network.</p> <p>For simplicity, however, we only consider the order</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad D_{f}\\left(p_{\\text {true }} \\| p_{\\theta}\\right) \\] <p>However, one can also consider</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad D_{f}\\left(p_{\\theta} \\| p_{\\text {true }}\\right) \\] <p>to obtain similar results.</p> <p>Definition 12.19 : Convex Conjugate</p> <p>Let \\(f: \\mathbb{R} \\rightarrow \\mathbb{R} \\cup\\{\\infty\\}\\). Define the convex conjugate of \\(f\\) as</p> \\[ f^{*}(t)=\\sup _{u \\in \\mathbb{R}}\\{t u-f(u)\\} \\] <p>where \\(f^{*}: \\mathbb{R} \\rightarrow \\mathbb{R} \\cup\\{\\infty\\}\\). This is also referred to as the Legendre transform.</p> <p>If \\(f\\) is a nice (closed and proper) convex function, then \\(f^{*}\\) is convex and \\(f^{* *}=f\\), i.e., the conjugate of the conjugate is the original function. (So conjugacy is an involution in the space of convex functions.) So</p> \\[ f(u)=\\sup _{t \\in \\mathbb{R}}\\left\\{t u-f^{*}(t)\\right\\} \\] <p>Example 12.20 : Examples of Convex Conjugate</p> <ul> <li> <p>KL</p> \\[ f_{\\mathrm{KL}}(u)= \\begin{cases} u \\log u &amp; \\text { for } u \\geq 0 \\\\ \\infty &amp; \\text { otherwise } \\\\ \\end{cases} \\] \\[ f_{\\mathrm{KL}}^{*}(t)=\\exp(t-1) \\] </li> <li> <p>LK (Reversed KL)</p> \\[ f_{\\mathrm{LK}}(u)= \\begin{cases} - \\log u &amp; \\text { for } u &gt; 0 \\\\ \\infty &amp; \\text { otherwise } \\\\ \\end{cases} \\] \\[ f_{\\mathrm{LK}}^{*}(u)= \\begin{cases} -1-\\log(-t) &amp; \\text { for } t &lt; 0 \\\\ \\infty &amp; \\text { otherwise } \\\\ \\end{cases} \\] </li> <li> <p>SH (Squared Hellinger Distance)</p> \\[ f_{\\mathrm{SH}}(u)=(\\sqrt{u}-1)^2 \\] \\[ f_{\\mathrm{SH}}^{*}(t)= \\begin{cases} \\frac{1}{1/t-1} &amp; \\text { for } t &lt; 1 \\\\ \\infty &amp; \\text { otherwise } \\\\ \\end{cases} \\] </li> <li> <p>JS</p> \\[ f_{\\mathrm{JS}}(u)= \\begin{cases} u\\log u-(u+1) \\log (u+1)+\\log 4 &amp; \\text { for } u \\geq 0 \\\\ \\infty &amp; \\text { otherwise } \\\\ \\end{cases} \\] \\[ f_{\\mathrm{JS}}^{*}(u)= \\begin{cases} -\\log (1-\\exp (t))-\\log 4 &amp; \\text { for } t &lt; 0 \\\\ \\infty &amp; \\text { otherwise } \\\\ \\end{cases} \\] </li> </ul> <p>We get the following f-divergences:</p> \\[ \\begin{aligned} &amp; D_{f_{\\mathrm{KL}}}(p \\| q)=D_{\\mathrm{KL}}(p \\| q) \\\\ &amp; D_{f_{\\mathrm{LK}}}(p \\| q)=D_{\\mathrm{KL}}(q \\| p) \\\\ &amp; D_{f_{\\mathrm{SH}}}(p \\| q)=D_{\\mathrm{SH}}(q, p) \\\\ &amp; D_{f_{\\mathrm{JS}}}(p \\| q)=2 D_{\\mathrm{JS}}(q, p) \\end{aligned} \\] <p>We don't use the following property, but it's interesting so we mention it. If \\(f\\) and \\(f^{*}\\) are differentiable, then</p> \\[ \\left(f^{\\prime}\\right)^{-1}=\\left(f^{*}\\right)^{\\prime} \\] \\[ \\begin{array}{ll} \\frac{d}{d u} f_{\\mathrm{KL}}(u)=1+\\log u &amp; \\frac{d}{d t} f_{\\mathrm{KL}}^{*}(t)=\\exp (t-1) \\\\ \\frac{d}{d u} f_{\\mathrm{LK}}(u)=-\\frac{1}{u} &amp; \\frac{d}{d t} f_{\\mathrm{LK}}^{*}(t)=-\\frac{1}{t} \\\\ \\frac{d}{d u} f_{\\mathrm{SH}}(u)=1-\\frac{1}{\\sqrt{u}} &amp; \\frac{d}{d t} f_{\\mathrm{SH}}^{*}(t)=\\frac{1}{(1-t)^{2}} \\\\ \\frac{d}{d u} f_{\\mathrm{JS}}(u)=\\log \\frac{u}{1+u} &amp; \\frac{d}{d t} f_{\\mathrm{JS}}^{*}(t)=\\frac{1}{e^{-t}-1} \\end{array} \\] <p>Concept 12.21 : Variational Formulation of f-Divergence</p> <p>Variational formulation of f-divergence:</p> \\[ \\begin{aligned} D_{f}(p \\| q) &amp; =\\int q(x) f\\left(\\frac{p(x)}{q(x)}\\right) d x \\\\ &amp; =\\int q(x) \\sup _{t}\\left\\{t \\frac{p(x)}{q(x)}-f^{*}(t)\\right\\} d x=\\int q(x) T^{\\star}(x) \\frac{p(x)}{q(x)}-q(x) f^{*}\\left(T^{\\star}(x)\\right) d x \\\\ &amp; =\\sup _{T \\in \\mathcal{T}}\\left(\\int p(x) T(x) d x-\\int q(x) f^{*}(T(x)) d x\\right)=\\sup _{T \\in \\mathcal{T}}\\left(\\mathbb{E}_{X \\sim p}[T(X)]-\\mathbb{E}_{\\tilde{X} \\sim q}\\left[f^{*}(T(\\tilde{X}))\\right]\\right) \\\\ &amp; \\geq \\sup _{\\phi \\in \\Phi}\\left(\\mathbb{E}_{X \\sim p}\\left[D_{\\phi}(X)\\right]-\\mathbb{E}_{\\tilde{X} \\sim q}\\left[f^{*}\\left(D_{\\phi}(\\tilde{X})\\right)\\right]\\right) \\end{aligned} \\] <p>where \\(\\mathcal{T}\\) is the set of all measurable functions. In particular, \\(\\mathcal{T}\\) contains \\(T^{\\star}(x)=\\underset{t}{\\operatorname{argmax}}\\left\\{t \\frac{p(x)}{q(x)}-f^*(t)\\right\\}\\) \\(D_{\\phi}\\) is a neural network parameterized by \\(\\phi\\).</p> <p>Definition 12.22 : f-GAN Minimax Formulation</p> <p>Minimax formulation of f-GANs.</p> \\[ \\begin{gathered} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} D_{f}\\left(p_{\\text {true }} \\| p_{\\theta}\\right) \\\\ \\approx \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[D_{\\phi}(X)\\right]-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[f^{*}\\left(D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\end{gathered} \\] <p>Concept 21.23 : f-GAN with KL Divergence</p> <p>Instantiate f-GAN with KL-divergence: </p> \\[ f^{*}(t)=e^{t-1} \\] \\[ \\begin{aligned} &amp; \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad D_{\\mathrm{KL}}\\left(p_{\\text {true }} \\| p_{\\theta}\\right) \\\\ &amp; \\approx \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[D_{\\phi}(X)\\right]-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[e^{D_{\\phi}\\left(G_{\\theta}(Z)\\right)-1}\\right] \\\\ &amp; \\stackrel{(*)}{=} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} 1+\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[D_{\\phi}(X)\\right]-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[e^{D_{\\phi}\\left(G_{\\theta}(Z)\\right)}\\right] \\\\ &amp; =\\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[D_{\\phi}(X)\\right]-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[e^{D_{\\phi}\\left(G_{\\theta}(Z)\\right)}\\right] \\end{aligned} \\] <p>Step (*) uses the substitution \\(D_{\\phi} \\mapsto D_{\\phi}+1\\), which is valid if the final layer of \\(D_{\\phi}\\) has a trainable bias term. ( \\(D_{\\phi}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}\\).)</p> <p>Note that most of the time, the convex conjugate \\(f^{*}(t)\\) has a constraint on it. When the constraint is violated, the \\(f^{*}(t)=\\infty\\) case makes the maximization objective \\(-\\infty\\). However, directly enforcing the neural networks to satisfy \\(f^{*}(D_{\\phi}\\left(G_{\\theta}(z)\\right))&lt;\\infty\\) is awkward.</p> <p>Concept 21.24 : Resolving Infinity with Output Activation</p> <p>When \\(D_{\\phi}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}\\) and \\(\\left\\{t \\mid f^{*}(t)&lt;\\infty\\right\\} \\neq \\mathbb{R}\\), then \\(f^{*}\\left(D_{\\phi}(\\tilde{X})\\right)=\\infty\\) is possible. To prevent this, substitute \\(T(x) \\mapsto \\rho(\\tilde{T}(x))\\), where \\(\\rho: \\mathbb{R} \\rightarrow\\left\\{t \\mid f^{*}(t)&lt;\\infty\\right\\}\\) is a one-to-one function:</p> \\[ \\begin{aligned} D_{f}(p \\| q) &amp; =\\sup _{T \\in \\mathcal{T}}\\left\\{\\mathbb{E}_{X \\sim p}[T(X)]-\\mathbb{E}_{\\tilde{X} \\sim q}\\left[f^{*}(T(\\tilde{X}))\\right]\\right\\} \\\\ &amp; \\stackrel{(*)}{=} \\sup _{\\substack{T \\in \\mathcal{T}}}\\left\\{\\mathbb{E}_{X \\sim p}[T(X)]-\\mathbb{E}_{\\tilde{X} \\sim q}\\left[f^{*}(T(\\tilde{X}))\\right]\\right\\} \\\\ &amp; \\stackrel{(* *)}{=} \\sup _{\\tilde{T} \\in \\mathcal{T}}\\left\\{\\mathbb{E}_{X \\sim p}[\\rho(\\tilde{T}(X))]-\\mathbb{E}_{\\tilde{X} \\sim q}\\left[f^{*}(\\rho(\\tilde{T}(\\tilde{X})))\\right]\\right\\} \\\\ &amp; \\geq \\sup _{\\phi \\in \\Phi}\\left\\{\\mathbb{E}_{X \\sim p}\\left[\\rho\\left(D_{\\phi}(X)\\right)\\right]-\\mathbb{E}_{\\tilde{X} \\sim q}\\left[f^{*}\\left(\\rho\\left(D_{\\phi}(\\tilde{X})\\right)\\right)\\right]\\right\\} \\end{aligned} \\] <p>(\\(*\\)) We can restrict the search over \\(T\\) since if \\(f^{*}(T(x))=\\infty\\), then the objective becomes \\(-\\infty\\).</p> <p>(\\(**\\)) With \\(T=\\rho \\circ \\tilde{T}\\), have \\(\\left[T \\in \\mathcal{T}\\right.\\) and \\(\\left.f^{*}(T(x))&lt;\\infty\\right] \\Leftrightarrow[\\tilde{T} \\in \\mathcal{T}]\\) since \\(\\rho\\) is one-to-one.</p> <p>Definition 21.25 : f-GAN with Output Activation</p> <p>Formulate f-GAN with output activation function \\(\\rho\\) (\\(\\rho: \\mathbb{R} \\rightarrow\\left\\{t \\mid f^{*}(t)&lt;\\infty\\right\\}\\) is a one-to-one function) :</p> \\[ \\begin{gathered} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad D_{f}\\left(p_{\\text {true }} \\| p_{\\theta}\\right) \\\\ \\approx \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\quad \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\rho\\left(D_{\\phi}(X)\\right)\\right]-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[f^{*}\\left(\\rho\\left(D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right)\\right] \\end{gathered} \\] <p>Concept 21.26 : f-GAN with Squared Hellinger</p> <p>Instantiate f-GAN with squared Hellinger distance using \\(\\rho(r)=1-e^{-r}\\) and</p> \\[ f^{*}(t)= \\begin{cases}\\frac{1}{1 / t-1} &amp; \\text { if } t&lt;1 \\\\ \\infty &amp; \\text { otherwise }\\end{cases} \\] <p>Note that \\(f^{*}(\\rho(r))=-1+e^{r}\\).</p> \\[ \\begin{gathered} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad D_{\\mathrm{SH}}\\left(p_{\\text {true }}, p_{\\theta}\\right) \\\\ \\approx \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} 2-\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[e^{-D_{\\phi}(X)}\\right]-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[e^{D_{\\phi}\\left(G_{\\theta}(Z)\\right)}\\right] \\\\ = \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} -\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[e^{-D_{\\phi}(X)}\\right]-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[e^{D_{\\phi}\\left(G_{\\theta}(Z)\\right)}\\right] \\end{gathered} \\] <p>Concept 21.27 : f-GAN with Reversed KL</p> <p>Instantiate f-GAN with reverse KL using \\(\\rho(r)=-e^{r}\\) and</p> \\[ f^{*}(t)= \\begin{cases}-1-\\log (-t) &amp; \\text { if } t&lt;0 \\\\ \\infty &amp; \\text { otherwise }\\end{cases} \\] <p>Note that \\(f^{*}(\\rho(r))=-1-r\\).</p> \\[ \\begin{gathered} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad D_{\\mathrm{KL}}\\left(p_{\\theta} \\| p_{\\text {true }}\\right) \\\\ \\approx \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} 1-\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[e^{D_{\\phi}(X)}\\right]+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right] \\\\ =\\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}}-\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[e^{D_{\\phi}(X)}\\right]+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right] \\end{gathered} \\] <p>Concept 21.28 : f-GAN with JS (Recovering Standard GAN)</p> <p>We recover standard GAN with</p> \\[ \\rho(r)=\\log (\\sigma(r)), \\quad \\sigma(r)=\\frac{1}{1+e^{-r}}, \\quad f^{*}(t)= \\begin{cases}-\\log (1-\\exp (t))-\\log 4 &amp; \\text { for } t&lt;0 \\\\ \\infty &amp; \\text { otherwise }\\end{cases} \\] <p>Note that \\(\\sigma\\) is the familiar sigmoid and</p> \\[ f^{*}(\\rho(r))=-\\log (1-\\sigma(r))-\\log 4 \\] \\[ \\begin{gathered} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} D_{\\mathrm{JS}}\\left(p_{\\text {true }}, p_{\\theta}\\right) \\\\ \\approx \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log \\sigma\\left(D_{\\phi}(X)\\right)\\right]+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[\\log \\left(1-\\sigma\\left(D_{\\phi}\\left(G_{\\theta}(X)\\right)\\right)\\right)\\right] \\end{gathered} \\] <p>where \\(D_{\\phi}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}\\).</p> <p>(Standard GAN has \\(D_{\\phi}: \\mathbb{R}^{n} \\rightarrow(0,1)\\). Here, \\(\\left(\\sigma \\circ D_{\\phi}\\right): \\mathbb{R}^{n} \\rightarrow(0,1)\\) serves the same purpose.)</p> <p>Definition 21.29 : WGAN</p> <p>The Wasserstein GAN (WGAN) minimizes the Wasserstein distance:</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad W\\left(p_{\\text {true }}, p_{\\theta}\\right) \\] <p>The \\(W(p, q)\\) is a distance (metric) on probability distributions defined as</p> \\[ W(p, q)=\\inf _{f} \\mathbb{E}_{(X, Y) \\sim f(x, y)}\\|X-Y\\| \\] <p>where the infimum is taken over joint probability distributions \\(f\\) with marginals \\(p\\) and \\(q\\), i.e.,</p> \\[ p(x)=\\int f(x, y) d y, \\quad q(y)=\\int f(x, y) d x \\] <p>(M. Arjovsky, S. Chintala, and L. Bottou, Wasserstein GAN, ICML, 2017.)</p> <p>Another equivalent formulation of the Wasserstein distance is by the theory of optimal transport. Given distributions \\(p\\) and \\(q\\) (initial and target)</p> \\[ W(p, q)=\\inf _{T} \\int\\|x-T(x)\\| p(x) d x \\] <p>where \\(T\\) is a transport plan that transports \\(p\\) to \\(q\\). Figuratively speaking, we are transporting grains of sand from one pile to another, and we wan to minimize the aggregate transport distance.</p> <p>(Image from W. Li, E. K. Ryu, S. Osher, W. Yin, and W. Gangbo, A parallel method for earth mover\u2019s distance, J. Sci. Comput., 2018.)</p> <p>Kantorovich-Rubinstein duality \\(^{\\#}\\) establishes:</p> \\[ W\\left(p_{\\text {true }}, p_{\\theta}\\right)=\\sup _{\\|T\\|_{L \\leq 1}} \\mathbb{E}_{X \\sim p_{\\text {true }}}[T(X)]-\\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}[T(\\tilde{X})] \\] <p>Minimax formulation of WGAN:</p> \\[ \\begin{gathered} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} W\\left(p_{\\text {true }}, p_{\\theta}\\right) \\\\ \\approx \\quad \\begin{aligned} &amp; \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathbb{E}_{X \\sim p_{\\text {true }}}[D_{\\phi}(X)]-\\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}[D_{\\phi}(\\tilde{X})] \\\\ &amp; \\text { subject to } D_{\\phi} \\text{ is } 1 \\text{-Lipschitz} \\end{aligned} \\end{gathered} \\] <p>(\\(^{\\#}\\) L.V. Kantorovich and G. Rubinstein, On a space of completely additive functions, Vestnik Leningradskogo Universiteta, 1958. The Kantorovich\u2013Rubinstein dual as the convex (Lagrange) dual of a \u201cflux\u201d formulation of the optimal transport.)</p> <p>Concept 21.30 : Spectral Normalization</p> <p>How do we enforce the constraint that \\(D_{\\phi}\\) is 1-Lipschitz? Consider an MLP:</p> \\[ \\begin{aligned} y_{L}= &amp; A_{L} y_{L-1}+b_{L} \\\\ y_{L-1}= &amp; \\sigma\\left(A_{L-1} y_{L-2}+b_{L-1}\\right) \\\\ &amp; \\vdots \\\\ y_{2}= &amp; \\sigma\\left(A_{2} y_{1}+b_{2}\\right) \\\\ y_{1}= &amp; \\sigma\\left(A_{1} x+b_{1}\\right), \\end{aligned} \\] <p>where \\(\\sigma\\) is a 1-Lipschitz continuous activation function, such as ReLU and tanh. If</p> \\[ \\left\\|A_{i}\\right\\|_{\\mathrm{op}}=\\sigma_{\\max }\\left(A_{i}\\right) \\leq 1 \\] <p>for \\(i=1, \\ldots, L\\), where \\(\\sigma_{\\text {max }}\\) denotes the largest singular value, then each layer is 1-Lipschitz continuous and the entire mapping \\(x \\mapsto y_{L}\\) is 1 -Lipschitz. (A sufficient, but not a necessary, condition.)</p> <p>Replace Lipschitz constraint with a singular-value constraint</p> \\[ \\begin{array}{rll} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} &amp; \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} &amp; \\frac{1}{N} \\sum_{i=1}^{N} D_{\\phi}\\left(X_{i}\\right)-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right] \\\\ &amp; \\text { subject to } &amp; \\sigma_{\\max }\\left(A_{i}\\right) \\leq 1, \\quad i=1, \\ldots, L \\end{array} \\] <p>Constraint is handled with a projected gradient method. (Note that \\(A_{1}, \\ldots, A_{L}\\) are part of the discriminator parameters \\(\\phi\\).)</p> <p>(Specifically, one performs an (approximate) projection after the ascent step in the stochastic gradient ascent-descent methods. The approximate projection involves computing the largest singular with the power iteration.)</p> <p>(T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida, Spectral normalization for generative adversarial networks, ICLR, 2018.)</p>"},{"location":"books-and-courses/mfdnn/13/","title":"\u00a7 13. Basics of Monte Carlo","text":""},{"location":"books-and-courses/mfdnn/13/#monte-carlo-estimation","title":"Monte Carlo Estimation","text":"<p>Definition 13.1 : Monte Carlo Estimation</p> <p>Consider IID data \\(X_{1}, \\ldots, X_{N} \\sim f\\). Let \\(\\phi(X) \\geq 0\\) be some function. (The assumption \\(\\phi(X) \\geq 0\\) can be relaxed.) Consider the problem of estimating</p> \\[ I=\\mathbb{E}_{X \\sim f}[\\phi(X)]=\\int \\phi(x) f(x) d x \\] <p>One commonly uses</p> \\[ \\hat{I}_{N}=\\frac{1}{N} \\sum_{i=1}^{N} \\phi\\left(X_{i}\\right) \\] <p>to estimate \\(I\\), which is called monte carlo estimation. After all, \\(\\mathbb{E}\\left[\\hat{I}_{N}\\right]=I\\) and \\(\\hat{I}_{N} \\rightarrow I\\) by the law of large numbers. (Convergence in probability by weak law of large numbers and almost sure convergence by strong law of large numbers.)</p> <p>Concept 13.2 : Evidence of Convergence for Monte Carlo Estimation</p> <p>We can quantify convergence with variance:</p> \\[ \\operatorname{Var}_{X \\sim f}\\left(\\hat{I}_{N}\\right)=\\sum_{i=1}^{N} \\operatorname{Var}_{X_{i} \\sim f}\\left(\\frac{\\phi\\left(X_{i}\\right)}{N}\\right)=\\frac{1}{N} \\operatorname{Var}_{X \\sim f}(\\phi(X)) \\] <p>In other words</p> \\[ \\mathbb{E}\\left[\\left(\\hat{I}_{N}-I\\right)^{2}\\right]=\\frac{1}{N} \\operatorname{Var}_{X \\sim f}(\\phi(X)) \\] <p>and</p> \\[ \\mathbb{E}\\left[\\left(\\hat{I}_{N}-I\\right)^{2}\\right] \\rightarrow 0 \\] <p>as \\(N \\rightarrow \\infty\\). So, \\(\\hat{I}_{N} \\rightarrow I\\) in \\(L^2\\) provided that \\(\\operatorname{Var}_{X \\sim f}(\\phi(X)) &lt; \\infty\\).</p> <p>Definition 13.3 : Empirical Risk Minimization (ERM)</p> <p>In machine learning and statistics, we often wish to solve</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad \\mathcal{L}(\\theta) \\] <p>where the objective function</p> \\[ \\mathcal{L}(\\theta)=\\mathbb{E}_{X \\sim p_{X}}\\left[\\ell\\left(f_{\\theta}(X), f_{\\star}(X)\\right)\\right] \\] <p>Is the (true) risk. However, the evaluation of \\(\\mathbb{E}_{X \\sim p_{X}}\\) is impossible (if \\(p_{X}\\) is unknown) or intractable (if \\(p_{X}\\) is known but the expectation has no closed-form solution). Therefore, we define the proxy loss function</p> \\[ \\mathcal{L}_{N}(\\theta)=\\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(f_{\\theta}\\left(X_{i}\\right), f_{\\star}\\left(X_{i}\\right)\\right) \\] <p>which we call the empirical risk, and solve</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad \\mathcal{L}_{N}(\\theta) \\] <p>This is called empirical risk minimization (ERM). The idea is that</p> \\[ \\mathcal{L}_{N}(\\theta) \\approx \\mathcal{L}(\\theta) \\] <p>with high probability, so minimizing \\(\\mathcal{L}_{N}(\\theta)\\) should be similar to minimizing \\(\\mathcal{L}(\\theta)\\).</p> <p>Concept 13.4 : Evidence of Convergence for Empirical Risk Minimization</p> <p>Technical note) The law of large numbers tells us that</p> \\[ \\mathbb{P}\\left(\\left|\\mathcal{L}_{N}(\\theta)-\\mathcal{L}(\\theta)\\right|&gt;\\varepsilon\\right)=\\text { small } \\] <p>for any given \\(\\theta\\), but we need</p> \\[ \\mathbb{P}\\left(\\sup _{\\theta \\in \\Theta}\\left|\\mathcal{L}_{N}(\\theta)-\\mathcal{L}(\\theta)\\right|&gt;\\varepsilon\\right)=\\text { small } \\] <p>for all compact \\(\\Theta\\) in order to conclude that the argmins of the two losses to be similar. These types of results are established by a uniform law of large numbers.</p>"},{"location":"books-and-courses/mfdnn/13/#importance-sampling","title":"Importance Sampling","text":"<p>Definition 13.5 : Importance Sampling (IS)</p> <p>Importance sampling (IS) is a technique for reducing the variance of a Monte Carlo estimator.</p> <p>Key insight of important sampling:</p> \\[ I=\\mathbb{E}_{X \\sim f}[\\phi(X)]=\\int \\phi(x) f(x) d x=\\int \\frac{\\phi(x) f(x)}{g(x)} g(x) d x=\\mathbb{E}_{X \\sim g}\\left[\\frac{\\phi(X) f(X)}{g(X)}\\right] \\] <p>(We do have to be mindful of division by 0.) Then</p> \\[ \\hat{I}_{N}=\\frac{1}{N} \\sum_{i=1}^{N} \\phi\\left(X_{i}\\right) \\frac{f\\left(X_{i}\\right)}{g\\left(X_{i}\\right)} \\] <p>with \\(X_{1}, \\ldots, X_{N} \\sim g\\) is also an estimator of \\(I\\). Indeed, \\(\\mathbb{E}\\left[\\hat{I}_{N}\\right]=I\\) and \\(\\hat{I}_{N} \\rightarrow I\\). The weight \\(\\frac{f(x)}{g(x)}\\) is called the likelihood ratio or the Radon-Nikodym derivative.</p> <p>So we can use samples from \\(g\\) to compute expectation with respect to \\(f\\).</p> <p>Example 13.6 : IS Example : Low Probability Events</p> <p>Consider the setup of estimating the probability</p> \\[ \\mathbb{P}(X&gt;3)=0.00135 \\] <p>where \\(X \\sim \\mathcal{N}(0,1)\\). If we use the regular Monte Carlo estimator</p> \\[ \\hat{I}_{N}=\\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}_{\\left\\{X_{i}&gt;3\\right\\}} \\] <p>where \\(X_{i} \\sim \\mathcal{N}(0,1)\\), if \\(N\\) is not sufficiently large, we can have \\(\\hat{I}_{N}=0\\). Inaccurate estimate.</p> <p>If we use the IS estimator</p> \\[ \\hat{I}_{N}=\\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}_{\\left\\{Y_{i}&gt;3\\right\\}} \\exp \\left(\\frac{\\left(Y_{i}-3\\right)^{2}-Y_{i}^{2}}{2}\\right) \\] <p>where \\(Y_{i} \\sim \\mathcal{N}(3,1)\\), having \\(\\hat{I}_{N}=0\\) is much less likely. Estimate is much more accurate.</p> <p>Concept 13.7 : Optimal Sampling Distribution</p> <p>Benefit of IS quantified by with variance:</p> \\[ \\operatorname{Var}_{X \\sim g}\\left(\\hat{I}_{N}\\right)=\\sum_{i=1}^{N} \\operatorname{Var}_{X \\sim g}\\left(\\frac{\\phi\\left(X_{i}\\right) f\\left(X_{i}\\right)}{n g\\left(X_{i}\\right)}\\right)=\\frac{1}{N} \\operatorname{Var}_{X \\sim g}\\left(\\frac{\\phi(X) f(X)}{g(X)}\\right) \\] <p>If \\(\\operatorname{Var}_{X \\sim g}\\left(\\frac{\\phi(X) f(X)}{g(X)}\\right)&lt;\\operatorname{Var}_{X \\sim f}(\\phi(X))\\), then IS provides variance reduction.</p> <p>We call \\(g\\) the importance or sampling distribution. Choosing \\(g\\) poorly can increase the variance. What is the best choice of \\(g\\) ?</p> <p>The sampling distribution</p> \\[ g(x)=\\frac{\\phi(x) f(x)}{I} \\] <p>makes \\(\\operatorname{Var}_{X \\sim g}\\left(\\frac{\\phi(X) f(X)}{g(X)}\\right)=\\operatorname{Var}_{X \\sim g}(I)=0\\) and therefore is optimal. (\\(I\\) serves as the normalizing factor that ensures the density \\(g\\) integrates to 1.)</p> <p>Problem: Since we do not know the normalizing factor \\(I\\), the answer we wish to estimate, sampling from \\(g\\) is usually difficult.</p> <p>Concept 13.8 : Optimized / Trained Sampling Distribution</p> <p>Instead, we consider the optimization problem</p> \\[ \\underset{g \\in \\mathcal{G}}{\\operatorname{minimize}} \\quad D_{\\mathrm{KL}}\\left(g \\| \\frac{\\phi f}{I}\\right) \\] <p>and compute a suboptimal, but good, sampling distribution within a class of sampling distributions \\(\\mathcal{G}\\). (In ML, \\(\\mathcal{G}=\\left\\{g_{\\theta} \\mid \\theta \\in \\Theta\\right\\}\\) is parameterized by neural networks.)</p> <p>Importantly, this optimization problem does not require knowledge of \\(I\\).</p> \\[ \\begin{aligned} D_{\\mathrm{KL}}\\left(g_{\\theta} \\| \\phi f / I\\right) &amp; =\\mathbb{E}_{X \\sim g_{\\theta}}\\left[\\log \\left(\\frac{I g_{\\theta}(X)}{\\phi(X) f(X)}\\right)\\right] \\\\ &amp; =\\mathbb{E}_{X \\sim g_{\\theta}}\\left[\\log \\left(\\frac{g_{\\theta}(X)}{\\phi(X) f(X)}\\right)\\right]+\\log I \\\\ &amp; =\\mathbb{E}_{X \\sim g_{\\theta}}\\left[\\log \\left(\\frac{g_{\\theta}(X)}{\\phi(X) f(X)}\\right)\\right]+\\text { constant independent of } \\theta \\end{aligned} \\] <p>How do we compute stochastic gradients?</p>"},{"location":"books-and-courses/mfdnn/13/#log-derivative-trick","title":"Log-Derivative Trick","text":"<p>Definition 13.9 : Log-Derivative Trick</p> <p>Generally, consider the setup where we wish to solve</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\mathbb{E}_{X \\sim f_{\\theta}}[\\phi(X)] \\] <p>with SGD. (Previous situation (concept 13.8) had \\(\\theta\\)-dependence both on and inside the expectation. For now, let's simplify the problem so that \\(\\phi\\) does not depend on \\(\\theta\\).)</p> <p>Incorrect gradient computation:</p> \\[ \\nabla_{\\theta} \\mathbb{E}_{X \\sim f_{\\theta}}[\\phi(X)] \\stackrel{?}{=} \\mathbb{E}_{X \\sim f_{\\theta}}\\left[\\nabla_{\\theta} \\phi(X)\\right]=\\mathbb{E}_{X \\sim f_{\\theta}}[0]=0 \\] <p>Correct gradient computation:</p> \\[ \\begin{aligned} \\nabla_{\\theta} \\mathbb{E}_{X \\sim f_{\\theta}}[\\phi(X)] &amp; =\\nabla_{\\theta} \\int \\phi(x) f_{\\theta}(x) d x=\\int \\phi(x) \\nabla_{\\theta} f_{\\theta}(x) d x \\\\ &amp; =\\int \\phi(x) \\frac{\\nabla_{\\theta} f_{\\theta}(x)}{f_{\\theta}(x)} f_{\\theta}(x) d x=\\mathbb{E}_{X \\sim f_{\\theta}}\\left[\\phi(X) \\frac{\\nabla_{\\theta} f_{\\theta}(X)}{f_{\\theta}(X)}\\right] \\\\ &amp; =\\mathbb{E}_{X \\sim f_{\\theta}}\\left[\\phi(X) \\nabla_{\\theta} \\log \\left(f_{\\theta}(X)\\right)\\right] \\end{aligned} \\] <p>Therefore, \\(\\phi(X) \\nabla_{\\theta} \\log \\left(f_{\\theta}(X)\\right)\\) with \\(X \\sim f_{\\theta}\\) is a stochastic gradient of the loss function. This technique is called the log-derivative trick, the likelihood ratio gradient\\(^{\\#}\\), or REINFORCE\\(^{\\star}\\).</p> <p>Formula with the log-derivative \\(\\left(\\nabla_{\\theta} \\log (\\cdot)\\right)\\) is convenient when dealing with Gaussians, or more generally exponential families, since the densities are of the form</p> \\[ f_{\\theta}(x)=h(x) \\exp (\\text {function of } \\theta) \\] <p>(\\({}^{\\#}\\)P. W. Glynn, Likelihood ratio gradient estimation for stochastic systems, Communications of the ACM, 1990. \\({}^{\\star}\\)R. J. Williams, Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 1992.)</p> <p>Example 13.10 : Log-Derivative Trick Example</p> <p>Learn \\(\\mu \\in \\mathbb{R}^{2}\\) to minimize the objective below.</p> \\[ \\underset{\\mu \\in \\mathbb{R}^{2}}{\\operatorname{minimize}} \\mathbb{E}_{X \\sim \\mathcal{N}(\\mu, I)}\\left\\|X-\\binom{5}{5}\\right\\|^{2} \\] <p>Then the loss function is</p> \\[ \\mathcal{L}(\\mu)=\\mathbb{E}_{X \\sim \\mathcal{N}(\\mu, I)}\\left\\|X-\\binom{5}{5}\\right\\|^{2}=\\int\\left\\|x-\\binom{5}{5}\\right\\|^{2} \\frac{1}{2 \\pi} \\exp \\left(-\\frac{1}{2}\\|x-\\mu\\|^{2}\\right) d x \\] <p>And, using \\(X_{1}, \\ldots, X_{B} \\sim \\mathcal{N}(\\mu, I)\\), we have stochastic gradients</p> \\[ \\nabla_{\\mu} \\mathcal{L}(\\mu)=\\mathbb{E}_{X \\sim \\mathcal{N}(\\mu, I)}\\left[\\left\\|x-\\binom{5}{5}\\right\\|^{2} \\nabla_{\\mu}\\left(-\\frac{1}{2}\\|x-\\mu\\|^{2}\\right)\\right] \\approx \\frac{1}{B} \\sum_{i=1}^{B}\\left\\|X_{i}-\\binom{5}{5}\\right\\|^{2}\\left(X_{i}-\\mu\\right) \\] <p>These stochastic gradients have large variance and thus SGD is slow.</p> <p> </p>"},{"location":"books-and-courses/mfdnn/13/#reparameterization-trick","title":"Reparameterization Trick","text":"<p>Definition 13.11 : Reparameterization Trick</p> <p>The reparameterization trick (RT) or the pathwise derivative (PD) relies on the key insight.</p> \\[ \\mathbb{E}_{X \\sim \\mathcal{N}\\left(\\mu, \\sigma^{2}\\right)}[\\phi(X)]=\\mathbb{E}_{Y \\sim \\mathcal{N}(0,1)}[\\phi(\\mu+\\sigma Y)] \\] <p>Gradient computation:</p> \\[ \\begin{aligned} \\nabla_{\\mu, \\sigma} \\mathbb{E}_{X \\sim \\mathcal{N}\\left(\\mu, \\sigma^{2}\\right)}[\\phi(X)] &amp; =\\mathbb{E}_{Y \\sim \\mathcal{N}(0,1)}\\left[\\nabla_{\\mu, \\sigma} \\phi(\\mu+\\sigma Y)\\right]=\\mathbb{E}_{Y \\sim \\mathcal{N}(0,1)}\\left[\\phi^{\\prime}(\\mu+\\sigma Y)\\left[\\begin{array}{c} 1 \\\\ Y \\end{array}\\right]\\right] \\\\ &amp; \\approx \\frac{1}{B} \\sum_{i=1}^{B} \\phi^{\\prime}\\left(\\mu+\\sigma Y_{i}\\right)\\left[\\begin{array}{c} 1 \\\\ Y_{i} \\end{array}\\right], \\quad Y_{1}, \\ldots, Y_{B} \\sim \\mathcal{N}(0, I) \\end{aligned} \\] <p>RT is less general than log-derivative trick, but it usually produces stochastic gradients with lower variance.</p> <p>Example 13.12 : Reparameterization Trick Example</p> <p>Consider the same example as before</p> \\[ \\mathcal{L}(\\mu)=\\mathbb{E}_{X \\sim \\mathcal{N}(\\mu, I)}\\left\\|X-\\binom{5}{5}\\right\\|^{2}=\\mathbb{E}_{Y \\sim \\mathcal{N}(0, I)}\\left\\|Y+\\mu-\\binom{5}{5}\\right\\|^{2} \\] <p>Gradient computation:</p> \\[ \\begin{aligned} \\nabla_{\\mu} \\mathcal{L}(\\mu) &amp; =\\mathbb{E}_{Y \\sim \\mathcal{N}(0, I)} \\nabla_{\\mu}\\left\\|Y+\\mu-\\binom{5}{5}\\right\\|^{2}=2 \\mathbb{E}_{Y \\sim \\mathcal{N}(0, I)}\\left(Y+\\mu-\\binom{5}{5}\\right) \\\\ &amp; \\approx \\frac{2}{B} \\sum_{i=1}^{B}\\left(Y_{i}+\\mu-\\binom{5}{5}\\right), \\quad Y_{1}, \\ldots, Y_{B} \\sim \\mathcal{N}(0, I) \\end{aligned} \\] <p>These stochastic gradients have smaller variance and thus SGD is faster.</p> <p>Example 13.13 : Log Derivative Trick vs Reparameterization Trick</p> <p>The image below is the result of SGD with the computed gradients by Example 13.10 and Example 13.12.  </p>"},{"location":"books-and-courses/mfdnn/2/","title":"\u00a7 2. Gradient Descent","text":""},{"location":"books-and-courses/mfdnn/2/#gradient-descent","title":"Gradient Descent","text":"<p>Definition 2.1 : Gradient Descent</p> <p>Consider the unconstrained optimization problem</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{P}}{\\operatorname{minimize}} f(\\theta) \\] <p>where \\(f\\) is differentiable.</p> <p>Gradient Descent (GD) algorithm:</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\nabla f\\left(\\theta^{k}\\right) \\quad \\text { for } k=0,1, \\ldots, \\] <p>where \\(\\theta^{0} \\in \\mathbb{R}^{p}\\) is the initial point and \\(\\alpha_{k}&gt;0\\) is the learning rate or the stepsize.</p> <p>The terminology learning rate is common in the machine learning literature while stepsize is more common in the optimization literature.</p> <p>In math, a function is \"differentiable\" if its derivative exists everywhere.</p> <p>In deep learning (DL), a function is often said to be differentiable if its derivative exists almost everywhere and the function is nice. ReLU activation functions are said to be differentiable.</p> <p>Concept 2.2 : Efficiency of gradient descent can be expected using the first-order Taylor expansion of \\(f\\).</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\nabla f\\left(\\theta^{k}\\right) \\] <p>Taylor expansion of \\(f\\) about \\(\\theta^{k}\\) :</p> \\[ f(\\theta)=f\\left(\\theta^{k}\\right)+\\nabla f\\left(\\theta^{k}\\right)^{\\top}\\left(\\theta-\\theta^{k}\\right)+\\mathcal{O}\\left(\\left\\|\\theta-\\theta^{k}\\right\\|^{2}\\right) \\] <p>Plug in \\(\\theta^{k+1}\\) :</p> \\[ f\\left(\\theta^{k+1}\\right)=f\\left(\\theta^{k}\\right)-\\alpha_{k}\\left\\|\\nabla f\\left(\\theta^{k}\\right)\\right\\|^{2}+\\mathcal{O}\\left(\\alpha_{k}^{2}\\right) \\] <p>\\(-\\nabla f\\left(\\theta^{k}\\right)\\) is steepest descent direction. For small (cautious) \\(\\alpha_{k}\\), GD step reduces function value.</p> <p>However, note that a step of GD need not result in descent, i.e., \\(f\\left(\\theta^{k+1}\\right)&gt;f\\left(\\theta^{k}\\right)\\) is possible.</p>  ![](./assets/2.1.png){: width=\"40%\"}  <p>We need an assumption that ensures the first-order Taylor expansion is a good approximation within a sufficiently large neighborhood.</p>"},{"location":"books-and-courses/mfdnn/2/#convergence-of-gradient-descent","title":"Convergence of Gradient Descent","text":"<p>Without further assumptions, there is no hope of finding the global minimum.</p> <p>We cannot prove the function value converges to global optimum. We instead prove \\(\\nabla f\\left(\\theta^{k}\\right) \\rightarrow 0\\). Roughly speaking, this is similar, but weaker than proving that \\(\\theta^{k}\\) converges to a local minimum.</p>  ![](./assets/2.2.png){: width=\"70%\"}  <p>Without further assumptions, we cannot show that \\(\\theta^{k}\\) converges to a limit, and even \\(\\theta^{k}\\) does converge to a limit, we cannot guarantee that that limit is not a saddle point or even a local maximum. Nevertheless, people commonly use the argument that \\(\\theta^{k}\\) usually converges and that it is unlikely that the limit is a local maximum or a saddle point.</p> <p>Definition 2.3 : \\(L\\)-Lipschitz</p> <p>We say \\(\\nabla f: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}^{p}\\) is \\(L\\)-Lipschitz if</p> \\[ \\|\\nabla f(x)-\\nabla f(y)\\| \\leq L\\|x-y\\| \\quad \\forall x, y \\in \\mathbb{R}^{p} . \\] <p>Roughly, this means \\(\\nabla f\\) does not change rapidly. As a consequence, we can trust the first-order Taylor expansion on a non-infinitesimal neighborhood.</p> <p>Theorem 2.4 : Lipschitz Gradient Lemma</p> <p>Let \\(f: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}\\) be differentiable and \\(\\nabla f: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}^{p}\\) be L-Lipschitz. Then</p> \\[ f(\\theta+\\delta) \\leq f(\\theta)+\\nabla f(\\theta)^{\\top} \\delta+\\frac{L}{2}\\|\\delta\\|^{2} \\quad \\forall \\theta, \\delta \\in \\mathbb{R}^{p} \\] <p>\\(f(\\theta)+\\nabla f(\\theta)^{\\top} \\delta-\\frac{L}{2}\\|\\delta\\|^{2} \\leq f(\\theta+\\delta)\\) is also true, but we do not need this other direction. Together the inequalities imply</p> \\[ \\left|f(\\theta+\\delta)-\\left(f(\\theta)+\\nabla f(\\theta)^{\\top} \\delta\\right)\\right| \\leq \\frac{L}{2}\\|\\delta\\|^{2} \\quad \\forall \\theta, \\delta \\in \\mathbb{R}^{p} \\] <p>Proof</p> <p>Define \\(g: \\mathbb{R} \\rightarrow \\mathbb{R}\\) as \\(g(t)=f(\\theta+t \\delta)\\). Then \\(g\\) is differentiable and</p> \\[ g^{\\prime}(t)=\\nabla f(\\theta+t \\delta)^{\\top} \\delta \\] <p>Note \\(g^{\\prime}\\) is \\(\\left(L\\|\\delta\\|^{2}\\right)\\)-Lipschitz continuous since</p> \\[ \\begin{gathered} \\left|g^{\\prime}\\left(t_{1}\\right)-g^{\\prime}\\left(t_{0}\\right)\\right|=\\left|\\left(\\nabla f\\left(\\theta+t_{1} \\delta\\right)-\\nabla f\\left(\\theta+t_{0} \\delta\\right)\\right)^{\\top} \\delta\\right| \\\\ \\leq\\left\\|\\nabla f\\left(\\theta+t_{1} \\delta\\right)-\\nabla f\\left(\\theta+t_{0} \\delta\\right)\\right\\|\\| \\| \\delta \\| \\\\ \\leq L\\left\\|t_{1} \\delta-t_{0} \\delta\\right\\|\\|\\delta\\| \\\\ =L\\|\\delta\\|^{2}\\left|t_{1}-t_{0}\\right| \\end{gathered} \\] <p>Finally, we conclude with</p> \\[ \\begin{gathered} f(\\theta+\\delta)=g(1)=g(0)+\\int_{0}^{1} g^{\\prime}(t) \\mathrm{d} t \\\\ \\leq f(\\theta)+\\int_{0}^{1}\\left(g^{\\prime}(0)+L\\|\\delta\\|^{2} t\\right) \\mathrm{d} t \\\\ =f(\\theta)+\\nabla f(\\theta)^{\\top} \\delta+\\frac{L}{2}\\|\\delta\\|^{2} \\end{gathered} \\] <p>Theorem 2.5 : Summability Lemma</p> <p>Let \\(V^{0}, V^{1}, \\ldots \\in \\mathbb{R}\\) and \\(S^{0}, S^{1}, \\ldots \\in \\mathbb{R}\\) be nonnegative sequences satisfying</p> \\[ V^{k+1} \\leq V^{k}-S^{k} \\] <p>for \\(k=0,1,2, \\ldots\\) Then \\(S^{k} \\rightarrow 0\\).</p> <p>Proof</p> <p>Key idea. \\(S^{k}\\) measures progress (decrease) made in iteration \\(k\\). Since \\(V^{k} \\geq 0, V^{k}\\) cannot decrease forever, so the progress (magnitude of \\(S^{k}\\) ) must diminish to 0.</p> <p>Sum the inequality from \\(i=0\\) to \\(k\\)</p> \\[ V^{k+1}+\\sum_{i=0}^{k} S^{i} \\leq V^{0} \\] <p>Let \\(k \\rightarrow \\infty\\)</p> \\[ \\sum_{i=0}^{\\infty} S^{i} \\leq V^{0}-\\lim _{k \\rightarrow \\infty} V^{k} \\leq V^{0} \\] <p>Since \\(\\sum_{i=0}^{\\infty} S^{i}&lt;\\infty, S^{i} \\rightarrow 0\\).</p> <p>Theorem 2.6 : Convergence of GD</p> <p>Assume \\(f: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}\\) is differentiable, \\(\\nabla f\\) is \\(L\\)-Lipschitz continuous, and \\(\\inf _{\\theta \\in \\mathbb{R}^{p}} f(\\theta)&gt;-\\infty\\). Then</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha \\nabla f\\left(\\theta^{k}\\right) \\] <p>with \\(\\alpha \\in\\left(0, \\frac{2}{L}\\right)\\) satisfies \\(\\nabla f\\left(\\theta^{k}\\right) \\rightarrow 0\\).</p> <p>Proof</p> <p>Use Lipschitz gradient lemma with \\(\\theta=\\theta^{k}\\) and \\(\\delta=-\\alpha \\nabla f\\left(\\theta^{k}\\right)\\) to get</p> \\[ f\\left(\\theta^{k+1}\\right) \\leq f\\left(\\theta^{k}\\right)-\\alpha\\left(1-\\frac{\\alpha L}{2}\\right)\\left\\|\\nabla f\\left(\\theta^{k}\\right)\\right\\|^{2} \\] <p>and</p> \\[ \\left(f\\left(\\theta^{k+1}\\right)-\\inf _{\\theta} f(\\theta)\\right) \\leq\\left(f\\left(\\theta^{k}\\right)-\\inf _{\\theta} f(\\theta)\\right)-\\alpha\\left(1-\\frac{\\alpha L}{2}\\right)\\left\\|\\nabla f\\left(\\theta^{k}\\right)\\right\\|^{2} \\\\ \\] \\[ \\left(f\\left(\\theta^{k+1}\\right)-\\inf _{\\theta} f(\\theta)\\right) \\ge 0,  \\left(f\\left(\\theta^{k}\\right)-\\inf _{\\theta} f(\\theta)\\right) \\ge 0,  \\alpha\\left(1-\\frac{\\alpha L}{2}\\right)\\left\\|\\nabla f\\left(\\theta^{k}\\right)\\right\\|^{2} &gt; 0 \\text{ for } \\alpha \\in\\left(0, \\frac{2}{L}\\right) \\] <p>By the summability lemma, \\(\\left\\|\\nabla f\\left(\\theta^{k}\\right)\\right\\|^{2} \\rightarrow 0\\) and thus \\(\\nabla f\\left(\\theta^{k}\\right) \\rightarrow 0\\).</p> <p>In deep learning, the condition that \\(\\nabla f\\) is \\(L\\)-Lipschitz is usually not true (due to the use of ReLU activation functions).</p> <p>Rather, the purpose of these mathematical analyses is to obtain qualitative insights; this convergence proof are meant to provide you with intuition on the training dynamics of GD and SGD.</p> <p>Because analyzing deep learning systems as is rigorously is usually difficult, people usually analyze modified (simplified) setups rigorously or analyze the full setup heuristically.</p> <p>In both cases, the goal is to obtain qualitative insights, rather than theoretical guarantees.</p>"},{"location":"books-and-courses/mfdnn/2/#stochastic-gradient-descent","title":"Stochastic Gradient Descent","text":"<p>Definition 2.7 : Finite-Sum Optimization Problem</p> <p>A finite-sum optimization problem has the structure</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} f_{i}(\\theta):=F(\\theta) \\] <p>Finite-sum is ubiquitous in ML. \\(N\\) usually corresponds to the number of data points.</p> <p>In finite-sum problem, using GD</p> \\[ \\theta^{k+1}=\\theta^{k}-\\frac{\\alpha_{k}}{N} \\sum_{i=1}^{N} \\nabla f_{i}\\left(\\theta^{k}\\right) \\] <p>is impractical when \\(N\\) is large since \\(\\frac{1}{N} \\sum_{i=1}^{N} \\nabla f_{i}\\left(\\theta^{k}\\right)\\) takes too long to compute.</p> <p>Concept 2.8 : Finite-sum problem can be reformulated with expectation.</p> <p>Although the finite-sum optimization problem has no inherent randomness, we can reformulate this problem with randomness:</p> \\[ \\operatorname{minimize}_{\\theta \\in \\mathbb{R}^{p}} \\quad \\mathbb{E}_{I}\\left[f_{I}(\\theta)\\right] \\] <p>where \\(I \\sim\\) Uniform \\(\\{1, \\ldots, N\\}\\). To see the equivalence,</p> \\[ \\mathbb{E}_{I}\\left[f_{I}(\\theta)\\right]=\\sum_{i=1}^{N} f_{i}(\\theta) \\mathbb{P}(I=i)=\\frac{1}{N} \\sum_{i=1}^{N} f_{i}(\\theta)=F(\\theta) \\] <p>Definition 2.9 : Stochastic Gradient Descent</p> <p>Stochastic gradient descent (SGD)</p> \\[ \\begin{gathered} i(k) \\sim \\operatorname{Uniform}\\{1, \\ldots, N\\} \\\\ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\nabla f_{i(k)}\\left(\\theta^{k}\\right) \\end{gathered} \\] <p>for \\(k=0,1, \\ldots\\), where \\(\\theta^{0} \\in \\mathbb{R}^{p}\\) is the initial point and \\(\\alpha_{k}&gt;0\\) is the learning rate.</p> <p>\\(\\nabla f_{i(k)}\\left(\\theta^{k}\\right)\\) is a stochastic gradient of \\(F\\) at \\(\\theta^{k}\\), i.e.,</p> \\[ \\mathbb{E}\\left[\\nabla f_{i(k)}\\left(\\theta^{k}\\right)\\right]=\\nabla \\mathbb{E}\\left[f_{i(k)}\\left(\\theta^{k}\\right)\\right]=\\nabla F\\left(\\theta^{k}\\right) \\] <p>Concept 2.10 : SGD is more efficient than GD.</p> <p>GD uses all indices \\(i=1, \\ldots, N\\) every iteration</p> \\[ \\theta^{k+1}=\\theta^{k}-\\frac{\\alpha_{k}}{N} \\sum_{i=1}^{N} \\nabla f_{i}\\left(\\theta^{k}\\right) \\] <p>SGD uses only a single random index \\(i(k)\\) every iteration</p> \\[ \\begin{gathered} i(k) \\sim \\text { Uniform }\\{1, \\ldots, N\\} \\\\ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\nabla f_{i(k)}\\left(\\theta^{k}\\right) \\end{gathered} \\] <p>When size of the data \\(N\\) is large, SGD is often more effective than GD.</p> <p>Concept 2.11 : Efficiency of stochastic gradient descent can be expected using the first-order Taylor expansion of \\(F\\).</p> <p>Plug \\(\\theta^{k+1}\\) into Taylor expansion of \\(F\\) about \\(\\theta^{k}\\) :</p> \\[ F\\left(\\theta^{k+1}\\right)=F\\left(\\theta^{k}\\right)-\\alpha_{k} \\nabla F\\left(\\theta^{k}\\right)^{\\top} \\nabla f_{i(k)}\\left(\\theta^{k}\\right)+\\mathcal{O}\\left(\\alpha_{k}^{2}\\right) \\] <p>Take expectation on both sides:</p> \\[ \\mathbb{E}_{k}\\left[F\\left(\\theta^{k+1}\\right)\\right]=F\\left(\\theta^{k}\\right)-\\alpha_{k}\\left\\|\\nabla F\\left(\\theta^{k}\\right)\\right\\|^{2}+\\mathcal{O}\\left(\\alpha_{k}^{2}\\right) \\] <p>( \\(\\mathbb{E}_{k}\\) is expectation conditioned on \\(\\theta^{k}\\) )</p> <p>\\(-\\nabla f_{i(k)}\\left(\\theta^{k}\\right)\\) is descent direction in expectation. For small (cautious) \\(\\alpha_{k}\\), SGD step reduces function value in expectation.</p>"},{"location":"books-and-courses/mfdnn/2/#variants-of-stochastic-gradient-descent","title":"Variants of Stochastic Gradient Descent","text":"<p>Consider</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} f_{i}(\\theta) \\] <p>SGD can be generalized to</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} g^{k} \\] <p>where \\(g^{k}\\) is a stochastic gradient. The choice \\(g^{k}=\\nabla f_{i(k)}\\left(\\theta^{k}\\right)\\) is just one option.</p> <p>Theorem 2.12 : Sampling with Replacement Lemma</p> <p>Let \\(X_{1}, \\ldots, X_{N} \\in \\mathbb{R}^{p}\\) be given (non-random) vectors. Let \\(\\frac{1}{N} \\sum_{i=1}^{N} X_{i}=\\mu\\). Let \\(i(1), \\ldots, i(B) \\subseteq\\{1, \\ldots, N\\}\\) be random indices. Then</p> \\[ \\mathbb{E} \\frac{1}{B} \\sum_{b=1}^{B} X_{i(b)}=\\mu \\] <p>Proof</p> \\[ \\mathbb{E} \\frac{1}{B} \\sum_{b=1}^{B} X_{i(b)}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{E} X_{i(b)}=\\frac{1}{B} \\sum_{b=1}^{B} \\mu=\\mu \\] <p>Definition 2.13 : Minibatch SGD with Replacement</p> <p>Minibatch SGD with replacement</p> \\[ \\begin{gathered} i(k, 1), \\ldots, i(k, B) \\sim \\text { Uniform }\\{1, \\ldots, N\\} \\\\ \\theta^{k+1}=\\theta^{k}-\\frac{\\alpha_{k}}{B} \\sum_{b=1}^{B} \\nabla f_{i(k, b)}\\left(\\theta^{k}\\right) \\end{gathered} \\] <p>To clarify, we sample \\(B\\) out of \\(N\\) indices with replacement, i.e., the same index can be sampled multiple times.</p> <p>By Theorem 2.12, \\(\\frac{1}{B} \\sum_{b=1}^{B} \\nabla f_{i(k, b)}\\left(\\theta^{k}\\right)\\) is a stochastic gradient of \\(F\\) at \\(\\theta^{k}\\).</p> <p>Theorem 2.14  : Sampling without Replacement Lemma</p> <p>Let \\(X_{1}, \\ldots, X_{N} \\in \\mathbb{R}^{p}\\) be given (non-random) vectors. Let \\(\\frac{1}{N} \\sum_{i=1}^{N} X_{i}=\\mu\\). Let \\(\\sigma\\) be a random permutation. Then</p> \\[ \\mathbb{E} \\frac{1}{B} \\sum_{b=1}^{B} X_{\\sigma(b)}=\\mu \\] <p>Proof</p> \\[ \\mathbb{E} \\frac{1}{B} \\sum_{b=1}^{B} X_{\\sigma(b)}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{E} X_{\\sigma(b)}=\\frac{1}{B} \\sum_{b=1}^{B} \\mu=\\mu \\] <p>Definition 2.15 : Minibatch SGD without Replacement</p> <p>Minibatch SGD without replacement</p> \\[ \\begin{gathered} \\sigma^{k} \\sim \\operatorname{permutation}(N) \\\\ \\theta^{k+1}=\\theta^{k}-\\frac{\\alpha_{k}}{B} \\sum_{b=1}^{B} \\nabla f_{\\sigma^{k}(b)}\\left(\\theta^{k}\\right) \\end{gathered} \\] <p>We assume \\(B \\leq N\\). To clarify, we sample \\(B\\) out of \\(N\\) indices without replacement, i.e., the same index cannot be sampled multiple times.</p> <p>By Theorem 2.14, \\(\\frac{1}{B} \\sum_{b=1}^{B} \\nabla f_{\\sigma^{k}(b)}\\left(\\theta^{k}\\right)\\) is a stochastic gradient of \\(F\\) at \\(\\theta^{k}\\).</p> <p>Concept 2.16 : How to choose batch size \\(B\\)?</p> <p>Note \\(B=1\\) minibatch SGD becomes SGD.</p> <p>Mathematically (measuring performance per iteration)</p> <ul> <li>Use large batch is when noise/randomness is large.</li> <li>Use small batch is when noise/randomness is small.</li> </ul> <p>Practically (measuring performance per unit time)</p> <ul> <li>Large batch allows more efficient computation on GPUs.</li> <li>Often best to increase batch size up to the GPU memory limit.</li> </ul> <p>In DL, SGD is applied to nice continuous but non-differentiable functions that are differentiable almost everywhere.</p> <p>In this case, if we choose \\(\\theta^{0} \\in \\mathbb{R}^{n}\\) randomly and run</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\nabla f\\left(\\theta^{k}\\right) \\] <p>the algorithm is usually well-defined, i.e., \\(\\theta^{k}\\) never hits a point of non-differentiability.</p> <p>With a proof or not, GD and SGD are applied to non-differentiable minimization in ML. The absence of differentiability does not seem to cause serious problems.</p> <p>Definition 2.17 : Cyclic SGD</p> <p>Consider the sequence of indices</p> \\[ \\{\\bmod (k, N)+1\\}_{k=0,1, \\ldots}=1,2, \\ldots, N, 1,2, \\ldots, N, \\ldots \\] <p>Here, \\(\\bmod (k, N)\\) is the remainder of \\(k\\) when divided by \\(N\\).</p> <p>Cyclic SGD:</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{\\mathbf{k}} \\nabla f_{\\bmod (k, N)+1}\\left(\\theta^{k}\\right) \\] <p>To clarify, this samples the indices in a (deterministic) cyclic order.</p> <p>Concept 2.18 : Pros and Cons of Cyclic SGD</p> <p>Strictly speaking, cyclic SGD is not an instance of SGD as unbiased estimation property lost.</p> <p>Advantage:</p> <ul> <li>Uses all indices (data) every \\(N\\) iterations.</li> </ul> <p>Disadvantage:</p> <ul> <li>Worse than SGD in some cases, theoretically and empirically.</li> <li>In DL, neural networks can learn to anticipate cyclic order.</li> </ul> <p>Definition 2.19 : Shuffled Cyclic SGD</p> <p>Shuffled Cyclic SGD:</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\nabla f_{\\sigma^{\\left \\lfloor \\frac{k}{N} \\right \\rfloor} (\\bmod (k, N)+1)}\\left(\\theta^{k}\\right) \\] <p>where \\(\\sigma^{0}, \\sigma^{1}, \\ldots\\) is a sequence of random permutations, i.e., we shuffle the order every cycle.</p> <p>Concept 2.19 : Pros and Cons of Shuffled Cyclic SGD</p> <p>Again, strictly speaking, shuffled cyclic SGD is not an instance of SGD as unbiased estimation property lost.</p> <p>Advantages :</p> <ul> <li>Uses all indices (data) every \\(N\\) iterations.</li> <li>Neural network cannot learn to anticipate data order.</li> <li>Empirically best performance.</li> </ul> <p>Disadvantages: - Theory not as strong as regular SGD.</p> <p>Concept 2.20 : Which variant of SGD to use?</p> <p>Theoretical comparison of SGD variants:</p> <ul> <li>Not that easy.</li> <li>Result does not strongly correlate with practical performance in DL.</li> </ul> <p>In DL, the most common choice is</p> <ul> <li>shuffled cyclic minibatch SGD (without replacement) and</li> <li>batchsize \\(B\\) is as large as possible within the GPU memory limit.</li> </ul> <p>One can generally consider this to be the default option.</p> <p>Definition 2.21 : Epoch in finite-sum optimization and machine learning training</p> <p>An epoch is loosely defined as the unit of optimization or training progress of processing all indices or data once.</p> <ul> <li>1 iteration of GD constitutes an epoch.</li> <li>\\(N\\) iterations of SGD, cyclic SGD, or shuffled cyclic SGD constitute an epoch.</li> <li>\\(N / B\\) iterations of minibatch SGD constitute an epoch.</li> </ul> <p>Epoch is often a convenient unit for counting iterations compared to directly counting the iteration number.</p> <p>Concept 2.22 : SGD with General Expectation</p> <p>Consider an optimization problem with its objective defined with a general expectation</p> \\[ \\operatorname{minimize}_{\\theta \\in \\mathbb{R}^{p}} \\quad \\mathbb{E}_{\\omega}\\left[f_{\\omega}(\\theta)\\right]:=F(\\theta) \\] <p>Here, \\(\\omega\\) is a random variable. We will encounter these expectations (non-finite sum) when we talk about generative models.</p> <p>For this setup, the SGD algorithm is</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\nabla f_{\\omega^{k}}\\left(\\theta^{k}\\right) \\] <p>where \\(\\omega^{0}, \\omega^{1}, \\ldots\\) are IID random samples of \\(\\omega\\). If \\(\\nabla_{\\theta} \\mathbb{E}_{\\omega}\\left[f_{\\omega}(\\theta)\\right]=\\mathbb{E}_{\\omega}\\left[\\nabla_{\\theta} f_{\\omega}(\\theta)\\right]\\), then \\(\\nabla f_{\\omega^{k}}\\left(\\theta^{k}\\right)\\) is a stochastic gradient of \\(F(\\theta)\\) at \\(\\theta^{k}\\). (Make sure you understand why the previous SGD for the finite-sum setup is a special case of this.)</p> <p>GD for this setup is</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\mathbb{E}_{\\omega}\\left[\\nabla_{\\theta} f_{\\omega}\\left(\\theta^{k}\\right)\\right] \\] <p>However, if the expectation is difficult to compute GD is impractical and SGD is preferred.</p>"},{"location":"books-and-courses/mfdnn/3/","title":"\u00a7 3. Shallow Neural Networks","text":""},{"location":"books-and-courses/mfdnn/3/#supervised-learning-problem","title":"Supervised Learning Problem","text":"<p>Definition 3.1 : Supervised Learning Setup</p> <p>We have data \\(X_{1}, \\ldots, X_{N} \\in \\mathcal{X}\\) and corresponding labels \\(Y_{1}, \\ldots, Y_{N} \\in \\mathcal{Y}\\).</p> <ul> <li>Example) \\(X_{i}\\) is the \\(i\\) th email and \\(Y_{i} \\in\\{-1,+1\\}\\) denotes whether \\(X_{i}\\) is a spam email.</li> <li>Example) \\(X_{i}\\) is the \\(i\\) th image and \\(Y_{i} \\in\\{0, \\ldots, 9\\}\\) denotes handwritten digit.</li> </ul> <p>Assume there is a true unknown function</p> \\[ f_{\\star}: x \\rightarrow y \\] <p>mapping data to its label. In particular, \\(Y_{i}=f_{\\star}\\left(X_{i}\\right)\\) for \\(i=1, \\ldots, N\\).</p> <p>The goal of supervised learning is to use \\(X_{1}, \\ldots, X_{N}\\) and \\(Y_{1}, \\ldots, Y_{N}\\) to find \\(f \\approx f_{\\star}\\).</p> <p>Definition 3.2 : Supervised Learning Objective</p> <p>Assume a loss function such that \\(\\ell\\left(y_{1}, y_{2}\\right)=0\\) if \\(y_{1}=y_{2}\\) and \\(\\ell\\left(y_{1}, y_{2}\\right)&gt;0\\) if \\(y_{1} \\neq y_{2}\\).</p> <p>Restrict search to a class of parametrized functions \\(f_{\\theta}(x)\\) where \\(\\theta \\in \\Theta \\subseteq \\mathbb{R}^{p}\\), i.e., only consider \\(f \\in\\left\\{f_{\\theta} \\mid \\theta \\in \\Theta\\right\\}\\) where \\(\\Theta \\subseteq \\mathbb{R}^{p}\\).</p> <p>Take a finite sample \\(X_{1}, \\ldots, X_{N} \\in \\mathcal{X}\\) and corresponding labels \\(Y_{1}, \\ldots, Y_{N} \\in \\mathcal{Y}\\). Then solve</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(f_{\\theta}\\left(X_{i}\\right), f_{\\star}\\left(X_{i}\\right)\\right) \\] <p>which is equivalent to</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(f_{\\theta}\\left(X_{i}\\right), Y_{i}\\right) \\] <p>This is the standard form of the optimization problem (except regularizers) we consider in the supervised learning. We will talk about regularizers later.</p> <p>Concept 3.3 : Training is optimization.</p> <p>In machine learning, the anthropomorphized word \"training\" refers to solving an optimization problem such as</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(f_{\\theta}\\left(X_{i}\\right), Y_{i}\\right) \\] <p>In most cases, SGD or variants of SGD are used.</p> <p>We call \\(f_{\\theta}\\) the machine learning model or the neural network.</p> <p>Example 3.4 : Least-Squares Regression</p> <p>In LS, \\(\\mathcal{X}=\\mathbb{R}^{p}, \\mathcal{Y}=\\mathbb{R}, \\Theta=\\mathbb{R}^{p}, f_{\\theta}(x)=x^{\\top} \\theta\\), and \\(\\ell\\left(y_{1}, y_{2}\\right)=\\frac{1}{2}\\left(y_{1}-y_{2}\\right)^{2}\\).</p> <p>So we solve</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{2}\\left(f_{\\theta}\\left(X_{i}\\right)-Y_{i}\\right)^{2}=\\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{2}\\left(X_{i}^{\\top} \\theta-Y_{i}\\right)^{2}=\\frac{1}{2 N}\\|X \\theta-Y\\|^{2} \\] <p>where \\(X=\\left[\\begin{array}{c}X_{1}^{\\top} \\\\ \\vdots \\\\ X_{N}^{\\top}\\end{array}\\right]\\) and \\(Y=\\left[\\begin{array}{c}Y_{1} \\\\ \\vdots \\\\ Y_{N}\\end{array}\\right]\\).</p> <p>The model \\(f_{\\theta}(x)=x^{\\top} \\theta\\) is a shallow neural network. (The terminology will makes sense when contrasted with deep neural networks.)</p>"},{"location":"books-and-courses/mfdnn/3/#linear-classification","title":"Linear Classification","text":"<p>Definition 3.5 : Binary Classification and Linear Separability</p> <p>In binary classification, we have \\(\\mathcal{X}=\\mathbb{R}^{p}\\) and \\(\\mathcal{Y}=\\{-1,+1\\}\\).</p> <p>The data is linearly separable if there is a hyperplane defined by \\((a_{\\text {true }}, b_{\\text {true }} )\\) such that</p> \\[ y=\\left\\{\\begin{array}{cl} 1 &amp; \\text { if } a_{\\text {true }}^{\\top} x+b_{\\text {true }}&gt;0 \\\\ -1 &amp; \\text { otherwise. } \\end{array}\\right. \\] <p> </p>"},{"location":"books-and-courses/mfdnn/3/#support-vector-machine","title":"Support Vector Machine","text":"<p>Consider linear (affine) models</p> \\[ f_{a, b}(x)= \\begin{cases}+1 &amp; \\text { if } a^{\\top} x+b&gt;0 \\\\ -1 &amp; \\text { otherwise }\\end{cases} \\] <p>Consider the loss function</p> \\[ \\ell\\left(y_{1}, y_{2}\\right)=\\frac{1}{2}\\left|1-y_{1} y_{2}\\right|= \\begin{cases}0 &amp; \\text { if } y_{1}=y_{2} \\\\ 1 &amp; \\text { if } y_{1} \\neq y_{2}\\end{cases} \\] <p>The optimization problem</p> \\[ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(f_{a, b}\\left(X_{i}\\right), Y_{i}\\right) \\] <p>has a solution with optimal value 0 when the data is linearly separable.</p> <p>Problem: Optimization problem is discontinuous and thus cannot be solved with SGD.</p> <p>Motivation for SVM</p> <p>Even if the underlying function or phenomenon to approximate is discontinuous, the model needs to be continuous in its parameters. The loss function also needs to be continuous. (The prediction need not be continuous.)</p> <p>We consider a relaxation, is a continuous proxy of the discontinuous thing. Specifically, consider</p> \\[ f_{a, b}(x)=a^{\\top} x+b \\] <p>Once trained, \\(f_{a, b}(x)&gt;0\\) means the neural network is predicting \\(y=+1\\) to be \"more likely\", and \\(f_{a, b}(x)&lt;0\\) means the neural network is predicting \\(y=-1\\) to be \"more likely\".</p> <p>Therefore, we train the model to satisfy</p> \\[ Y_{i} f_{a, b}\\left(X_{i}\\right)&gt;0 \\text { for } i=1, \\ldots, N . \\] <p>Problem with strict inequality \\(Y_{i} f_{a, b}\\left(X_{i}\\right)&gt;0\\) :</p> <ul> <li>Strict inequality has numerical problems with round-off error.</li> <li>The magnitude \\(\\left|f_{a, b}(x)\\right|\\) can be viewed as the confidence of the prediction, but having a small positive value for \\(Y_{i} f_{a, b}\\left(X_{i}\\right)\\) indicates small confidence of the neural network.</li> </ul> <p>We modify our model's desired goal to be \\(Y_{i} f_{a, b}\\left(X_{i}\\right) \\geq 1\\).</p> <p>To train the neural network to satisfy</p> \\[ 0 \\geq 1-Y_{i} f_{a, b}\\left(X_{i}\\right) \\text { for } i=1, \\ldots, N . \\] <p>we minimize the excess positive component of the RHS</p> \\[ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\max \\left\\{0,1-Y_{i} f_{a, b}\\left(X_{i}\\right)\\right\\} \\] <p>which is equivalent to</p> \\[ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\max \\left\\{0,1-Y_{i}\\left(a^{\\top} X_{i}+b\\right)\\right\\} \\] <p>If the optimal value is 0, then the data is linearly separable.</p> <p>Definition 3.6 : Support Vector Machine (SVM)</p> <p>Use the model</p> \\[ f_{a, b}(x)=a^{\\top} x+b \\] <p>This following formulation is called the support vector machine (SVM)</p> \\[ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\max \\left\\{0,1-Y_{i} f_{a, b}\\left(X_{i}\\right)\\right\\} \\] \\[ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\max \\left\\{0,1-Y_{i}\\left(a^{\\top} X_{i}+b\\right)\\right\\} \\] <p>It is also common to add a regularizer</p> \\[ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\max \\left\\{0,1-Y_{i}\\left(a^{\\top} X_{i}+b\\right)\\right\\}+\\frac{\\lambda}{2}\\|a\\|^{2} \\] <p>Concept 3.7 : Prediction with SVM</p> <p>Once the SVM is trained, make predictions with</p> \\[ \\operatorname{sign}\\left(f_{a, b}(x)\\right)=\\operatorname{sign}\\left(a^{\\top} x+b\\right) \\] <p>when \\(f_{a, b}(x)=0\\), we assign \\(\\operatorname{sign}\\left(f_{a, b}(x)\\right)\\) arbitrarily.</p> <p>Note that the prediction is discontinuous, but predictions are in \\(\\{-1,+1\\}\\) so it must be discontinuous.</p> <p>If \\(\\sum_{i=1}^{N} \\max \\left\\{0,1-Y_{i} f_{a, b}\\left(X_{i}\\right)\\right\\}=0\\), then \\(\\operatorname{sign}\\left(f_{a, b}\\left(X_{i}\\right)\\right)=Y_{i}\\) for \\(i=1, \\ldots, N\\), i.e., the neural network predicts the known labels perfectly. Of course, it is a priori not clear how accurate the prediction will be for new unseen data.</p>"},{"location":"books-and-courses/mfdnn/3/#logistic-regression","title":"Logistic Regression","text":"<p>Concept 3.8 : Relaxed Supervised Learning Setup</p> <p>We relax the supervised learning setup to predict probabilities, rather than make point predictions. So, labels are generated based on data, perhaps randomly. Consider data \\(X_{1}, \\ldots, X_{N} \\in \\mathcal{X}\\) and labels \\(Y_{1}, \\ldots, Y_{N} \\in \\mathcal{Y}\\). Assume there exists a function</p> \\[ f_{\\star}: \\mathcal{X} \\rightarrow \\mathcal{P}(\\mathcal{Y}) \\] <p>where \\(\\mathcal{P}(\\mathcal{Y})\\) denotes the set of probability distributions on \\(\\mathcal{Y}\\). Assume the generation of \\(Y_{i}\\) given \\(X_{i}\\) is independent of \\(Y_{j}\\) and \\(X_{j}\\) for \\(j \\neq i\\).</p> <ul> <li>Example) \\(f(X)=\\left[\\begin{array}{l}0.8 \\\\ 0.2\\end{array}\\right]\\) in dog vs. cat classifier.</li> <li>Example) An email saying \"Buy this thing at our store!\" may be spam to some people, but it may not be spam to others.</li> </ul> <p>The relaxed SL setup is more general and further realistic.</p> <p>Definition 3.9 : Empirical Distribution for Binary Classification</p> <p>In basic binary classification, define the empirical distribution</p> \\[ \\mathcal{P}(y)= \\begin{cases}{\\left[\\begin{array}{l} 1 \\\\ 0 \\end{array}\\right]} &amp; \\text { if } y=-1 \\\\ {\\left[\\begin{array}{l} 0 \\\\ 1 \\end{array}\\right]} &amp; \\text { if } y=+1\\end{cases} \\] <p>More generally, the empirical distribution describes the data we have seen. In this context, we have only seen one label per datapoint, so our empirical distributions are one-hot vectors.</p> <p>(If there are multiple annotations per data point \\(x\\) and they don't agree, then the empirical distribution may not be one-hot vectors. For example, given the same email, some users may flag it as spam while others consider it useful information.)</p> <p>Definition 3.10 : KL-Divergence, Cross Entropy</p> <p>Let \\(p, q \\in \\mathbb{R}^{n}\\) represent probability masses, i.e., \\(p_{i} \\geq 0\\) for \\(i=1, \\ldots, n\\) and \\(\\sum_{i=1}^{n} p_{i}=1\\) and the same for \\(q\\). The Kullback-Leibler-divergence (KL-divergence) from \\(q\\) to \\(p\\) is</p> \\[ \\begin{array}{ll} D_{\\mathrm{KL}}(p \\| q)=\\displaystyle \\sum_{i=1}^{n} p_{i} \\log \\left(\\frac{p_{i}}{q_{i}}\\right)= &amp; -\\displaystyle \\sum_{i=1}^{n} p_{i} \\log \\left(q_{i}\\right) &amp; +\\displaystyle \\sum_{i=1}^{n} p_{i} \\log \\left(p_{i}\\right) \\\\ &amp; =H(p, q) &amp; =-H(p) \\\\ &amp; \\text { cross entropy of } q &amp; =-H \\\\ &amp; \\text { relative to } p &amp; \\text { entropy of } p \\end{array} \\] <p>The cross entropy of \\(q\\) relative to \\(p\\) is</p> \\[ H(p, q) = -\\sum_{i=1}^{n} p_{i} \\log \\left(q_{i}\\right) \\] <p>Theorem 3.11 : Properties of KL-Divergence</p> \\[ D_{\\mathrm{KL}}(p \\| q)=\\sum_{i=1}^{n} p_{i} \\log \\left(\\frac{p_{i}}{q_{i}}\\right) \\] <ul> <li>Not symmetric, i.e., \\(D_{\\mathrm{KL}}(p \\| q) \\neq D_{\\mathrm{KL}}(q \\| p)\\).</li> <li>\\(D_{\\mathrm{KL}}(p \\| q)&gt;0\\) if \\(p \\neq q\\) and \\(D_{\\mathrm{KL}}(p \\| q)=0\\) if \\(p=q\\).</li> <li>\\(D_{\\mathrm{KL}}(p \\| q)=\\infty\\) is possible. (Further detail below.)</li> </ul> <p>Often used as a \"distance\" between \\(p\\) and \\(q\\) despite not being a metric.</p> <p>Clarification: Use the convention</p> <ul> <li>\\(0 \\log \\left(\\frac{0}{0}\\right)=0\\left(\\right.\\) when \\(\\left.p_{i}=q_{i}=0\\right)\\)</li> <li>\\(0 \\log \\left(\\frac{0}{q_{i}}\\right)=0\\) if \\(q_{i}&gt;0\\)</li> <li>\\(p_{i} \\log \\left(\\frac{p_{i}}{0}\\right)=\\infty\\) if \\(p_{i}&gt;0\\)</li> </ul> <p>Probabilistic interpretation:</p> \\[ D_{\\mathrm{KL}}(p \\| q)=\\mathbb{E}_{I}\\left[\\log \\left(\\frac{p_{I}}{q_{I}}\\right)\\right] \\] <p>with the random variable \\(I\\) such that \\(\\mathbb{P}(I=i)=p_{i}\\).</p> <p>Definition 3.12 : Logistic Regression (LR)</p> <p>Logistic regression (LR), is another model for binary classification:</p> <p>Use the model</p> \\[ f_{a, b}(x)= \\mu\\left(\\left[\\begin{array}{c}0 \\\\ a^{\\top} x+b \\end{array}\\right]\\right) = \\left[\\begin{array}{c} \\frac{1}{1+e^{a^{\\top} x+b}} \\\\ \\frac{e^{a^{\\top} x+b}}{1+e^{a^{\\top} x+b}} \\end{array}\\right]=\\left[\\begin{array}{c} \\frac{1}{1+e^{a^{\\top} x+b}} \\\\ \\frac{1}{1+e^{-\\left(a^{\\top} x+b\\right)}} \\end{array}\\right]\\begin{array}{c} = \\mathbb{P}(y=-1) \\\\ = \\mathbb{P}(y=+1) \\end{array} \\] <p>Minimize KL-Divergence (or cross entropy) from the model \\(f_{a, b}\\left(X_{i}\\right)\\) output probabilities to the empirical distribution \\(\\mathcal{P}\\left(Y_{i}\\right)\\).</p> \\[ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\sum_{i=1}^{N} D_{\\mathrm{KL}}\\left(\\mathcal{P}\\left(Y_{i}\\right) \\| f_{a, b}\\left(X_{i}\\right)\\right) \\] <p>Concept 3.13 : Other Expression of Logistic Regression</p> \\[ \\begin{gathered} \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\sum_{i=1}^{N} D_{\\mathrm{KL}}\\left(\\mathcal{P}\\left(Y_{i}\\right) \\| f_{a, b}\\left(X_{i}\\right)\\right) \\\\ \\mathbb{\\Updownarrow} \\\\ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\sum_{i=1}^{N} H\\left(\\mathcal{P}\\left(Y_{i}\\right), f_{a, b}\\left(X_{i}\\right)\\right)+(\\text { terms independent of } a, b) \\\\ \\mathbb{\\Updownarrow} \\\\ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\sum_{i=1}^{N} \\log \\left(1+\\exp \\left(-Y_{i}\\left(a^{\\top} X_{i}+b\\right)\\right)\\right) \\\\ \\mathbb{\\Updownarrow} \\\\ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(Y_{i}\\left(a^{\\top} X_{i}+b\\right)\\right) \\end{gathered} \\] <p>where \\(\\ell(z)=\\log \\left(1+e^{-z}\\right)\\).</p> <p>Concept 3.14 : Prediction with LR</p> <p>When performing point prediction with \\(\\mathrm{LR}, a^{\\top} x+b&gt;0\\) means \\(\\mathbb{P}(y=+1)&gt;0.5\\) and vice versa.</p> <p>Once the LR is trained, make predictions with</p> \\[ \\operatorname{sign}\\left(a^{\\top} x+b\\right) \\] <p>when \\(a^{\\top} x+b=0\\), we assign \\(\\operatorname{sign}\\left(a^{\\top} x+b\\right)\\) arbitrarily. This is the same as SVM.</p> <p>Again, it is a priori not clear how accurate the prediction will be for new unseen data.</p> <p>Concept 3.15 : SVM vs LR</p> <p>Both support vector machine and logistic regression can be written as</p> \\[ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(Y_{i}\\left(a^{\\top} X_{i}+b\\right)\\right) \\] <ul> <li>SVM uses \\(\\ell(z)=\\max \\{0,1-z\\}\\). Obtained from relaxing the discontinuous prediction loss.</li> <li>LR uses \\(\\ell(z)=\\log \\left(1+e^{-z}\\right)\\). Obtained from relaxing the supervised learning setup from predicting the label to predicting the label probabilities.</li> </ul> <p> </p> <p>SVM and LR are both \"linear\" classifiers:</p> <ul> <li>Decision boundary \\(a^{\\top} x+b=0\\) is linear.</li> <li>Model completely ignores information perpendicular to \\(a\\).</li> </ul> <p>LR naturally generalizes to multi-class classification via softmax regression. Generalizing SVM to multi-class classification is trickier and less common.</p> <p>Concept 3.16 : Maximum Likelihood Estimation \\(\\cong\\) minimizing KL divergence</p> <p>Consider the setup where you have IID discrete random variables \\(X_{1}, \\ldots, X_{N}\\) that can take values \\(1, \\ldots, k\\). We model the probability masses with \\(\\mathbb{P}_{\\theta}(X=1), \\ldots, \\mathbb{P}_{\\theta}(X=k)\\). The maximum likelihood estimation (MLE) is obtained by solving</p> \\[ \\underset{\\theta}{\\operatorname{maximize}} \\frac{1}{N} \\sum_{i=1}^{N} \\log \\left(\\mathbb{P}_{\\theta}\\left(X_{i}\\right)\\right) \\] <p>Next, define</p> \\[ f_{\\theta}=\\left[\\begin{array}{c} \\mathbb{P}_{\\theta}(X=1) \\\\ \\vdots \\\\ \\mathbb{P}_{\\theta}(X=k) \\end{array}\\right], \\quad \\mathcal{P}\\left(X_{1}, \\ldots, X_{N}\\right)=\\frac{1}{N}\\left[\\begin{array}{c} \\#\\left(X_{i}=1\\right) \\\\ \\vdots \\\\ \\#\\left(X_{i}=k\\right) \\end{array}\\right] . \\] <p>Then MLE is equivalent to minimizing the KL divergence from the model to the empirical distribution.</p> \\[ \\begin{gathered} \\text{MLE} \\\\ \\mathbb{\\Updownarrow} \\\\ \\underset{\\theta}{\\operatorname{minimize}} H \\left( \\mathcal{P}\\left(X_{1}, \\ldots, X_{N}\\right), f_{\\theta}\\right) \\\\ \\mathbb{\\Updownarrow} \\\\ \\underset{\\theta}{\\operatorname{minimize}} D_{\\mathrm{KL}} \\left( \\mathcal{P}\\left(X_{1}, \\ldots, X_{N}\\right), f_{\\theta}\\right) \\\\ \\end{gathered} \\] <p>One can also derive LR equivalently as the MLE.</p> <p>Generally, one can view the MLE as minimizing the KL divergence between the model and the empirical distribution. (For continuous random variables like the Gaussian, this requires extra work, since we haven't defined the KL divergence for continuous random variables.)</p> <p>In deep learning, the distance measure need not be KL divergence.</p>"},{"location":"books-and-courses/mfdnn/3/#prediction","title":"Prediction","text":"<p>Definition 3.17 : Estimation, Prediction</p> <p>Finding \\(f \\approx f_{\\star}\\) for unknown</p> \\[ f_{\\star}: \\mathcal{X} \\rightarrow \\mathcal{P}(\\mathcal{Y}) \\] <p>is called estimation. When we consider a parameterized model \\(f_{\\theta}\\), finding \\(\\theta\\) is the estimation. However, estimation is usually not the end goal.</p> <p>The end goal is prediction. It is to use \\(f_{\\theta} \\approx f_{\\star}\\) on new data \\(X_{1}^{\\prime}, \\ldots, X_{M}^{\\prime} \\in \\mathcal{X}\\) to find labels \\(Y_{1}^{\\prime}, \\ldots, Y_{M}^{\\prime} \\in \\mathcal{Y}\\).</p> <p>Concept 3.18 : Is prediction possible?</p> <p>In the worst hypotheticals, prediction is impossible.</p> <ul> <li>Even though smoking is harmful for every other human being, how can we be 100% sure that this one person is not a mutant who benefits from the chemicals of a cigarette?</li> <li>Water freezes at \\(0^{\\circ}\\), but will the same be true tomorrow? How can we be 100% sure that the laws of physics will not suddenly change tomorrow?</li> </ul> <p>Of course, prediction is possible in practice.</p> <p>Theoretically, prediction requires assumptions on the distribution of \\(X\\) and the model of \\(f_{\\star}\\) is needed. This is in the realm of statistics of statistical learning theory.</p> <p>For now, we will take the view that if we predict known labels of the training data, we can reasonably hope to do well on the new data. (We will discuss the issue of generalization and overfitting later.)</p> <p>Concept 3.19 : Training Data vs Test Data</p> <p>When testing a machine learning model, it is essential that one separates the training data with the test data.</p> <p>In other classical disciplines using data, one performs a statistical hypothesis test to obtain a \\(p\\)-value. In ML, people do not do that.</p> <p>The only sure way to ensure that the model is doing well is to assess its performance on new data.</p> <p>Usually, training data and test data is collected together. This ensures that they have the same statistical properties. The assumption is that this test data will be representative of the actual data one intends to use machine learning on.</p>"},{"location":"books-and-courses/mfdnn/3/#datasets","title":"Datasets","text":"<p>Concept 3.20 : MNIST</p> <p>Images of hand-written digits with \\(28 \\times 28=784\\) pixels and integervalued intensity between 0 and 255 . Every digit has a label in \\(\\{0,1, \\ldots, 8,9\\}\\).</p> <p>70,000 images (60,000 for training / 10,000 testing) of 10 almost balanced classes.</p> <p>One of the simplest data set used in machine learning.</p> <p> </p> <p>The USA government needed a standardized test to assess handwriting recognition software being sold to the government. So the NIST (National Institute of Standards and Technology) created the dataset in the 1990s. In 1990, NIST Special Database 1 distributed on CD-ROMs by mail. NIST SD 3 (1992) and SD 19 (1995) were improvements.</p> <p>Humans were instructed to fill out handwriting sample forms. However, humans cannot be trusted to follow instructions, so a lab technician performed \"human ground truth adjudication\".</p> <p>In 1998, Man LeCun, Corinna Cortes, Christopher J. C. Barges took the NIST dataset and modified it to create the MNIST dataset.</p> <p>Concept 3.21 : CIFAR10</p> <p>60,000 \\(32 \\times 32\\) color images in 10 (perfectly) balanced classes.</p> <p> </p> <p>(There is no overlap between automobiles and trucks. \u201cAutomobile\u201d includes sedans, SUVs, things of that sort. \u201cTruck\u201d includes only big trucks. Neither includes pickup trucks.)</p> <p>In 2008, a MIT and NYU team created the 80 million tiny images data set by searching on Google, Flickr, and Altavista for every non-abstract English noun and downscaled the images to \\(32 \\times 32\\). The search term provided an unreliable label for the image. This dataset was not very easy to use since the classes were too numerous.</p> <p>In 2009, Alex Krizhevsky published the CIFAR10, by distilling just a few classes and cleaning up the labels. Students were paid to verify the labels.</p> <p>The dataset was named CIFAR-10 after the funding agency Canadian Institute For Advanced Research.</p> <p>There is also a CIFAR-100 with 100 classes.</p> <p>Concept 3.22 : Roles of Datasets in ML Research</p> <p>An often underappreciated contribution.</p> <p>Good datasets play a crucial role in driving progress in ML research.</p> <p>Thinking about the dataset is the essential first step of understanding the feasibility of a ML task.</p> <p>Accounting for the cost of producing datasets and leveraging freely available data as much as possible (semi-supervised learning) is a recent trend in machine learning.</p>"},{"location":"books-and-courses/mfdnn/4/","title":"\u00a7 4. Deep Neural Networks","text":""},{"location":"books-and-courses/mfdnn/4/#deep-neural-networks","title":"Deep Neural Networks","text":"<p>Concept 4.1 : LR can be seen as 1-layer (shallow) neural network.</p> <p>In LR, we solve</p> \\[ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(f_{\\theta}\\left(X_{i}\\right), Y_{i}\\right) \\] <p>where \\(\\ell\\left(y_{1}, y_{2}\\right)=\\log \\left(1+e^{-y_{1} y_{2}}\\right)\\) and \\(f_{\\theta}\\) is linear.</p> <p>We can view \\(f_{\\theta}(x)=O=a^{\\top} x+b\\) as a 1-layer (shallow) neural network.</p> <p> </p> <p>Concept 4.2 : Deep Neural Networks with Nonlinearities</p> <p>What happens if we stack multiple linear layers?</p> <p>Problem: This is pointless because composition of linear functions is linear. ( \\(O = W_2 h = W_2(W_1 x) = (W_2 W_1) x \\leftarrow\\) linear in \\(x\\) )</p> <p>Solution: use a nonlinear activation function \\(\\sigma\\) to inject nonlinearities.</p> <p> <p>Hidden layer \\(h=\\sigma(W_1 x)\\) (\\(\\sigma\\) : Nonlinear function) Output layer \\(O=W_2 h = W_2 \\sigma(W_1 x) \\leftarrow\\) nonlinear in \\(x\\) </p> <p>Definition 4.3 : Common Activation Functions</p> <ul> <li> <p>Rectified Linear Unit (ReLU)</p> \\[\\operatorname{ReLU}(z)=\\max (z, 0)\\] <p> </p> </li> <li> <p>Sigmoid</p> \\[\\operatorname{Sigmoid}(z)=\\frac{1}{1+e^{-z}}\\] <p> </p> </li> <li> <p>Hyperbolic Tangent</p> \\[\\tanh (z)=\\frac{1-e^{-2 z}}{1+e^{-2 z}}\\] <p> </p> </li> </ul> <p>Definition 4.4 : Multilayer Perceptron (MLP)</p> <p>The multilayer perceptron, also called fully connected neural network, has the form</p> \\[ \\begin{aligned} y_{L}= &amp; W_{L} y_{L-1}+b_{L} \\\\ y_{L-1}= &amp; \\sigma\\left(W_{L-1} y_{L-2}+b_{L-1}\\right) \\\\ &amp; \\vdots \\\\ y_{2}= &amp; \\sigma\\left(W_{2} y_{1}+b_{2}\\right) \\\\ y_{1}= &amp; \\sigma\\left(W_{1} x+b_{1}\\right), \\end{aligned} \\] <p>where \\(x \\in \\mathbb{R}^{n_{0}}, W_{\\ell} \\in \\mathbb{R}^{n_{\\ell} \\times n_{\\ell-1}}, b_{\\ell} \\in \\mathbb{R}^{n_{\\ell}}\\), and \\(n_{L}=1\\). To clarify, \\(\\sigma\\) is applied element-wise.</p> <p>Definition 4.5 : Linear Layer (with Batches)</p> <ul> <li>Input tensor: \\(X \\in \\mathbb{R}^{B \\times n}, B\\) batch size, \\(n\\) number of indices.</li> <li>Output tensor: \\(Y \\in \\mathbb{R}^{B \\times m}, B\\) batch size, \\(m\\) number of indices.</li> </ul> <p>With weight \\(A \\in \\mathbb{R}^{m \\times n}\\), bias \\(b \\in \\mathbb{R}^{m}, k=1, \\ldots B\\), and \\(i=1, \\ldots, m\\) :</p> \\[ Y_{k, i}=\\sum_{j=1}^{n} A_{i, j} X_{k, j}+b_{i} \\] <p>Operation is independent across elements of the batch.</p> <p>If <code>bias=False</code>, then \\(b=0\\).</p>"},{"location":"books-and-courses/mfdnn/4/#multi-class-classification","title":"Multi-Class Classification","text":"<p>Definition 4.6 : Multi-Class Classification Problem</p> <p>Consider supervised learning with data \\(X_{1}, \\ldots, X_{N} \\in \\mathbb{R}^{n}\\) and labels \\(Y_{1}, \\ldots, Y_{N} \\in\\{1, \\ldots, k\\}\\). (A \\(k\\) class classification problem.) Assume there exists a function \\(f_{\\star}: \\mathbb{R}^{n} \\rightarrow \\Delta^{k}\\) mapping from data to label probabilities. Here, \\(\\Delta^{k} \\subset \\mathbb{R}^{k}\\) denotes the set of probability mass functions on \\(\\{1, \\ldots, k\\}\\).</p> <p>Define the empirical distribution \\(\\mathcal{P}(y) \\in \\mathbb{R}^{k}\\) as the one-hot vector:</p> \\[ (\\mathcal{P}(y))_{i}=\\left\\{\\begin{array}{cc} 1 &amp; \\text { if } y=i \\\\ 0 &amp; \\text { otherwise } \\end{array}\\right. \\] <p>for \\(i=1, \\ldots, k\\).</p> <p>Definition 4.7 : Softmax Function</p> <p>Softmax function \\(\\mu: \\mathbb{R}^{k} \\rightarrow \\Delta^{k}\\) is defined by</p> \\[ \\mu_{i}(z)=(\\mu(z))_{i}=\\frac{e^{z_{i}}}{\\sum_{j=1}^{k} e^{z_{j}}} \\] <p>for \\(i=1, \\ldots, k\\), where \\(z=\\left(z_{1}, \\ldots, z_{k}\\right) \\in \\mathbb{R}^{k}\\). Since</p> \\[ \\sum_{i=1}^{k} \\mu_{i}(z)=1, \\quad \\mu&gt;0 \\] <p>Name \"softmax\" is a misnomer. \"Softargmax\" would be more accurate</p> <ul> <li>\\(\\mu(z) \\not \\approx \\max (z)\\)</li> <li>\\(\\mu(z) \\approx \\operatorname{argmax}(z)\\)</li> </ul> <p>Examples:</p> \\[ \\mu\\left(\\left[\\begin{array}{l} 1 \\\\ 2 \\\\ 3 \\end{array}\\right]\\right)=\\left[\\begin{array}{l} 0.09 \\\\ 0.24 \\\\ 0.6 \\end{array}\\right], \\mu\\left(\\left[\\begin{array}{c} 999 \\\\ 0 \\\\ -2 \\end{array}\\right]\\right) \\approx\\left[\\begin{array}{l} 1 \\\\ 0 \\\\ 0 \\end{array}\\right], \\mu\\left(\\left[\\begin{array}{c} -2 \\\\ -2 \\\\ -99 \\end{array}\\right]\\right) \\approx\\left[\\begin{array}{c} 0.5 \\\\ 0.5 \\\\ 0 \\end{array}\\right] \\] <p>Definition 4.8 : Softmax Regression (SR)</p> <p>In softmax regression (SR):</p> <p>Choose the model</p> \\[ \\mu\\left(f_{A, b}(x)\\right)=\\frac{1}{\\sum_{i=1}^{k} e^{a_{i}^{\\top} x+b_{i}}}\\left[\\begin{array}{c} e^{a_{1}^{\\top} x+b_{1}} \\\\ e^{a_{2}^{\\top} x+b_{2}} \\\\ \\vdots \\\\ e^{a_{k}^{\\top} x+b_{k}} \\end{array}\\right], \\quad f_{A, b}(x)=A x+b, A=\\left[\\begin{array}{c} a_{1}^{\\top} \\\\ a_{2}^{\\top} \\\\ \\vdots \\\\ a_{k}^{\\top} \\end{array}\\right] \\in \\mathbb{R}^{k \\times n}, \\quad b=\\left[\\begin{array}{c} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{k} \\end{array}\\right] \\in \\mathbb{R}^{k} . \\] <p>Minimize KL-Divergence (or cross entropy) from the model \\(\\mu\\left(f_{A, b}\\left(X_{i}\\right)\\right)\\) output probabilities to the empirical distribution \\(\\mathcal{P}\\left(Y_{i}\\right)\\).</p> \\[ \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} \\sum_{i=1}^{N} D_{\\mathrm{KL}}\\left(\\mathcal{P}\\left(Y_{i}\\right) \\| \\mu\\left(f_{A, b}\\left(X_{i}\\right)\\right)\\right) \\Longleftrightarrow \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} \\sum_{i=1}^{N} H\\left(\\mathcal{P}\\left(Y_{i}\\right), \\mu\\left(f_{A, b}\\left(X_{i}\\right)\\right)\\right) \\] <p>Concept 4.9 : Other Expression of Softmax Regression</p> \\[ \\begin{gathered} \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} \\sum_{i=1}^{N} H\\left(\\mathcal{P}\\left(Y_{i}\\right), \\mu\\left(f_{A, b}\\left(X_{i}\\right)\\right)\\right) \\\\ \\mathbb{\\Updownarrow} \\\\ \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N}-\\log \\left(\\mu_{Y_{i}}\\left(f_{A, b}\\left(X_{i}\\right)\\right)\\right) \\\\ \\mathbb{\\Updownarrow} \\\\ \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N}-\\log \\left(\\frac{\\exp \\left(a_{Y_{i}}^{\\top} X_{i}+b_{Y_{i}}\\right)}{\\sum_{j=1}^{k} \\exp \\left(a_{j}^{\\top} X_{i}+b_{j}\\right)}\\right) \\\\ \\mathbb{\\Updownarrow} \\\\ \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N}\\left(-\\left(a_{Y_{i}}^{\\top} X_{i}+b_{Y_{i}}\\right)+\\log \\left(\\sum_{j=1}^{k} \\exp \\left(a_{j}^{\\top} X_{i}+b_{j}\\right)\\right)\\right) \\end{gathered} \\] <p>Definition 4.10 : Cross Entropy Loss</p> <p>Where \\(f \\in \\mathbb{R}^{k}, y \\in \\{1, 2, \\cdots, k\\}\\), the cross entropy loss is</p> \\[ \\ell^{\\mathrm{CE}}(f, y)=-\\log \\left(\\frac{\\exp \\left(f_{y}\\right)}{\\sum_{j=1}^{k} \\exp \\left(f_{j}\\right)}\\right) \\] <p>Concept 4.11 : SR uses cross entropy loss as loss function.</p> \\[ \\begin{gathered} \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} \\sum_{i=1}^{N} H\\left(\\mathcal{P}\\left(Y_{i}\\right), \\mu\\left(f_{A, b}\\left(X_{i}\\right)\\right)\\right) \\\\ \\mathbb{\\Updownarrow} \\\\ \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N}-\\log \\left(\\frac{\\exp \\left(a_{Y_{i}}^{\\top} X_{i}+b_{Y_{i}}\\right)}{\\sum_{j=1}^{k} \\exp \\left(a_{j}^{\\top} X_{i}+b_{j}\\right)}\\right) \\\\ \\mathbb{\\Updownarrow} \\\\ \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell^{\\mathrm{CE}}\\left(f_{A, b}\\left(X_{i}\\right), Y_{i}\\right) \\end{gathered} \\] <ul> <li>SR = linear model \\(f_{A, b}\\) with cross entropy loss:</li> </ul> \\[ \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell^{\\mathrm{CE}}\\left(f_{A, b}\\left(X_{i}\\right), Y_{i}\\right) \\Longleftrightarrow \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} \\sum_{i=1}^{N} D_{\\mathrm{KL}}\\left(\\mathcal{P}\\left(Y_{i}\\right) \\| \\mu\\left(f_{A, b}\\left(X_{i}\\right)\\right)\\right) \\] <ul> <li> <p>The natural extension of SR is to consider</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell^{\\mathrm{CE}}\\left(f_{\\theta}\\left(X_{i}\\right), Y_{i}\\right) \\Leftrightarrow \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\quad \\sum_{i=1}^{N} D_{\\mathrm{KL}}\\left(\\mathcal{P}\\left(Y_{i}\\right) \\| \\mu\\left(f_{\\theta}\\left(X_{i}\\right)\\right)\\right) \\] <p>where \\(f_{\\theta}\\) is a deep neural network.</p> </li> </ul>"},{"location":"books-and-courses/mfdnn/4/#gpus-in-deep-learning","title":"GPUs in Deep Learning","text":"<p>Concept 4.12 : History of GPU Computing</p> <p>Rendering graphics involves computing many small tasks in parallel. Graphics cards provide many small processors to render graphics.</p> <p>In 1999, Nvidia released GeForce 256 and introduced programmability in the form of vertex and pixel shaders. Marketed as the first 'Graphical Processing Unit (GPU)'.</p> <p>Researchers quickly learned how to implement linear algebra by mapping matrix data into textures and applying shaders.</p> <p>In 2007, Nvidia released 'Compute Unified Device Architecture (CUDA)', which enabled general purpose computing on a CUDA-enabled GPUs.</p> <p>Unlike CPUs which provide fast serial processing, GPUs provide massive parallel computing with its numerous slower processors.</p> <p>The 2008 financial crisis hit Nvidia very hard as GPUs were luxury items used for games. This encouraged Nvidia to invest further in 'General Purpose GPUs (GPGPU)' and create a more stable consumer base.</p> <p>Concept 4.13 : CPU Computing Model</p> <p> </p> <p>Concept 4.14 : GPU Computing Model</p> <p> </p> <p>Concept 4.15 : GPUs for Machine Learning</p> <p>Raina et al.'s 2009* paper demonstrated that GPUs can be used to train large neural networks. (This was not the first to use of GPUs in machine learning, but it was one of the most influential.)</p> <p>Modern deep learning is driven by big data and big compute, respectively provided by the internet and GPUs.</p> <p>Krizhevsky et al.'s 2012 landmark paper introduced AlexNet trained on GPUs and kickstarted the modern deep learning boom.</p> <p>(R. Raina, A. Madhavan, and A. Y. Ng , Large-scale Deep Unsupervised Learning using Graphics Processors, ICML, 2009. /  A. Krizhevsky, I. Sutskever, G. E. Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NeurIPS, 2012.)</p> <p>Concept 4.16 : Deep Learning on GPUs</p> <p>Steps for training neural network on GPU:</p> <ol> <li>Create the neural network on CPU and send it to GPU. Neural network parameters stay on GPU.<ul> <li>Sometimes you load parameters from CPU to GPU.</li> </ul> </li> <li>Select data batch (image, label) and send it to GPU every iteration<ul> <li>Data for real-world setups is large, so keeping all data on GPU is infeasible.</li> </ul> </li> <li>On GPU, compute network output (forward pass)</li> <li>On GPU, compute gradients (backward pass)</li> <li>On GPU, perform gradient update</li> <li>Once trained, perform prediction on GPU.<ul> <li>Send test data to GPU.</li> <li>Compute network output.</li> <li>Retrieve output on CPU.</li> <li>Alternatively, neural network can be loaded on CPU and prediction can be done on CPU.</li> </ul> </li> </ol>"},{"location":"books-and-courses/mfdnn/5/","title":"\u00a7 5. Convolutional Neural Networks","text":""},{"location":"books-and-courses/mfdnn/5/#convolutional-layers","title":"Convolutional Layers","text":"<p>Concept 5.1 : Pros and Cons of Fully Connected Layers</p> <p>Advantages of fully connected layers:</p> <ul> <li>Simple.</li> <li>Very general, in theory. (Sufficiently large MLPs can learn any function, in theory.)</li> </ul> <p>Disadvantage of fully connected layers:</p> <ul> <li>Too many trainable parameters.</li> <li>Does not encode shift equivariance/invariance and therefore has poor inductive bias. (More on this later.)</li> </ul> <p>Concept 5.2 : Shift Equivarience/Invariance in Vision</p> <p>Many tasks in vision are equivariant/invariant with respect shifts/translations.</p> <p> </p> <p>Roughly speaking, equivariance/invariance means shifting the object does not change the meaning (it only changes the position).</p> <p>Logistic regression (with a single fully connected layer) does not encode shift invariance.</p> <p>Since convolution is equivariant with respect to translations, constructing neural network layers with them is a natural choice.</p> <p>Definition 5.3 : 2D Convolutional Layer</p> <ul> <li>\\(B\\) : batch size</li> <li>\\(C_{\\text{in}}\\) : # of input channels</li> <li>\\(C_{\\text{out}}\\) : # of output channels</li> <li>\\(m, n\\) : # of vertical and horizontal indices of input</li> <li>\\(f_1, f_2\\) : # of vertical and horizontal indices of filter</li> </ul> <ul> <li>Input tensor : \\(X \\in \\mathbb{R}^{B \\times C_{\\text {in }} \\times m \\times n}\\)</li> <li>Output tensor : \\(Y \\in \\mathbb{R}^{B \\times C_{\\text {out }} \\times\\left(m-f_{1}+1\\right) \\times\\left(n-f_{2}+1\\right)}\\)</li> <li>Filter : \\(w \\in \\mathbb{R}^{C_{\\text {out }} \\times C_{\\text {in }} \\times f_{1} \\times f_{2}}\\)</li> <li>Bias : \\(b \\in \\mathbb{R}^{C_{\\text{out}}}\\)</li> </ul> <p>For \\(k = 1, \\dots, B, \\quad \\ell = 1, \\dots, C_{\\text{out}}, \\quad i = 1, \\dots, m-f_1+1, \\quad j = 1, \\dots, n-f_2+1\\):</p> \\[ Y_{k, \\ell, i, j}=\\sum_{\\gamma=1}^{c_{\\text {in }}} \\sum_{\\alpha=1}^{f_{1}} \\sum_{\\beta=1}^{f_{2}} w_{\\ell, \\gamma, \\alpha, \\beta} X_{k, \\gamma, i+\\alpha-1, j+\\beta-1}+b_{\\ell} \\] <p>Operation is independent across elements of the batch. The vertical and horizontal indices are referred to as spatial dimensions. If <code>bias=False</code>, then \\(b=0\\).</p> <p>Convolve a filter with an image : slide the filter spatially over the image and compute dot products.</p> <p>Take a \\(C_{\\text {in }} \\times f_{1} \\times f_{2}\\) chunk of the image and take the inner product with \\(w\\) and add bias \\(b\\).</p> <p>Example 5.4 : Example of 2D Convolutional Layer</p> <p> </p> <ul> <li>\\(B = 1, C_{\\text{in}} = 3, C_{\\text{out}} = 4, m=n=32, f_1=f_2=5\\)</li> <li>Input tensor : \\(X \\in \\mathbb{R}^{1 \\times 3 \\times 32 \\times 32}\\)</li> <li>Output tensor : \\(Y \\in \\mathbb{R}^{1 \\times 4 \\times 28 \\times 28}\\)</li> <li>Filter : \\(w \\in \\mathbb{R}^{4 \\times 3 \\times 5 \\times 5}\\)</li> <li>Bias : \\(b \\in \\mathbb{R}^{4}\\)</li> </ul> <p>Concept 5.5 : Zero Padding</p> <ul> <li>Problem</li> </ul> <p>Spatial dimension is reduced when passed through convolutional layers.</p> <p> </p> <p>\\((C \\times 7 \\times 7\\) image\\() \\circledast(C \\times 5 \\times 5\\) filter\\()=(1 \\times 3 \\times 3\\) feature map\\()\\). Spatial dimension 7 reduced to 3.</p> <ul> <li>Solution</li> </ul> <p>Zero padding on boundaries can preserve spatial dimension through convolutional layers.</p> <p> </p> <p>\\((C \\times 7 \\times 7\\) image with zero padding \\(=2) \\circledast(C \\times 5 \\times 5\\) filter\\()=(1 \\times 7 \\times 7\\) feature map\\()\\). Spatial dimension is preserved.</p> <p>Concept 5.6 : Stride</p> <p>The horizontal/vertical distance of two adjacent inner product calculations with the filter when sliding it across the image is called stride. It is originally set to 1, but can be adjusted.</p> <ul> <li>\\((7 \\times 7\\) image\\() \\circledast(3 \\times 3\\) filter\\()\\) with stride \\(1=(\\)output \\(5 \\times 5)\\)</li> <li>\\((7 \\times 7\\) image\\() \\circledast(3 \\times 3\\) filter\\()\\) with stride \\(2=(\\)output \\(3 \\times 3)\\) </li> <li>\\((7 \\times 7\\) image with zero padding \\(=1) \\circledast(3 \\times 3\\) filter\\()\\) with stride \\(3=(\\)output \\(3 \\times 3)\\)</li> </ul> <p>Concept 5.7 : Convolution Summary</p> <ul> <li>Input tensor : \\(C_{\\text {in}} \\times W_{\\text {in}} \\times H_{\\text {in}}\\)</li> <li>Convolution Layer parameters<ul> <li>\\(C_{\\text {out}}\\) filters, each of \\(C_{\\text {in}} \\times F \\times F\\)</li> <li>Stride : \\(S\\)</li> <li>Padding : \\(P\\)</li> </ul> </li> <li> <p>Output tensor : \\(C_{\\text {out}} \\times W_{\\text {out}} \\times H_{\\text {out}}\\)</p> \\[ \\begin{aligned} &amp; W_{\\text{out}}=\\left\\lfloor\\frac{W_{\\text{in}}-F+2 P}{S}+1\\right\\rfloor \\\\ &amp; H_{\\text{out}}=\\left\\lfloor\\frac{H_{\\text{in}}-F+2 P}{S}+1\\right\\rfloor \\end{aligned} \\] <p>To avoid the complication of this floor operation, it is best to ensure the formula inside evaluates to an integer.</p> </li> <li> <p>Number of trainable parameters : \\(F^2 C_{\\text{in}} C_{\\text{out}}\\) (filters) \\(+ C_{\\text{out}}\\) (biases)</p> </li> </ul> <p>Concept 5.8 : Pooling</p> <p>Pooling is primarily used to reduce spatial dimension. Similar to convolution. Pooling operates over each channel independently.</p> <p> </p> <p>Concept 5.9 : Weight Sharing</p> <p>In neural networks, weight sharing is a way to reduce the number of parameters by reusing the same parameter in multiple operations. Convolutional layers are the primary example.</p> \\[ A_{w}=\\left[\\begin{array}{cccccccc} w_{1} &amp; \\cdots &amp; w_{r} &amp; 0 &amp; \\cdots &amp; &amp; &amp; 0 \\\\ 0 &amp; w_{1} &amp; \\cdots &amp; w_{r} &amp; 0 &amp; \\cdots &amp; &amp; 0 \\\\ 0 &amp; 0 &amp; w_{1} &amp; \\cdots &amp; w_{r} &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; &amp; &amp; \\ddots &amp; &amp; \\ddots &amp; &amp; \\vdots \\\\ 0 &amp; &amp; \\cdots &amp; 0 &amp; w_{1} &amp; \\cdots &amp; w_{r} &amp; 0 \\\\ 0 &amp; &amp; \\cdots &amp; 0 &amp; 0 &amp; w_{1} &amp; \\cdots &amp; w_{r} \\end{array}\\right] \\] <p>If we consider convolution with filter \\(w\\) as a linear operator, the components of \\(w\\) appear may times in the matrix representation. This is because the same \\(w\\) is reused for every patch in the convolution. Weight sharing in convolution may now seem obvious, but it was a key contribution back when the LeNet architecture was presented.</p> <p>Some models (not studied in this course) use weight sharing more explicitly in other ways.</p> <p>Concept 5.10 : Geometric Deep Learning</p> <p>More generally, given a group \\(\\mathcal{G}\\) encoding a symmetry or invariance, one can define operations \"equivariant\" with respect \\(\\mathcal{G}\\) and construct equivariant neural networks.</p> <p>This is the subject of geometric deep learning, and its formulation utilizes graph theory and group theory.</p> <p>Geometric deep learning is particularly useful for non-Euclidean data. Examples include as protein molecule data and social network service connections.</p>"},{"location":"books-and-courses/mfdnn/6/","title":"\u00a7 6. Foundations of Design and Training of Deep Neural Networks","text":""},{"location":"books-and-courses/mfdnn/6/#data-augmentation","title":"Data Augmentation","text":"<p>Definition 6.1 : Spurious Correlation</p> <p>Hypothetical: A photographer prefers to take pictures with cats looking to the left and dogs looking to the right. Neural network learns to distinguish cats from dogs by which direction it is facing. This learned correlation will not be useful for pictures taken by another photographer.</p> <p>This is a spurious correlation, a correlation between the data and labels that does not capture the \"true\" meaning. Spurious correlations are not robust in the sense that the spurious correlation will not be a useful predictor when the data changes slightly.</p> <p>Definition 6.2 : Data Augmentation (DA)</p> <p> </p> <p>Translation invariance are encoded in convolution, but other invariances are harder to encode (unless one uses geometric deep learning). Therefore encode invariances in data and have neural networks learn the invariance.</p> <p>Data augmentation (DA) applies transforms to the data while preserving meaning and label.</p> <ul> <li> <p>Option 1: Enlarge dataset itself.     Usually cumbersome and unnecessary.</p> </li> <li> <p>Option 2: Use randomly transformed data in training loop.     In PyTorch, we use <code>Torchvision.transforms</code>.</p> </li> </ul> <p>We use DA to :</p> <ul> <li>Inject our prior knowledge of the structure of the data and force the neural network to learn it.</li> <li>Remove spurious correlations.</li> <li>Increase the effective data size. In particular, we ensure neural network never encounters the exact same data again and thereby prevent the neural network from performing exact memorization. (Neural network can memorize quite well.)</li> </ul> <p>Effects of DA :</p> <ul> <li>DA usually worsens the training error (but we don't care about training error).</li> <li>DA often, but not always, improves the test error.     If DA removes a spurious correlation, then the test error can be worsened.</li> <li>DA usually improves robustness.</li> </ul>"},{"location":"books-and-courses/mfdnn/6/#overfitting-underfitting","title":"Overfitting &amp; Underfitting","text":"<p>Definition 6.3 : Classical Statistics - Overfitting vs Underfitting</p> <p> </p> <p>Given separate train and test data</p> <ul> <li>When (training loss) &lt;&lt; (testing loss) you are overfitting. What you have learned from the training data does not carry over to test data.</li> <li>When (training loss) \\(\\approx\\) (testing loss) you are underfitting. You have the potential to learn more from the training data.</li> </ul> <p>The goal of ML is to learn patterns that generalize to data you have not seen. From each datapoint, you want to learn enough (don't underfit) but if you learn too much you overcompensate for an observation specific to the single experience.</p> <p>In classical statistics, underfitting vs. overfitting (bias vs. variance tradeoff) is characterized rigorously.</p> <p>Definition 6.4 : Modern Deep Learning - Double Descent</p> <p>In modern deep learning, you can overfit, but the state-of-the art neural networks do not overfit (or \"benignly overfit\") despite having more model parameters than training data.</p> <p>We do not yet have clarity with this new phenomenon called double descent. When overfitting happens and when it does not is unclear.</p> <p> </p> <p>Example 6.5 : Double Descent on 2-Layer Neural Network on MNIST</p> <p>Belkin et al. experimentally demonstrates the double descent phenomenon with an MLP trained on the MNIST dataset.</p> <p> </p> <p>(M. Belkin, D. Hsu, S. Ma, and S. Mandal, Reconciling modern machine-learning practice and the classical bias-variance trade-off, PNAS, 2019.)</p> <p>Concept 6.6 : How to Avoid Overfitting</p> <p>Regularization is loosely defined as mechanisms to prevent overfitting.</p> <p>When you are overfitting, regularize with:</p> <ul> <li>Smaller NN (fewer parameters) or larger NN (more parameters).</li> <li>Improve data by:<ul> <li>using data augmentation</li> <li>acquiring better, more diverse, data</li> <li>acquiring more of the same data</li> </ul> </li> <li>Weight decay</li> <li>Dropout</li> <li>Early stopping on SGD or late stopping on SGD</li> </ul> <p>Concept 6.7 : How to Avoid Underfitting</p> <p>When you are underfitting, use:</p> <ul> <li>Larger NN (if computationally feasible)</li> <li>Less weight decay</li> <li>Less dropout</li> <li>Run SGD longer (if computationally feasible)</li> </ul> <p>Concept 6.8 : Summary of Overfitting vs Underfitting</p> <p>In modern deep learning, the double descent phenomenon has brought a conceptual and theoretical crisis regarding over and underfitting. Much of the machine learning practice is informed by classical statistics and learning theory, which do not take the double descent phenomenon into account.</p> <p>Double descent will bring fundamental changes to statistics, and researchers need more time to figure things out. Most researchers, practitioners and theoreticians, agree that not all classical wisdom is invalid, but what part do we keep, and what part do we replace?</p> <p>In the meantime, we will have to keep in mind the two contradictory viewpoints and move forward in the absence of clarity.</p>"},{"location":"books-and-courses/mfdnn/6/#weight-decay","title":"Weight Decay","text":"<p>Definition 6.9 : \\(\\ell^{2}\\) - Regularization</p> <p>\\(\\ell^{2}\\)-regularization augments the loss function with</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(f_{\\theta}\\left(x_{i}\\right), y_{i}\\right)+\\frac{\\lambda}{2}\\|\\theta\\|^{2} \\] <p>SGD on the augmented loss is usually implemented by changing SGD update rather than explicitly changing the loss since</p> \\[ \\begin{gathered} \\theta^{k+1}=\\theta^{k}-\\alpha\\left(g^{k}+\\lambda \\theta^{k}\\right) \\\\ =(1-\\alpha \\lambda) \\theta^{k}-\\alpha g^{k} \\end{gathered} \\] <p>Where \\(g^{k}\\) is stochastic gradient of original (unaugmented) loss.</p> <p>In classical statistics, this is called ridge regression or maximum a posteriori (MAP) estimation with Gaussian prior.</p> <p>Concept 6.10 : Weight Decay \\(\\cong \\ell^{2}\\) - Regularization</p> <p>In Pytorch, you can use SGD + weight decay by:</p> <p>augmenting the loss function <pre><code>for param in model.parameters():\n    loss += (lamda/2)*param.pow(2.0).sum()\ntorch.optim.SGD(model.parameters(), lr=... , weight_decay=0)\n</code></pre> or by using <code>weight_decay</code> in the optimizer <pre><code>torch.optim.SGD(model.parameters(), lr=... , weight_decay=lamda)\n</code></pre></p> <p>For plain SGD, weight decay and \\(\\ell^{2}\\)-regularization are equivalent. For other optimizers, the two are similar but not the same. More on this later.</p>"},{"location":"books-and-courses/mfdnn/6/#dropout","title":"Dropout","text":"<p>Definition 6.11 : Dropout</p> <p>Dropout is a regularization technique that randomly disables neurons.</p> <p>Standard layer,</p> \\[ h_{2}=\\sigma\\left(W_{1} h_{1}+b_{1}\\right) \\] <p>Dropout with drop probability \\(p\\) defines</p> \\[ h_{2}=\\sigma\\left(W_{1} h_{1}^{\\prime}+b_{1}\\right) \\] <p>with \\(h_{1}^{\\prime}\\) defined as</p> \\[ \\left(h_{1}^{\\prime}\\right)_{j}= \\begin{cases}0 &amp; \\text { with probability } p \\\\ \\frac{\\left(h_{1}\\right)_{j}}{1-p} &amp; \\text { otherwise }\\end{cases} \\] <p>Note that \\(h_{1}^{\\prime}\\) is defined so that \\(\\mathbb{E}[h_{1}^{\\prime}]=h_1\\).</p> <p> </p> <p>During training, dropout masks are different in every forward pass due to their random nature.</p> <p>(N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov, Dropout: A Simple Way to Prevent Neural Networks from Overfitting, JMLR, 2014.)</p> <p>Concept 6.12 : Why is dropout helpful?</p> <p>\"A motivation for dropout comes from a theory of the role of sex in evolution (Livnat et al., 2010).\"</p> <p>Sexual reproduction, compared to asexual reproduction, creates the criterion for natural selection mix-ability of genes rather than individual fitness, since genes are mixed in a more haphazard manner.</p> <p>\"Since a gene cannot rely on a large set of partners to be present at all times, it must learn to do something useful on its own or in collaboration with a small number of other genes. ... Similarly, each hidden unit in a neural network trained with dropout must learn to work with a randomly chosen sample of other units. This should make each hidden unit more robust and drive it towards creating useful features on its own without relying on other hidden units to correct its mistakes.</p> <p>The analogy to evolution is very interesting, but it is ultimately a heuristic argument. It also shifts the burden to the question: \"why is sexual evolution more powerful than asexual evolution?\"</p> <p>However, dropout can be shown to be loosely equivalent to \\(\\ell^{2}\\)-regularization. However, we do not yet have a complete understanding of the mathematical reason behind dropout's performance.</p> <p>Concept 6.13 : Dropout in Pytorch</p> <p>Dropout simply multiplies the neurons with a random \\(0-\\frac{1}{1-p_{\\text {drop }}}\\) mask.</p> <p>A direct implementation in PyTorch: <pre><code>def dropout_layer(X, p_drop):\n    mask = (torch.rand(X.shape) &gt; p_drop).float()\n    return mask * X / (1.0 - p_drop)\n</code></pre></p> <p>PyTorch provides an implementation of dropout through <code>torch.nn.Dropout</code>.</p> <p>Concept 6.14 : Dropout in Training vs Test</p> <p>Typically, dropout is used during training and turned off during prediction/testing. (Dropout should be viewed as an additional onus imposed during training to make training more difficult and thereby effective, but it is something that should be turned off later.)</p> <p>In PyTorch, activate the training mode with <pre><code>model.train()\n</code></pre> and activate evaluation mode with <pre><code>model.eval()\n</code></pre> dropout (and batchnorm) will behave differently in these two modes.</p> <p>Concept 6.15 : When to Use Dropout</p> <p>Dropout is usually used on linear layers but not on convolutional layers.</p> <ul> <li>Linear layers have many weights and each weight is used only once per forward pass. (If \\(y=\\operatorname{Linear}_{A, b}(x)\\), then \\(A_{i j}\\) only affect \\(y_{i}\\).) So regularization seems more necessary.</li> <li>A convolutional filter has fewer weights and each weight is used multiple times in each forward pass. (If \\(y=\\operatorname{Conv} 2 \\mathrm{D}_{w, b}(x)\\), then \\(w_{i j k t}\\) affects \\(\\left.y_{i, .,:}.\\right)\\) So regularization seems less necessary.</li> </ul> <p>Dropout seems to be going out of fashion:</p> <ul> <li>Dropout's effect is somehow subsumed by batchnorm. (This is poorly understood.)</li> <li>Linear layers are less common due to their large number of trainable parameters.</li> </ul> <p>There is no consensus on whether dropout should be applied before or after the activation function. However, Dropout- \\(\\sigma\\) and \\(\\sigma\\)-Dropout are equivalent when \\(\\sigma\\) is \\(\\operatorname{ReLU}\\) or leaky \\(\\operatorname{ReLU}\\), or, more generally, when \\(\\sigma\\) is nonnegative homogeneous.</p>"},{"location":"books-and-courses/mfdnn/6/#sgd-early-late-stopping","title":"SGD Early / Late Stopping","text":"<p>Definition 6.16 : SGD Early Stopping</p> <p>Early stopping of SGD refers to stopping the training early even if you have time for more iterations.</p> <p>The rationale is that SGD fits data, so too many iterations lead to overfitting.</p> <p>A similar phenomenon (too many iterations hurt) is observed in classical algorithms for inverse problems.</p> <p> </p> <p>Definition 6.17 : Epochwise Double Descent</p> <p>Recently, however, an epochwise double descent has been observed.</p> <p>So perhaps one should stop SGD early or very late.</p> <p>We do not yet have clarity with this new phenomenon.</p> <p> </p> <p>(P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak, and I. Sutskever, Deep double descent: Where bigger models and more data hurt, ICLR, 2020.)</p>"},{"location":"books-and-courses/mfdnn/6/#more-data","title":"More Data","text":"<p>Concept 6.18 : More Data (by Data Augmentation)</p> <p>With all else fixed, using more data usually leads to less overfitting.</p> <p>However, collecting more data is often expensive.</p> <p>Think of data augmentation (DA) as a mechanism to create more data for free. You can view DA as a form of regularization.</p>"},{"location":"books-and-courses/mfdnn/6/#sgd-optimizer","title":"SGD Optimizer","text":"<p>Definition 6.19 : SGD with Momentum</p> <p>SGD:</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha g^{k} \\] <p>SGD with momentum:</p> \\[ \\begin{gathered} v^{k+1}=g^{k}+\\beta v^{k} \\\\ \\theta^{k+1}=\\theta^{k}-\\alpha v^{k+1} \\end{gathered} \\] <p>\\(\\beta=0.9\\) is a common choice.</p> <p> </p> <p>When different coordinates (parameters) have very different scalings (i.e., when the problem is ill-conditioned, momentum can help find a good direction of progress.</p> <p>(I. Sutskever, J. Martens, G. Dahl, and G. Hinton, On the importance of initialization and momentum in deep learning, ICML, 2013.)</p> <p>Definition 6.20 : RMSProp</p> <p>RMSProp:</p> \\[ \\begin{gathered} m_{2}^{k+1}=\\beta_{2} m_{2}^{k}+\\left(1-\\beta_{2}\\right)\\left(g^{k} \\circledast g^{k}\\right) \\\\ \\theta^{k+1}=\\theta^{k}-\\alpha g^{k} \\oslash \\sqrt{m_{2}^{k+1}+\\epsilon} \\end{gathered} \\] <p>\\(\\beta_{2}=0.99\\) and \\(\\epsilon=10^{-8}\\) are common values. \\(\\circledast\\) and \\(\\oslash\\) are elementwise mult. and div.</p> <p>\\(m_{2}^{k}\\) is a running estimate of the \\(2^{\\text {nd }}\\) moment of the stochastic gradients, i.e., \\(\\left(m_{2}^{k}\\right)_{i} \\approx \\mathbb{E}\\left(g^{k}\\right)_{i}^{2}\\).</p> <p>\\(\\alpha \\oslash \\sqrt{m_{2}^{k+1}+\\epsilon}\\) is the learning rate scaled elementwise. Progress along steep and noisy directions are dampened while progress along flat and non-noisy directions are accelerated.</p> <p>(T. Tieleman, and G. Hinton, Lecture 6.5 - RMSProp, COURSERA: Neural Networks for Machine Learning, 2012.)</p> <p>Definition 6.21 : Adam (Adaptive Moment Estimation)</p> <p>Adam:</p> \\[ \\begin{gathered} m_{1}^{k+1}=\\beta_{1} m_{1}^{k}+\\left(1-\\beta_{1}\\right) g^{k}, m_{2}^{k+1}=\\beta_{2} m_{2}^{k}+\\left(1-\\beta_{2}\\right)\\left(g^{k} \\circledast g^{k}\\right) \\\\ \\tilde{m}_{1}^{k+1}=\\frac{m_{1}^{k+1}}{1-\\beta_{1}^{k+1}}, \\quad \\widetilde{m}_{2}^{k+1}=\\frac{m_{2}^{k+1}}{1-\\beta_{2}^{k+1}} \\\\ \\theta^{k+1}=\\theta^{k}-\\alpha \\widetilde{m}_{1}^{k+1} \\oslash \\sqrt{\\widetilde{m}_{2}^{k+1}+\\epsilon} \\end{gathered} \\] <ul> <li>\\(\\beta_{1}^{k+1}\\) means \\(\\beta_{1}\\) to the \\((k+1)\\) th power.</li> <li>\\(\\beta_{1}=0.9, \\beta_{2}=0.999\\), and \\(\\epsilon=10^{-8}\\) are common values. Initialize with \\(m_{1}^{0}=m_{2}^{0}=0\\).</li> <li>\\(m_{1}^{k}\\) and \\(m_{2}^{k}\\) are running estimates of the \\(1^{\\text {st }}\\) and \\(2^{\\text {nd }}\\) moments of \\(g^{k}\\).</li> <li>\\(\\tilde{m}_{1}^{k}\\) and \\(\\tilde{m}_{2}^{k}\\) are bias-corrected estimates of \\(m_{1}^{k}\\) and \\(m_{2}^{k}\\).</li> <li>Using \\(\\widetilde{m}_{1}^{k}\\) instead of \\(g^{k}\\) adds the effect of momentum.</li> </ul> <p>(D. P. Kingma and J. Ba, Adam: A method for stochastic optimization, ICLR, 2015.)</p> <p>Concept 6.22 : Bias correction of Adam</p> <p>To understand the bias correction, consider the hypothetical \\(g^{k}=g\\) for \\(k=0,1, \\ldots\\). Then</p> \\[ \\begin{gathered} m_{1}^{k}=\\left(1-\\beta_{1}^{k}\\right) g \\\\ m_{2}^{k}=\\left(1-\\beta_{2}^{k}\\right)(g \\circledast g) \\end{gathered} \\] <p>Even though \\(m_{1}^{k} \\rightarrow g\\) and \\(m_{2}^{k} \\rightarrow(g \\circledast g)\\) as \\(k \\rightarrow \\infty\\), the estimators are not exact despite there being no variation in \\(g^{k}\\).</p> <p>On the other hand, the bias-corrected estimators are exact:</p> \\[ \\begin{gathered} \\widetilde{m}_{1}^{k}=g \\\\ \\widetilde{m}_{2}^{k}=(g \\circledast g) \\end{gathered} \\] <p>Concept 6.23 : The Cautionary Tale of Adam</p> <p>Adam's original 2015 paper justified the effectiveness of the algorithm through experiments training deep neural networks with Adam. After all, this non-convex optimization is what Adam was proposed to do.</p> <p>However, the paper also provided a convergence proof under the assumption of convexity. This was perhaps unnecessary in an applied paper focusing on non-convex optimization.</p> <p>The proof was later shown to be incorrect! Adam does not always converge in the convex setup, i.e., the algorithm, rather than the proof, is wrong.</p> <p>Reddi and Kale presented the AMSGrad optimizer, which does come with a correct convergence proof, but AMSGrad tends to perform worse than Adam, empirically.</p> <p>(S. J. Reddi, S. Kale, and S. Kumar, On the convergence of Adam and beyond, ICLR, 2018.)</p> <p>Concept 6.24 : How to Choose Optimizer</p> <p>Extensive research has gone into finding the \"best\" optimizer. Schmidt et al.\\({ }^{\\star}\\) reports that, roughly speaking, that Adam works well most of the time.</p> <p>So, Adam is a good default choice. Currently, it seems to be the best default choice.</p> <p>However, Adam does not always work. For example, it seems to be that the widely used EfficientNet model can only be trained \\({ }^{\\dagger}\\) with RMSProp.</p> <p>However, there are some setups where the LR of SGD is harder to tune, but SGD outperforms Adam when properly tuned.\\({ }^{\\#}\\)</p> <p>(\\({ }^{\\star}\\) R. M. Schmidt, F. Schneider, and P. Hennig, Descending through a crowded valley \u2014 benchmarking deep learning optimizers, ICML, 2021. \\({ }^{\\dagger}\\) M. Tan and Q. V. Le, EfficientNet: Rethinking model scaling for convolutional neural networks, ICML, 2019. \\({ }^{\\#}\\) A. C. Wilson, R. Roelofs, M. Stern, N. Srebro, and B. Recht, The marginal value of adaptive gradient methods in machine learning, NeurlPS, 2017.)</p> <p>Concept 6.25 : How to Tune Parameters</p> <p>Everything should be chosen by trial and error. The weight parameters and \\(\\beta, \\beta_{1}, \\beta_{2}\\) and the weight decay parameter \\(\\lambda\\), and the optimizers should be chosen based on trial and error.</p> <p>The LR (the stepsize \\(\\alpha\\) ) of different optimizers are not really comparable between the different optimizers. When you change the optimizer, the LR should be tuned again.</p> <p>Roughly, large stepsize, large momentum, small weight decay is faster but less stable, while small stepsize, small momentum, and large weight decay is slower but more stable.</p> <p>Concept 6.26 : Using Different Optimizers in Pytorch</p> <p>In PyTorch, the <code>torch.optim</code> module implements the commonly used optimizers.</p> <ul> <li> <p>Using SGD: <pre><code>torch.optim.SGD(model.parameters(), lr=X)\n</code></pre></p> </li> <li> <p>Using SGD with momentum: <pre><code>torch.optim.SGD(model.parameters(), momentum=0.9, lr=X)\n</code></pre></p> </li> <li> <p>Using RMSprop: <pre><code>torch.optim.RMSprop(model.parameters(), lr=X)\n</code></pre></p> </li> <li> <p>Using Adam: <pre><code>torch.optim.Adam(model.parameters(), lr=X)\n</code></pre></p> </li> </ul> <p>Concept 6.27 : Learning Rate Scheduling</p> <p>Sometimes, it is helpful to change (usually reduce) the learning rate as the training progresses. PyTorch provides learning rate schedulers to do this.</p> <pre><code>optimizer = SGD(model.parameters(), lr=0.1)\nscheduler = ExponentialLR(optimizer, gamma=0.9) # lr = 0.9*lr\nfor _ in range(...):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler.step() # .step() call updates (changes) the learning rate\n</code></pre> <p>One common choice is to specify a diminishing learning rate via a function (a lambda expression). Choices like <code>C/epoch</code> or <code>C / sqrt(iteration)</code>, where <code>C</code> is an appropriately chosen constant, are common. <pre><code># lr_lambda allows us to set lr with a function\nscheduler = LambdaLR(optimizer, lr_lambda = lambda ep: 1e-2/ep)\nfor epoch in range(...):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler.step() # lr=0.01/epoch\n</code></pre></p> <p>Concept 6.28 : Cosine Learning Rate</p> <p>The cosine learning rate scheduler, which sets the learning rate with the cosine function, is also commonly used.</p> <p>The \\(2^{\\text {nd }}\\) case in the specification means \\(k\\) and its purpose is to prevent the learning rate from becoming 0 .</p> <p>It is also common to use only a half-period of the cosine rather than having the learning rate oscillate.</p> <p> </p> <p>(I. Loshchilov and F. Hutter, SGDR: Stochastic gradient descent with warm restarts, ICLR, 2017)</p> <p>Concept 6.29 : Wide vs Sharp Minima</p> <ul> <li>Large step makes large and rough progress towards regions with small loss.</li> <li>Small steps refines the model by finding sharper minima.</li> </ul> <p>Also small steps better suppress the effect of noise. Mathematically, one can show that SGD with small steps becomes very similar to GD with small steps.\\({ }^{\\#}\\)</p> <p>However, using small steps to converge to sharp minima may not always be optimal. There is some empirical evidence that wide minima have better test error than sharp minima.\\({ }^{\\star}\\)</p> <p>(\\({ }^{\\#}\\) D. Davis, D. Drusvyatskiy, S. Kakade and J. D. Lee, Stochastic subgradient method converges on tame functions, Found. Comput. Math., 2020. \\({ }^{\\star}\\) Y. Jiang, B. Neyshabur, H. Mobahi, D. Krishnan, and S. Bengio, Fantastic generalization measures and where to find them, ICLR, 2020.)</p>"},{"location":"books-and-courses/mfdnn/6/#weight-initialization","title":"Weight Initialization","text":"<p>Concept 6.30 : Importance of Weight Initialization</p> <p>Remember, SGD is</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha g^{k} \\] <p>where \\(\\theta^{0} \\in \\mathbb{R}^{p}\\) is an initial point. Using a good initial point is important in NN training.</p> <p>Prescription by LeCun et al.: \"Weights should be chosen randomly but in such a way that the [tanh] is primarily activated in its linear region. If weights are all very large then the [tanh] will saturate resulting in small gradients that make learning slow. If weights are very small then gradients will also be very small.\" (Cf. Vanishing gradient)</p> <p>\"Intermediate weights that range over the [tanh's] linear region have the advantage that (1) the gradients are large enough that learning can proceed and (2) the network will learn the linear part of the mapping before the more difficult nonlinear part.\"</p> <p>(Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. M\u00fcller. Efficient BackProp, In: G. Montavon, G. B. Orr, and K.-R. M\u00fcller. (eds), Neural Networks: Tricks of the Trade, 1998.)</p> <p>Concept 6.31 : Mathematics Review</p> <ul> <li> <p>Using the \\(1^{\\text {st }}\\) order Taylor approximation,</p> \\[ \\tanh (z) \\approx z \\] </li> <li> <p>Write \\(X \\sim \\mathcal{N}\\left(\\mu, \\sigma^{2}\\right)\\) to denote that \\(X\\) is a Gaussian (normal) random variable with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).</p> </li> <li> <p>If \\(X\\) and \\(Y\\) are random variables, with expected values \\(\\mu_X, \\mu_Y\\) and standard deviations \\(\\sigma_X, \\sigma_Y\\), the following properties hold.</p> \\[ \\text{Cov}(X, Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] = \\mathbb{E}[(X-\\mu_X)(Y-\\mu_Y)] \\] \\[ \\text{Corr}(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y} \\] \\[ \\mathbb{E}[X+Y] = \\mathbb{E}[X] + \\mathbb{E}[Y] \\] \\[ \\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\text{Cov}(X, Y) \\] \\[ \\mathbb{E}[aX+b] = a\\mathbb{E}[X] + b \\] \\[ \\text{Var}[aX+b] = a^2 \\text{Var}[X] \\] </li> <li> <p>If \\(X\\) and \\(Y\\) are random variables, such that</p> \\[ \\text{Cov}(X, Y)=\\text{Corr}(X, Y)=0 \\] <p>\\(X\\) and \\(Y\\) are uncorrelated random variables, and following properties hold.</p> \\[ \\mathbb{E}[XY] = \\mathbb{E}[X] \\mathbb{E}[Y] \\] \\[ \\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y) \\] </li> <li> <p>If \\(X\\) and \\(Y\\) are random variables, with probability density function \\(f_X(x), f_Y(y)\\) and joint probability density funciton \\(f_{X, Y}(x, y)\\), such that</p> \\[ f_{X, Y}(x, y) = f_X(x)f_Y(y) \\] <p>\\(X\\) and \\(Y\\) are independent random variables, and following properties hold.</p> \\[ \\mathbb{E}[X^n Y^m] = \\mathbb{E}[X^n] \\mathbb{E}[Y^m] \\] \\[ \\text{Cov}(X, Y) = \\text{Corr}(X, Y) = 0 \\] \\[ \\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y) \\] \\[ \\text{Var}(XY) = \\text{Var}(X)\\text{Var}(Y) + \\text{Var}(X)\\mathbb{E}[Y]^2 + \\text{Var}(Y)\\mathbb{E}[X]^2 \\] \\[ \\text{Var}(XY) = \\text{Var}(X)\\text{Var}(Y) \\quad (\\text{if} \\ \\mathbb{E}[X]=\\mathbb{E}[Y]=0) \\] </li> <li> <p>If \\(X\\) and \\(Y\\) are independent, then \\(X\\) and \\(Y\\) are uncorrelated. The converse does not hold.</p> </li> <li>IID means, \"independent and identically distributed\" random variables.</li> </ul> <p>Definition 6.32 : LeCun Initialization</p> <p>Consider the layer</p> \\[ \\begin{gathered} \\tilde{y}=A x+b \\\\ y=\\tanh (\\tilde{y}) \\end{gathered} \\] <p>where \\(x \\in \\mathbb{R}^{n_{\\text {in }}}\\) and \\(y, \\tilde{y} \\in \\mathbb{R}^{n_{\\text {out }}}\\). Assume \\(x_{j}\\) have mean \\(=0\\), variance \\(=1\\) and are uncorrelated. If we initialize \\(A_{i j} \\sim \\mathcal{N}\\left(0, \\sigma_{A}^{2}\\right)\\) and \\(b_{i} \\sim \\mathcal{N}\\left(0, \\sigma_{b}^{2}\\right)\\), IID, then</p> \\[ \\begin{aligned} &amp; \\tilde{y}_{i}=\\sum_{j=1}^{n_{\\mathrm{in}}} A_{i j} x_{j}+b_{i} \\quad \\text { has mean }=0 \\text {, variance }=n_{\\mathrm{in}} \\sigma_{A}^{2}+\\sigma_{b}^{2} \\\\ &amp; y_{i}=\\tanh \\left(\\tilde{y}_{i}\\right) \\approx \\tilde{y}_{i} \\quad \\text { has mean } \\approx 0 \\text {, variance } \\approx n_{\\mathrm{in}} \\sigma_{A}^{2}+\\sigma_{b}^{2} \\end{aligned} \\] <p>If we choose</p> \\[ \\sigma_{A}^{2}=\\frac{1}{n_{\\text {in }}}, \\quad \\sigma_{b}^{2}=0, \\] <p>(so \\(b=0\\) ) then we have \\(y_{i}\\) mean \\(\\approx 0\\) variance \\(\\approx 1\\) and are uncorrelated.</p> <p>By induction, with an \\(L\\)-layer MLP,</p> <ul> <li>if the input to has mean \\(=0\\) variance \\(=1\\) and uncorrelated elements,</li> <li>the weights and biases are initialized with \\(A_{i j} \\sim \\mathcal{N}\\left(0, \\frac{1}{n_{\\text {in }}}\\right)\\) and \\(b_{i}=0\\), and</li> <li>the linear approximations \\(\\tanh (z) \\approx z\\) are valid,</li> </ul> <p>then we can expect the output layer to have mean \\(\\approx 0\\), variance \\(\\approx 1\\).</p> <p>(Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. M\u00fcller. Efficient BackProp, In: G. Montavon, G. B. Orr, and K.-R. M\u00fcller. (eds), Neural Networks: Tricks of the Trade, 1998.)</p> <p>Definition 6.33 : Xavier Initialization</p> <p>Consider the layer</p> \\[ \\begin{gathered} \\tilde{y}=A x+b \\\\ y=\\tanh (\\tilde{y}) \\end{gathered} \\] <p>where \\(x \\in \\mathbb{R}^{n_{\\text {in }}}\\) and \\(y, \\tilde{y} \\in \\mathbb{R}^{n_{\\text {out }}}\\). Consider the gradient with respect to some loss \\(\\ell(y)\\). Assume \\(\\left(\\frac{\\partial \\ell}{\\partial y}\\right)_{i}\\) have mean \\(=0\\), variance \\(=1\\) and are uncorrelated. Then</p> \\[ \\frac{\\partial y}{\\partial x}=\\operatorname{diag}\\left(\\tanh ^{\\prime}(A x+b)\\right) A \\approx A \\] <p>if \\(\\tanh (\\tilde{y}) \\approx \\tilde{y}\\) and</p> \\[ \\frac{\\partial \\ell}{\\partial x}=\\frac{\\partial \\ell}{\\partial y} A \\] <p>If we initialize \\(A_{i j} \\sim \\mathcal{N}\\left(0, \\sigma_{A}^{2}\\right)\\) and \\(b_{i} \\sim \\mathcal{N}\\left(0, \\sigma_{b}^{2}\\right)\\), IID, and assume that \\(\\frac{\\partial \\ell}{\\partial y}\\) and \\(A\\) are independent, then</p> \\[ \\left(\\frac{\\partial \\ell}{\\partial x}\\right)_{j}=\\sum_{i=1}^{n_{\\text {out }}}\\left(\\frac{\\partial \\ell}{\\partial y}\\right)_{i} A_{i j} \\text { has mean } \\approx 0 \\text { and variance } \\approx n_{\\text {out }} \\sigma_{A}^{2} \\] <p>If we choose</p> \\[ \\sigma_{A}^{2}=\\frac{1}{n_{\\mathrm{out}}} \\] <p>then \\(\\left(\\frac{\\partial \\ell}{\\partial x}\\right)_{j}\\) have mean \\(\\approx 0\\), variance \\(\\approx 1\\) and are uncorrelated.</p> <p>\\(\\frac{\\partial \\ell}{\\partial y}\\) and \\(A\\) are not independent; \\(\\frac{\\partial \\ell}{\\partial y}\\) depends on the forward evaluation, which in turn depends on \\(A\\). Nevertheless, the calculation is an informative exercise and its result seems to be representative of common behavior.</p> <p>If \\(y=\\tanh (A x+b)\\) is an early layer (close to input) in a deep neural network, then the randomness of \\(A\\) is diluted through the forward and backward propagation and \\(\\frac{\\partial \\ell}{\\partial y}\\) and \\(A\\) will be nearly independent.</p> <p>If \\(y=\\tanh (A x+b)\\) is an later layer (close to output) in a deep neural network, then \\(\\frac{\\partial \\ell}{\\partial y}\\) and \\(A\\) will have strong dependency.</p> <p>Consideration of forward and backward passes result in different prescriptions.</p> <p>The Xavier initialization uses the harmonic mean of the two:</p> \\[ \\sigma_{A}^{2}=\\frac{2}{n_{\\mathrm{in}}+n_{\\mathrm{out}}}, \\quad \\sigma_{b}^{2}=0 \\] <p>In the literature, the alternate notation \\(\\text{fan}_{\\text {in }}\\) and \\(\\text{fan}_{\\text {out }}\\) are often used instead of \\(n_{\\text {in }}\\) and \\(n_{\\text {out }}\\). The fan-in and fan-out terminology originally refers to the number of electric connections entering and exiting a circuit or an electronic device.</p> <p>(Xavier Glorot and Y. Bengio, Understanding the difficulty of training deep feedforward neural networks, AISTATS, 2010.)</p> <p>Definition 6.34 : (Kaiming) He Initialization</p> <p>Consider the layer</p> \\[ y=\\operatorname{ReLU}(A x+b) \\] <p>We cannot use the Taylor expansion with ReLU.</p> <p>However, a similar line of reasoning with the forward pass gives rise to</p> \\[ \\sigma_{A}^{2}=\\frac{2}{n_{\\mathrm{in}}} \\] <p>And a similar consideration with backprop gives rise to</p> \\[ \\sigma_{A}^{2}=\\frac{2}{n_{\\text {out }}} \\] <p>In PyTorch, use <code>mode='fan_in'</code> and <code>mode='fan_out'</code> to toggle between the two modes.</p> <p>(Kaiming He, X. Zhang, S. Ren, and J. Sun, Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification, ICCV, 2015.)</p> <p>Concept 6.35 : Discussions on Initializations</p> <p>In the original description of the Xavier and He initializations, the biases are all initialized to 0 . However, the default initialization of Linear\\({ }^{\\star}\\) and Conv2d \\({ }^{\\#}\\) layers in PyTorch uses initialize the biases randomly. A documented reasoning behind this choice (in the form of papers or GitHub discussions) do not seem to exist.</p> <p>Initializing weights with the proper scaling is sometimes necessary to get the network to train, as you will see with the VGG network. However, so long as the network gets trained, the choice of initialization does not seem to affect the final performance.</p> <p>Since initializations rely on the assumption that the input to each layer has roughly unit variance, it is important that the data is scaled properly. This is why PyTorch dataloader scales pixel intensity values to be in \\([0,1]\\), rather than \\([0,255]\\).</p> <p>(\\({ }^{\\star}\\) https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html \\({ }^{\\#}\\) https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html)</p> <p>Definition 6.36 : Initialization for Convolutional Layer</p> <p>Consider the layer</p> \\[ \\begin{aligned} &amp; \\tilde{y}=\\operatorname{Conv} 2 \\mathrm{D}_{w, b}(x) \\\\ &amp; y=\\tanh (\\tilde{y}) \\\\ \\end{aligned} \\] <p>where \\(w \\in \\mathbb{R}^{C_{\\text {out }} \\times C_{\\text {in }} \\times f_{1} \\times f_{2}}\\) and \\(b \\in \\mathbb{R}^{C_{\\text {out }}}\\). Assume \\(x_{j}\\) have mean \\(=0\\) variance \\(=1\\) and are uncorrelated. If we initialize \\(w_{i j k \\ell} \\sim \\mathcal{N}\\left(0, \\sigma_{w}^{2}\\right)\\) and \\(b_{i} \\sim \\mathcal{N}\\left(0, \\sigma_{b}^{2}\\right)\\), IID, then</p> \\[ \\begin{aligned} &amp; \\tilde{y}_{i} \\quad \\text { has mean }=0 \\text { variance }=\\left(C_{\\text {in }} f_{1} f_{2}\\right) \\sigma_{w}^{2}+\\sigma_{b}^{2} \\\\ &amp; y_{i} \\approx \\tilde{y}_{i} \\text { has mean } \\approx 0 \\text { variance } \\approx\\left(C_{\\text {in }} f_{1} f_{2}\\right) \\sigma_{w}^{2}+\\sigma_{b}^{2} \\end{aligned} \\] <p>If we choose</p> \\[ \\sigma_{w}^{2}=\\frac{1}{c_{\\text {in }} f_{1} f_{2}}, \\quad \\sigma_{b}^{2}=0 \\] <p>(so \\(b=0\\) ) then we have \\(y_{i}\\) mean \\(\\approx 0\\) variance \\(\\approx 1\\) and are correlated.</p> <p>Outputs from a convolutional layer are correlated. The uncorrelated assumption is false. Nevertheless, the calculation is an informative exercise and its result seems to be representative of common behavior.</p> <p>Xavier and He initialization is usually used with</p> \\[ n_{\\mathrm{in}}=C_{\\mathrm{in}} f_{1} f_{2} \\] <p>and</p> \\[ n_{\\text {out }}=C_{\\text {out }} f_{1} f_{2} \\] <p>Justification of \\(n_{\\text {out }}=C_{\\text {out }} f_{1} f_{2}\\) requires working through the complex indexing or considering the \"transpose convolution\". We will return to it later.</p>"},{"location":"books-and-courses/mfdnn/6/#automatic-differentation","title":"Automatic Differentation","text":"<p>Definition 6.37 : Automatic Differentation</p> <p>Autodiff (automatic differentiation) is an algorithm that automates gradient computation. In deep learning libraries, you only need to specify how to evaluate the function.</p> <p>Backprop (back propagation) is an instance of autodiff. (backprop \\(\\subseteq\\) autodiff)</p> <p>Gradient computation costs roughly \\(5 \\times\\) the computation cost of forward evaluation.</p> <p>To clarify, backprop and autodiff are not</p> <ul> <li>finite difference (numerical differentation) or</li> <li>symbolic differentiation.</li> </ul> <p>Autodiff \\(\\approx\\) chain rule of vector calculus</p> <p>Autodiff is an essential yet often an underappreciated feature of the deep learning libraries. It allows deep learning researchers to use complicated neural networks, while avoiding the burden of performing derivative calculations by hand.</p> <p>Most deep learning libraries support \\(2^{\\text {nd }}\\) and higher order derivative computation, but we will only use \\(1^{\\text {st }}\\) order derivatives (gradients) in this class.</p> <p>Autodiff includes forward-mode, reverse-mode (backprop), and other orders. In deep learning, reverse-mode is most commonly used.</p> <p>Concept 6.38 : Autodiff by Jacobial Multiplication</p> <p>Consider \\(g=f_{L} \\circ f_{L-1} \\circ \\cdots \\circ f_{2} \\circ f_{1}\\), where \\(f_{\\ell}: \\mathbb{R}^{n_{\\ell-1}} \\rightarrow \\mathbb{R}^{n_{\\ell}}\\) for \\(\\ell=1, \\cdots, L\\).</p> <p>Chain rule: \\(D g=D f_{L} \\quad D f_{L-1} \\quad \\cdots \\quad D f_{2} \\quad D f_{1}\\)</p> <p>Forward-mode: \\(D f_{L}\\left(D f_{L-1}\\left(\\cdots\\left(D f_{2} D f_{1}\\right) \\cdots\\right)\\right)\\)</p> <p>Reverse-mode (back propagation): \\(\\left(\\left(\\left(D f_{L} D f_{L-1}\\right) D f_{L-2}\\right) \\cdots\\right) D f_{1}\\)</p> <p>Reverse mode is optimal (can be proved using DP) when \\(n_{L} \\leq n_{L-1} \\leq \\cdots \\leq n_{1} \\leq n_{0}\\). The number of neurons in each layer tends to decrease in deep neural networks for classification. So reverse-mode is often close to the most efficient mode of autodiff in deep learning.</p> <p>Definition 6.39 : General Backprop</p> <p>Backprop in PyTorch:</p> <ol> <li>When the loss function is evaluated, a computation graph is constructed.</li> <li>The computation graph is a directed acyclic graph (DAG) that encodes dependencies of the individual computational components.</li> <li>A topological sort is performed on the DAG and the backprop is performed on the reversed order of this topological sort. (The topological sort ensures that nodes ahead in the DAG are processed first.)</li> </ol> <p>The general form combines a graph theoretic formulation with the principles of backprop.</p> <p>Definition 6.40 : Computation Graph</p> <p>Let \\(y_{1}, \\ldots, y_{L}\\) be the output values (neurons) of the computational nodes. Assume \\(y_{1}, \\ldots, y_{L}\\) follow a linear topological ordering, i.e., the computation of \\(y_{\\ell}\\) depends on \\(y_{1}, \\ldots, y_{\\ell-1}\\) and does not depend on \\(y_{\\ell+1}, \\ldots, y_{L}\\).</p> <p>Define the graph \\(G=(V, E)\\), where \\(V=\\{1, \\ldots, L\\}\\) and \\((i, \\ell) \\in E\\), i.e., \\(i \\rightarrow \\ell\\), if the computation of \\(y_{\\ell}\\) directly depends on \\(y_{i}\\). Write the computation of \\(y_{1}, \\ldots, y_{L}\\) as</p> \\[ y_{\\ell}=f_{\\ell}\\left(\\left[y_{i}: \\text { for } i \\rightarrow \\ell\\right]\\right) \\] <p>Definition 6.41 : Forward Pass on Computation Graph</p> <p>In the forward pass, sequentially compute \\(y_{1}, \\ldots, y_{L}\\) via</p> \\[ y_{\\ell}=f_{\\ell}\\left(\\left[y_{i}: \\text { for } i \\rightarrow \\ell\\right]\\right) \\] <pre><code># Use 1-based indexing\n# y[1] given\nfor l = 2,...,L\n    inputs = [y[i] for j such that (i-&gt;l)]\n    y[l] = f[l].eval(inputs)\nend\n</code></pre> <p>Example 6.42 : Forward Pass &amp; Forward-mode Autodiff</p> <p>Consider \\(f(x, y)=y \\log x+\\sqrt{y \\log x}\\). Evaluate \\(f\\) with the computation graph:  </p> <ul> <li> <p>Step 0 :</p> \\[ \\begin{gathered} x=3, y=2 \\\\ \\frac{\\partial x}{\\partial x}=1, \\frac{\\partial x}{\\partial y}=0, \\frac{\\partial y}{\\partial x}=0, \\frac{\\partial y}{\\partial y}=1 \\end{gathered} \\] </li> <li> <p>Step 1 :</p> \\[ \\begin{gathered} a=\\log x=\\log 3 \\\\ \\frac{\\partial a}{\\partial x}=\\frac{1}{x} \\cdot \\frac{\\partial x}{\\partial x}=\\frac{1}{3}, \\frac{\\partial a}{\\partial y}=0 \\end{gathered} \\] </li> <li> <p>Step 2 :</p> \\[ \\begin{gathered} b=y a=2 \\log 3 \\\\ \\frac{\\partial b}{\\partial x}=\\frac{\\partial y}{\\partial x} a+y \\frac{\\partial a}{\\partial x}=\\frac{2}{3}, \\frac{\\partial b}{\\partial y}=\\frac{\\partial y}{\\partial y} a+y \\frac{\\partial a}{\\partial y}=a=\\log 3 \\end{gathered} \\] </li> <li> <p>Step 3 :</p> \\[ \\begin{gathered} c=\\sqrt{b}=\\sqrt{2 \\log 3} \\\\ \\frac{\\partial c}{\\partial x}=\\frac{1}{2 \\sqrt{b}} \\frac{\\partial b}{\\partial x}=\\frac{1}{3 \\sqrt{2 \\log 3}}, \\frac{\\partial c}{\\partial y}=\\frac{1}{\\sqrt{b}} \\frac{\\partial b}{\\partial y}=\\frac{1}{2} \\sqrt{\\frac{\\log 3}{2}} \\end{gathered} \\] </li> <li> <p>Step 4 :</p> \\[ \\begin{gathered} f=c+b=\\sqrt{2 \\log 3}+2 \\log 3 \\\\ \\frac{\\partial f}{\\partial x}=\\frac{\\partial c}{\\partial x}+\\frac{\\partial b}{\\partial x}=\\frac{1}{3}\\left(2+\\frac{1}{3 \\sqrt{2 \\log 3}}\\right), \\frac{\\partial f}{\\partial y}=\\frac{\\partial c}{\\partial y}+\\frac{\\partial b}{\\partial y}=\\frac{1}{2} \\sqrt{\\frac{\\log 3}{2}}+\\log 3 \\end{gathered} \\] </li> </ul> <p>Definition 6.42 : Backprop on Computation Graph</p> <p>To perform backprop, use</p> \\[ \\frac{\\partial y_{L}}{\\partial y_{i}}=\\sum_{\\ell: i \\rightarrow \\ell} \\frac{\\partial y_{L}}{\\partial y_{\\ell}} \\frac{\\partial f_{\\ell}}{\\partial y_{i}} \\] <p>to sequentially compute \\(\\frac{\\partial y_{L}}{\\partial y_{L}}, \\frac{\\partial y_{L}}{\\partial y_{L-1}}, \\ldots, \\frac{\\partial y_{L}}{\\partial y_{1}}\\).     </p> <pre><code># Use 1-based indexing\n# y[1],...,y[L] already computed\ng[:] = 0 // .zero_grad()\ng[L] = 1 // dy[L]/dy[L]=1\nfor l = L,...,2\n    for i such that (i-&gt;l)\n        g[i] += g[l]*f[l].grad(i)\n    end\nend\n</code></pre> <p>Example 6.43 : Reverse-mode Autodiff (Backprop)</p> <p>Consider \\(f(x, y)=y \\log x+\\sqrt{y \\log x}\\). Evaluate \\(f\\) with the computation graph:  </p> <ul> <li> <p>Step 0 :</p> \\[ x=3, y=2 \\] </li> <li> <p>Step 1 :</p> \\[ a=\\log 3 \\] </li> <li> <p>Step 2 :</p> \\[ b=2 \\log 3 \\] </li> <li> <p>Step 3 :</p> \\[ c=\\sqrt{2 \\log 3} \\] </li> <li> <p>Step 4 :</p> \\[ f=\\sqrt{2 \\log 3}+2 \\log 3 \\] </li> </ul> <ul> <li> <p>Step 0' :</p> \\[ \\frac{\\partial f}{\\partial f}=1 \\] </li> <li> <p>Step 1' :</p> \\[ \\frac{\\partial f}{\\partial c}=\\frac{\\partial f}{\\partial f} \\frac{\\partial f}{\\partial c}=\\frac{\\partial f}{\\partial f} 1=1 \\] </li> <li> <p>Step 2' :</p> \\[ \\frac{\\partial f}{\\partial b}=\\frac{\\partial f}{\\partial c} \\frac{\\partial c}{\\partial b}+\\frac{\\partial f}{\\partial f} \\frac{\\partial f}{\\partial c}=\\frac{1}{2 \\sqrt{b}} 1+1=\\frac{1}{2 \\sqrt{2 \\log 3}}+1 \\] </li> <li> <p>Step 3' :</p> \\[ \\frac{\\partial f}{\\partial a}=\\frac{\\partial f}{\\partial b} \\frac{\\partial b}{\\partial a}=\\frac{\\partial f}{\\partial b} y=2+\\frac{1}{\\sqrt{2 \\log 3}} \\] </li> <li> <p>Step 4' :</p> \\[ \\begin{gathered} \\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial a} \\frac{\\partial a}{\\partial x}=\\frac{\\partial f}{\\partial a} \\frac{1}{x}=\\frac{1}{3}\\left(2+\\frac{1}{\\sqrt{2 \\log 3}}\\right) \\\\ \\frac{\\partial f}{\\partial y}=\\frac{\\partial f}{\\partial b} \\frac{\\partial b}{\\partial y}=\\frac{\\partial f}{\\partial b} a=\\frac{1}{2} \\sqrt{\\frac{\\log 3}{2}}+\\log 3 \\end{gathered} \\] </li> </ul> <p>Concept 6.44 : Backprop in Pytorch</p> <p> </p> <p>In NN training, parameters (shown blue in the image) and fixed inputs are distinguished. In PyTorch, you (1) clear the existing gradient with <code>.zero_grad()</code> (2) forward-evaluate the loss function by providing the input and label and (3) perform backprop with <code>.backward()</code>.</p> <p>The forward pass stores the intermediate neuron values so that they can later be used in backprop. In the test loop, however, we don't compute gradients so the intermediate neuron values are unnecessary. The <code>torch.no_grad()</code> context manager allows intermediate node values to discarded or not be stored. This saves memory and can accelerate the test loop.</p>"},{"location":"books-and-courses/mfdnn/6/#batch-normalization","title":"Batch Normalization","text":"<p>Concept 6.45 : Idea of Batch Normalization</p> <p>The first step of many data processing algorithms is often to normalize data to have zero mean and unit variance.</p> <ul> <li>Step 1. Compute \\(\\hat{\\mu}=\\frac{1}{N} \\sum_{i=1}^{N} X_{i}, \\widehat{\\sigma^{2}}=\\frac{1}{N} \\sum_{i=1}^{N}\\left(X_{i}-\\hat{\\mu}\\right)^{2}\\)</li> </ul> \\[ \\hat{X}_{i}=\\frac{X_{i}-\\widehat{\\mu}}{\\sqrt{\\sigma^{2}}+\\varepsilon} \\] <ul> <li>Step 2. Run method with data \\(\\hat{X}_{1}, \\ldots, \\hat{X}_{N}\\)</li> </ul> <p>Batch normalization (BN) (sort of) enforces this normalization layer-by-layer. BN is an indispensable tool for training very deep neural networks. Theoretical justification is weak.</p> <p>(S. Ioffe and C. Szegedy, Batch normalization: Accelerating deep network training by reducing internal covariate shift, ICML, 2015.)</p> <p>Definition 6.46 : BN for Linear Layers</p> <p>Underlying assumption: Each element of the batch is an IID sample.</p> <p>Input: \\(X\\), \\(\\text{shape}(X) = \\text{(batch size)} \\times \\text{(# entries)}\\)</p> <p>Output: \\(\\mathrm{BN}_{\\beta, \\gamma}(X)\\), \\(\\text{shape} \\left(\\mathrm{BN}_{\\beta, \\gamma}(X)\\right)=\\operatorname{shape}(X)\\)</p> <p>\\(\\mathrm{BN}_{\\beta, \\gamma}\\) for linear layers acts independently over neurons.</p> \\[ \\begin{gathered} \\hat{\\mu}[:]=\\frac{1}{B} \\sum_{b=1}^{B} X[b,:]\\\\ \\hat{\\sigma}^{2}[:]=\\frac{1}{B} \\sum_{b=1}^{B}(X[b,:]-\\hat{\\mu}[:])^{2} \\\\ \\mathrm{BN}_{\\gamma, \\beta}(X)[b,:]=\\gamma[:] \\frac{X[b,:]-\\hat{\\mu}[:]}{\\sqrt{\\hat{\\sigma}^{2}[:]+\\varepsilon}}+\\beta[:] \\quad b=1, \\ldots, B \\end{gathered} \\] <p>where operations are elementwise. BN normalizes each output neuron. The mean and variance are explicitly controlled through learned parameters \\(\\beta\\) and \\(\\gamma\\). In Pytorch, <code>nn.BatchNorm1d</code>.</p> <p>Definition 6.47 : BN for Convolutional Layers</p> <p>Underlying assumption: Each element of the batch, horizontal pixel, and vertical pixel is an IID sample.</p> <p>Input: \\(X\\), \\(\\text{shape}(X) =  \\text{(batch size)} \\times \\text{(channels)} \\times \\text{(vertical dim)} \\times \\text{(horizontal dim)}\\)</p> <p>Output: \\(\\mathrm{BN}_{\\beta, \\gamma}(X)\\), \\(\\text{shape} \\left(\\mathrm{BN}_{\\beta, \\gamma}(X)\\right)=\\operatorname{shape}(X)\\)</p> <p>\\(\\mathrm{BN}_{\\beta, \\gamma}\\) for conv. layers acts independently over channels.</p> \\[ \\begin{gathered} \\hat{\\mu}[:]=\\frac{1}{B P Q} \\sum_{b=1}^{B} \\sum_{i=1}^{P} \\sum_{j=1}^{Q} X[b,:, i, j] \\\\ \\hat{\\sigma}^{2}[:]=\\frac{1}{B P Q} \\sum_{b=1}^{B} \\sum_{i=1}^{P} \\sum_{j=1}^{Q}(X[b,:, i, j]-\\hat{\\mu}[:])^{2} \\\\ \\operatorname{BN}_{\\gamma, \\beta}(X)[b,:, i, j]=\\gamma[:] \\frac{X[b,:, i, j]-\\hat{\\mu}[:]}{\\sqrt{\\hat{\\sigma}^{2}[:]+\\varepsilon}}+\\beta[:] \\quad \\begin{array}{l} b=1, \\ldots, B \\\\ i=1, \\ldots, P \\\\ j=1, \\ldots, Q \\end{array} \\end{gathered} \\] <p>BN normalizes over each convolutional filter. The mean and variance are explicitly controlled through learned parameters \\(\\beta\\) and \\(\\gamma\\). In Pytorch, <code>nn.BatchNorm2d</code>.</p> <p>Definition 6.48 : BN during Testing</p> <p>\\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}\\) are estimated from batches during training. During testing, we don't update the NN, and we may only have a single input (so no batch).</p> <p>There are 2 strategies for computing final values of \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}\\) :</p> <ol> <li>After training, fix all parameters and evaluate NN on full training set to compute \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}\\) layer-by-layer. Store this computed value. (Computation of \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}\\) must be done sequentially layer-by-layer. Why?)</li> <li>During training, compute running average of \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}\\). This is the default behavior of PyTorch.</li> </ol> <p>In PyTorch, use <code>model.train()</code> and <code>model.eval()</code> to switch BN behavior between training and testing.</p> <p>Concept 6.49 : Efficiency of BN</p> <p>BN does not change the representation power of NN ; since \\(\\beta\\) and \\(\\gamma\\) are trained, the output of each layer can have any mean and variance. However, controlling the mean and variance as explicit trainable parameters makes training easier.</p> <p>With BN, the choice of batch size becomes a more important hyperparameter to tune.</p> <p>BN is indispensable in practice. Training of VGGNet and GoogLeNet becomes much easier with BN. Training of ResNet requires BN.</p> <p>Concept 6.51 : BN and Internal Covariate Shift</p> <p>BN has insufficient theoretical justification. The original paper by loffe and Szegedy hypothesized that BN mitigates internal covariate shift (ICS), the shift in the mean and variance of the intermediate layer neurons throughout the training, and that this mitigation leads to improved training.</p> \\[ \\mathrm{BN} \\Rightarrow(\\text { reduced ICS }) \\Rightarrow \\text { (improved training }) \\] <p>However, Santukar et al. demonstrated that when experimentally measured, BN does not mitigate ICS, but nevertheless improves the training.</p> \\[ \\mathrm{BN} \\nRightarrow \\text { (reduced ICS) } \\] <p>Nevertheless</p> \\[ \\mathrm{BN} \\Rightarrow \\text { (improved training performance) } \\] <p>Santukar et al. argues that</p> \\[ \\mathrm{BN} \\Rightarrow \\text { (smoother loss landscape) } \\Rightarrow \\text { (improved training performance) } \\] <p>While this claim is more evidence-based than that of loffe and Szegedy, it is still not conclusive. It is also unclear why BN makes the loss landscape smoother, and it is not clear whether the smoother loss landscape fully explains the improved training performance.</p> <p>This story is a cautionary tale: we should carefully distinguish between speculative hypotheses and evidence-based claims, even in a primarily empirical subject.</p> <p>(S. Ioffe and C. Szegedy, Batch normalization: Accelerating deep network training by reducing internal covariate shift, ICML, 2015. S. Santurkar, D. Tsipras, A. Ilyas, and A. M\u0105dry, How does batch normalization help optimization?, NeurIPS, 2018.)</p> <p>Concept 6.50 : BN has trainable parameters.</p> <p>BN is usually not considered a trainable layer, much like pooling or dropout, and they are usually excluded when counting the \"depth\" of a NN. However, BN does have trainable parameters. Interestingly, if one randomly initializes a CNN, freezes all other parameters, and only train BN parameters, the performance is surprisingly good.</p> <p> </p> <p>(J. Frankle, D. J. Schwab, and A. S. Morcos, Training BatchNorm and only BatchNorm: On the expressive power of random features in CNNs, NeurIPS SEDL Workshop, 2019.)</p> <p>Concept 6.51 : Discussion of BN</p> <p>BN seems to also act as a regularizer, and for some reason subsumes effect Dropout. (Using dropout together with BN seems to worsen performance.) Since BN has been popularized, Dropout is used less often.</p> <p>After training, functionality of BN can be absorbed into the previous layer when the previous layer is a linear layer or a conv layer.</p> <p>The use of batch norm makes the scaling of weight initialization less important irrelevant.</p> <p>Use <code>bias=false</code> on layers preceding BN , since \\(\\beta\\) subsumes the bias.</p> <p>(X. Li, S. Chen, X. Hu and J. Yang, Understanding the disharmony between dropout and batch normalization by variance shift, CVPR, 2019.)</p>"},{"location":"books-and-courses/mfdnn/7/","title":"\u00a7 7. ImageNet Challenge","text":"<p>Definition 7.1 : ImageNet Dataset</p> <p>ImageNet contains more 14 million hand-annotated images in more than 20,000 categories. Many classes, higher resolution, non-uniform image size, multiple objects per image.</p> <p> </p> <p>History</p> <ul> <li>Fei-Fei Li started the ImageNet project in 2006 with the goal of expanding and improving the data available for training Al algorithms.</li> <li>Images were annotated with Amazon Mechanical Turk.</li> <li>The ImageNet team first presented their dataset in the 2009 Conference on Computer Vision and Pattern Recognition (CVPR).</li> <li>From 2010 to 2017, the ImageNet project ran the ImageNet Large Scale Visual Recognition Challenge (ILSVRC).</li> <li>In the 2012 ILSVRC challenge, 150,000 images of 1000 classes were used.</li> <li>In 2017, 29 teams achieved above 95\\% accuracy. The organizers deemed task complete and ended the ILSVRC competition.</li> </ul> <p>ImageNet-1k</p> <p>Commonly referred to as \"the ImageNet dataset\". Also called ImageNet2012. However, ImageNet-1k is really a subset of full ImageNet dataset. ImageNet-1k has 150,000 images of 1000 roughly balanced classes.</p> <p> </p> <p>Top-1 vs Top-5 Accuracy</p> <p>Classifiers on ImageNet-1k are often assessed by their top-5 accuracy, which requires the 5 categories with the highest confidence to contain the label. In contrast, the top-1 accuracy simply measures whether the network's single prediction is the label.</p> <p>For example, AlexNet had a top-5 accuracy of 84.6% and a top-1 accuracy of 63.3%. Nowadays, accuracies of classifiers has improved, so the top 1 accuracy is becoming the more common metric.</p>"},{"location":"books-and-courses/mfdnn/7/#lenet","title":"LeNet","text":"<p>Definition 7.2 : LeNet5</p> <p> </p> <p>(Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, Gradient-based learning applied to document recognition, Proceedings of the IEEE, 1998.)</p> <p>Concept 7.3 : Architectural Contribution : LeNet</p> <p>One of the earliest demonstration of using a deep CNN to learn a nontrivial task.</p> <p>Laid the foundation of the modern CNN architecture.</p>"},{"location":"books-and-courses/mfdnn/7/#alexnet","title":"AlexNet","text":"<p>Definition 7.4 : AlexNet</p> <p>Won the 2012 ImageNet challenge by a large margin: top-5 error rate \\(15.3 \\%\\) vs. \\(26.2 \\%\\) second place.</p> <p>Started the era of deep neural networks and their training via GPU computing.</p> <p>AlexNet was split into 2 as GPU memory was limited. (A single modern GPU can easily hold AlexNet.)</p> <p> </p> <p>(A. Krizhevsky, I. Sutskever, and G. E. Hinton, ImageNet classification with deep convolutional neural networks, NeurIPS, 2012.)</p> <p>Definition 7.5 : AlexNet for ImageNet</p> <p> </p> <p>Definition 7.6 : AlexNet for Cifar10</p> <p> </p> <p>Concept 7.7 : Architectural Contribution : AlexNet</p> <p>A scaled-up version of LeNet.</p> <p>Demonstrated that deep CNNs can learn significantly complex tasks. (Some thought CNNs could only learn simple, toy tasks like MNIST.)</p> <p>Demonstrated GPU computing to be an essential component of deep learning.</p> <p>Demonstrated effectiveness of ReLU over sigmoid or tanh in deep CNNs for classification.</p>"},{"location":"books-and-courses/mfdnn/7/#vggnet","title":"VGGNet","text":"<p>Definition 7.8 : VGGNet</p> <p> </p> <p> </p> <p>(K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale image recognition, ICLR, 2015.)</p> <p>Definition 7.9 : VGGNet for Cifar10</p> <p> </p> <p>Concept 7.10 : Architectural Contribution : VGGNet</p> <p>Demonstrated simple deep CNNs can significantly improve upon AlexNet.</p> <p>In a sense, VGGNet represents the upper limit of the simple CNN architecture. (It is the best simple model.) Future architectures make gains through more complex constructions.</p> <p>Demonstrated effectiveness of stacked \\(3 \\times 3\\) convolutions over larger \\(5 \\times 5\\) or \\(11 \\times 11\\) convolutions. Large convolutions (larger than \\(5 \\times 5\\) ) are now uncommon.</p> <p>Due to its simplicity, VGGNet is one of the most common test subjects for testing something on deep CNNs.</p>"},{"location":"books-and-courses/mfdnn/7/#nin-network","title":"NiN Network","text":"<p>Concept 7.11 : Linear layers have too many parameters.</p> <p>Linear layers have too many parameters.</p> <ul> <li> <p>AlexNet:</p> <p>Conv layer params: 2,469,696 (4%) Linear layer params: 58,631,144 (96%) Total params: 61,100,840</p> </li> <li> <p>VGG19:</p> <p>Conv layer params: 20,024,384 (14%) Linear layer params: 123,642,856 (86%) Total params: 143,667,240</p> </li> </ul> <p>Definition 7.12 : Network in Network (NiN)</p> <p> </p> <p>(M. Lin, Q. Chen, and S. Yan, Network In Network, arXiv, 2013.)</p> <p>Concept 7.13 : \\(1 \\times 1\\) Convolution</p> <p>A \\(1 \\times 1\\) convolution is like a fully connected layer acting independently and identically on each spatial location.</p> <p> </p> <ul> <li>96 filters act on 192 channels separately for each pixel</li> <li>\\(96 \\times 192+96\\) parameters for weights and biases</li> </ul> <p>Concept 7.14 : Regular Convolution Layer vs Network in Network</p> <p>Regular Convolution Layer</p> <p>Input: \\(X \\in \\mathbb{R}^{C_{0} \\times m \\times n}\\)</p> <ul> <li>Select an \\(f \\times f\\) patch \\(\\tilde{X}=X[:, i: i+f, j: j+f]\\).</li> <li>Inner product \\(\\tilde{X}\\) and \\(w_{1}, \\ldots, w_{C_{1}} \\in \\mathbb{R}^{C_{0} \\times f \\times f}\\) and add bias \\(b_{1} \\in \\mathbb{R}^{C_{1}}\\).</li> <li>Apply \\(\\sigma\\). (Output in \\(\\mathbb{R}^{C_{1}}\\).)</li> </ul> <p>Repeat this for all patches. Output in \\(X \\in \\mathbb{R}^{C_{1} \\times(m-f+1) \\times(n-f+1)}\\). Repeat this for all batch elements.</p> <p>Network in Network</p> <p>Input: \\(X \\in \\mathbb{R}^{c_{0} \\times m \\times n}\\)</p> <ul> <li>Select an \\(f \\times f\\) patch \\(\\tilde{X}=X[:, i: i+f, j: j+f]\\).</li> <li>Inner product \\(\\tilde{X}\\) and \\(w_{1}, \\ldots, w_{C_{1}} \\in \\mathbb{R}^{C_{0} \\times f \\times f}\\) and add bias \\(b_{1} \\in \\mathbb{R}^{C_{1}}\\).</li> <li>Apply \\(\\sigma\\). (Output in \\(\\mathbb{R}^{C_{1}}\\).)</li> <li>Apply Linear \\(A_{A_{2}, b_{2}}(x)\\) where \\(A_{2} \\in \\mathbb{R}^{C_{2} \\times C_{1}}\\) and \\(b_{2} \\in \\mathbb{R}^{C_{2}}\\).</li> <li>Apply \\(\\sigma\\). (Output in \\(\\mathbb{R}^{C_{2}}\\).)</li> <li>Apply Linear \\(A_{A_{3}, b_{3}}(x)\\) where \\(A_{3} \\in \\mathbb{R}^{C_{3} \\times C_{2}}\\) and \\(b_{3} \\in \\mathbb{R}^{C_{3}}\\).</li> <li>Apply \\(\\sigma\\). (Output in \\(\\mathbb{R}^{C_{3}}\\).)</li> </ul> <p>Repeat this for all patches. Output in \\(X \\in \\mathbb{R}^{C_{3} \\times(m-f+1) \\times(n-f+1)}\\). Repeat this for all batch elements. Why is this equivalent to (\\(3 \\times 3\\) conv)-(\\(1 \\times 1\\) conv)-(\\(1 \\times 1\\) conv)?</p> <p>Concept 7.15 : Global Average Pool</p> <p>When using CNNs for classification, position of object is not important.</p> <p>The global average pool has no trainable parameters (linear layers have many) and it is translation invariant. Global average pool removes the spatial dependency.</p> <p>Concept 7.16 : Architectural Contribution : NiN Network</p> <p>Used \\(1 \\times 1\\) convolutions to increase the representation power of the convolutional modules.</p> <p>Replaced linear layer with average pool to reduce number of trainable parameters.</p> <p>First step in the trend of architectures becoming more abstract. Modern CNNs are built with smaller building blocks.</p>"},{"location":"books-and-courses/mfdnn/7/#googlenet","title":"GoogLeNet","text":"<p>Definition 7.17 : GoogLeNet (Inception v1)</p> <p>Utilizes the inception module. Structure inspired by NiN and name inspired by 2010 Inception movie meme.</p> <p>Used \\(1 \\times 1\\) convolutions.</p> <ul> <li>Increased depth adds representation power (improves ability to represent nonlinear functions).</li> <li>Reduce the number of channels before the expensive \\(3 \\times 3\\) and \\(5 \\times 5\\) convolutions, and thereby reduce number of trainable weights and computation time.</li> </ul> <p>The name GoogLeNet is a reference to the authors' Google affiliation and is an homage to LeNet.</p> <p> </p> <p>(C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, Going deeper with convolutions, CVPR, 2015)</p> <p> </p> <p>Definition 7.18 : GoogLeNet for Cifar10</p> <p> </p> <p>Concept 7.19 : Architectural Contribution : GoogLeNet</p> <p>Demonstrated that more complex modular neural network designs can outperform VGGNet's straightforward design.</p> <p>Together with VGGNet, demonstrated the importance of depth.</p> <p>Kickstarted the research into deep neural network architecture design.</p>"},{"location":"books-and-courses/mfdnn/7/#resnet","title":"ResNet","text":""},{"location":"books-and-courses/mfdnn/8/","title":"\u00a7 8. CNNs for Other Supervised Learning Tasks","text":""},{"location":"books-and-courses/mfdnn/8/#inverse-problem","title":"Inverse Problem","text":"<p>Definition 8.1 : Inverse Problem Model</p> <p>In inverse problems, we wish to recover a signal \\(X_{\\text {true }}\\) given measurements \\(Y\\). The unknown and the measurements are related through</p> \\[ \\mathcal{A}\\left[X_{\\text {true }}\\right]+\\varepsilon=Y, \\] <p>where \\(\\mathcal{A}\\) is often, but not always, linear, and \\(\\varepsilon\\) represents small error.</p> <p>The forward model \\(\\mathcal{A}\\) may or may not be known. In other words, the goal of an inverse problem is to find an approximation of \\(\\mathcal{A}^{-1}\\).</p> <p>In many cases, \\(\\mathcal{A}\\) is not even be invertible. In such cases, we can still hope to find an mapping that serves as an approximate inverse in practice.</p> <p>Concept 8.2 : Inverse Problems via Deep Learning</p> <p>In deep learning, we use a neural network to approximate the inverse mapping</p> \\[ f_{\\theta} \\approx \\mathcal{A}^{-1} \\] <p>i.e., we want \\(f_{\\theta}(Y) \\approx X_{\\text {true }}\\) for the measurements \\(X\\) that we care about.</p> <p>If we have \\(X_{1}, \\ldots, X_{N}\\) and \\(Y_{1}, \\ldots, Y_{N}\\) (but no direct knowledge of \\(\\mathcal{A}\\) ), we can solve</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\sum_{i=1}^{N}\\left\\|f_{\\theta}\\left(Y_{i}\\right)-X_{i}\\right\\| \\] <p>If we have \\(X_{1}, \\ldots, X_{N}\\) and knowledge of \\(\\mathcal{A}\\), we can solve</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\sum_{i=1}^{N}\\left\\|f_{\\theta}\\left[\\mathcal{A}\\left(X_{i}\\right)\\right]-X_{i}\\right\\| \\] <p>If we have \\(Y_{1}, \\ldots, Y_{N}\\) and knowledge of \\(\\mathcal{A}\\), we can solve</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\sum_{i=1}^{N}\\left\\|\\mathcal{A}\\left[f_{\\theta}\\left(Y_{i}\\right)\\right]-Y_{i}\\right\\| \\]"},{"location":"books-and-courses/mfdnn/8/#gaussian-denoising","title":"Gaussian Denoising","text":"<p>Definition 8.3 : Gaussian Denoising</p> <p>Given \\(X_{\\text {true }} \\in \\mathbb{R}^{w \\times h}\\), we measure</p> \\[ Y=X_{\\text {true }}+\\varepsilon \\] <p>where \\(\\varepsilon_{i j} \\sim \\mathcal{N}\\left(0, \\sigma^{2}\\right)\\) is IID Gaussian noise. For the sake of simplicity, assume we know \\(\\sigma\\). Goal is to recover \\(X_{\\text {true }}\\) from \\(Y\\).</p> <p>Gaussian denoising is the simplest setup in which the goal is to remove noise from the image. In more realistic setups, the noise model will be more complicated and the noise level \\(\\sigma\\) will be unknown.</p> <p>Definition 8.4 : DnCNN</p> <p>In 2017, Zhang et al. presented the denoising convolutional neural networks (DnCNNs). They trained a 17-layer CNN \\(f_{\\theta}\\) to learn the noise with the loss</p> \\[ \\mathcal{L}(\\theta)=\\sum_{i=1}^{N}\\left\\|f_{\\theta}\\left(Y_{i}\\right)-\\left(Y_{i}-X_{i}\\right)\\right\\|^{2} \\] <p>so that the clean recovery can be obtained with \\(Y_{i}-f_{\\theta}\\left(Y_{i}\\right)\\). (This is equivalent to using a residual connection from beginning to end.)</p> <p> </p> <p>Image denoising is was an area with a large body of prior work. DnCNN dominated all prior approaches that were not based on deep learning.</p> <p>Nowadays, all state-of-the-art denoising algorithms are based on deep learning.</p> <p> </p> <p>(K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang, Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising, IEEE TIP, 2017.)</p>"},{"location":"books-and-courses/mfdnn/8/#image-super-resolution","title":"Image Super-Resolution","text":"<p>Definition 8.5 : Image Super-Resolution</p> <p>Given \\(X_{\\text {true }} \\in \\mathbb{R}^{w \\times h}\\), we measure</p> \\[ Y=\\mathcal{A}\\left(X_{\\text {true }}\\right) \\] <p>where \\(\\mathcal{A}\\) is a \"downsampling\" operator. So \\(Y \\in \\mathbb{R}^{w_{2} \\times h_{2}}\\) with \\(w_{2}&lt;w\\) and \\(h_{2}&lt;h\\). Goal is to recover \\(X_{\\text {true }}\\) from \\(Y\\).</p> <p>In the simplest setup, \\(\\mathcal{A}\\) is an average pool operator with \\(r \\times r\\) kernel and a stride \\(r\\).</p> <p>Definition 8.6 : SRCNN</p> <p>In 2015, Dong et al. presented super-resolution convolutional neural network (SRCNN). They trained a 3-layer \\(\\operatorname{CNN} f_{\\theta}\\) to learn the high-resolution reconstruction with the loss</p> \\[ \\mathcal{L}(\\theta)=\\sum_{i=1}^{N}\\left\\|f_{\\theta}\\left(\\tilde{Y}_{i}\\right)-X_{i}\\right\\|^{2} \\] <p>where \\(\\tilde{Y}_{i} \\in \\mathbb{R}^{w \\times h}\\) is an upsampled version of \\(Y_{i} \\in \\mathbb{R}^{(w / r) \\times(h / r)}\\), i.e., \\(\\tilde{Y}_{i}\\) has the same number of pixels as \\(X_{i}\\), but the image is pixelated or blurry. The goal is to have \\(f_{\\theta}\\left(\\tilde{Y}_{i}\\right)\\) be a sharp reconstruction.</p> <p> </p> <p>SRCNN showed that simple learning based approaches can match the state-of the art performances of superresolution task.</p> <p> </p> <p>(C. Dong, C. C. Loy, K. He, and X. Tang, Image super-resolution using deep convolutional networks, IEEE TPAMI, 2015.)</p> <p>Definition 8.7 : VDSR</p> <p>In 2016, Kim et al. presented VDSR. They trained a 20-layer CNN with a residual connection \\(f_{\\theta}\\) to learn the high-resolution reconstruction with the loss</p> \\[ \\mathcal{L}(\\theta)=\\sum_{i=1}^{N}\\left\\|f_{\\theta}\\left(\\tilde{Y}_{i}\\right)-X_{i}\\right\\|^{2} \\] <p>The residual connection was the key insight that enabled the training of much deeper CNNs.</p> <p> </p> <p>VDSR dominated all prior approaches not based on deep learning. Showed that simple learning based approaches can batch the state-of theart performances of super-resolution task.</p> <p> </p> <p>(J. Kim, J. K. Lee, and K. M. Lee, Accurate image super-resolution using very deep convolutional networks, CVPR, 2016.)</p>"},{"location":"books-and-courses/mfdnn/8/#other-examples","title":"Other Examples","text":"<p>Example 8.8 : SRGAN</p> <p> </p> <p>(C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi, Photo-realistic single image super-resolution using a generative adversarial network, CVPR, 2017.)</p> <p>Example 8.9 : Image Colorization</p> <p> </p> <p>(R. Zhang, P. Isola, and A. A. Efros, Colorful image colorization, ECCV, 2016.)</p> <p>Example 8.10 : Image Inpainting</p> <p> </p> <p>(J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang, Generative image inpainting with contextual attention, CVPR, 2018.)</p>"},{"location":"books-and-courses/mfdnn/8/#operations-increasing-spatial-dimensions","title":"Operations Increasing Spatial Dimensions","text":"<p>Concept 8.11 : Operations Increasing Spatial Dimensions</p> <p>In image classification tasks, the spatial dimensions of neural networks often decrease as the depth progresses.</p> <p>This is because we are trying to forget location information. (In classification, we care about what is in the image, but we do not where it is in the image.)</p> <p>However, there are many networks for which we want to increase the spatial dimension:</p> <ul> <li>Linear layers</li> <li>Upsampling</li> <li>Transposed convolution</li> </ul>"},{"location":"books-and-courses/mfdnn/8/#transposed-convolution","title":"Transposed convolution","text":"<p>Concept 8.12 : Linear Operator \\(\\cong\\) Matrix</p> <p>Core tenet of linear algebra: matrices are linear operators and linear operators are matrices.</p> <p>Let \\(f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}\\) be linear, i.e.,</p> \\[ f(x+y)=f(x)+f(y) \\text { and } f(\\alpha x)=\\alpha f(x) \\] <p>for all \\(x, y \\in \\mathbb{R}^{n}\\) and \\(\\alpha \\in \\mathbb{R}\\).</p> <p>There exists a matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) that represents \\(f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}\\), i.e.,</p> \\[ f(x)=A x \\] <p>for all \\(x \\in \\mathbb{R}^{n}\\).</p> <p>Let \\(e_{i}\\) be the \\(i\\)-th unit vector, i.e., \\(e_{i}\\) has all zeros elements except entry 1 in the \\(i\\)-th coordinate.</p> <p>Given a linear \\(f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}\\), we can find the matrix</p> \\[ A=\\left[\\begin{array}{llll} A_{;, 1} &amp; A_{;, 2} &amp; \\cdots &amp; A_{;, n} \\end{array}\\right] \\in \\mathbb{R}^{m \\times n} \\] <p>representing \\(f\\) with</p> \\[ f\\left(e_{j}\\right)=A e_{j}=A_{;, j} \\] <p>for all \\(j=1, \\ldots, n\\), or with</p> \\[ e_{i}^{\\top} f\\left(e_{j}\\right)=e_{i}^{\\top} A e_{j}=A_{i, j} \\] <p>for all \\(i=1, \\ldots, m\\) and \\(j=1, \\ldots, n\\).</p> <p>Concept 8.13 : Linear Operator \\(\\ncong\\) Matrix</p> <p>In applied mathematics and machine learning, there are many setups where explicitly forming the matrix representation \\(A \\in \\mathbb{R}^{m \\times n}\\) is costly, even though the matrix-vector products \\(A x\\) and \\(A^{\\top} y\\) are efficient to evaluate.</p> <p>In machine learning, convolutions are the primary example. Other areas, linear operators based on FFTs are the primary example.</p> <p>In such setups, the matrix representation is still a useful conceptual tool, even if we never intend to form the matrix.</p> <p>Given a matrix \\(A\\), the transpose \\(A^{\\top}\\) is obtained by flipping the row and column dimensions, i.e., \\(\\left(A^{\\top}\\right)_{i j}=(A)_{j i}\\). However, using this definition is not always the most effective when understanding the action of \\(A^{\\top}\\).</p> <p>Another approach is to use the adjoint view. Since</p> \\[ y^{\\top}(A x)=\\left(A^{\\top} y\\right)^{\\top} x \\] <p>for any \\(x \\in \\mathbb{R}^{n}\\) and \\(y \\in \\mathbb{R}^{m}\\), understand the action of \\(A^{\\top}\\) by finding an expression of the form</p> \\[ y^{\\top} A x=\\sum_{j=1}^{n}(\\text { something })_{j} x_{j}=\\left(A^{\\top} y\\right)^{\\top} x \\] <p>Example 8.14 : 1D Transpose Convolution</p> <p>Consider the 1D convolution represented by \\(A \\in \\mathbb{R}^{(n-f+1) \\times n}\\) defined with a given \\(w \\in \\mathbb{R}^{f}\\) and</p> \\[ A=\\left[\\begin{array}{cccccccc} w_{1} &amp; \\cdots &amp; w_{f} &amp; 0 &amp; \\cdots &amp; &amp; &amp; 0 \\\\ 0 &amp; w_{1} &amp; \\cdots &amp; w_{f} &amp; 0 &amp; \\cdots &amp; &amp; 0 \\\\ 0 &amp; 0 &amp; w_{1} &amp; \\cdots &amp; w_{f} &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; &amp; &amp; \\ddots &amp; &amp; \\ddots &amp; &amp; \\vdots \\\\ 0 &amp; &amp; \\cdots &amp; 0 &amp; w_{1} &amp; \\cdots &amp; w_{f} &amp; 0 \\\\ 0 &amp; &amp; \\cdots &amp; 0 &amp; 0 &amp; w_{1} &amp; \\cdots &amp; w_{f} \\end{array}\\right] \\] <p>Then we have</p> \\[ (A x)_{j}=\\sum_{i=1}^{f} w_{i} x_{j+i-1} \\] <p>and we have the following formula which coincides with transposing the matrix \\(A\\).</p> \\[ \\begin{aligned} y^{\\top} A x &amp; =\\sum_{j=1}^{n-f+1} y_{j} \\sum_{i=1}^{f} w_{i} x_{j+i-1} \\\\ &amp; =\\sum_{j=1}^{n-f+1} \\sum_{i=1}^{f} y_{j} w_{i} x_{j+i-1} \\sum_{k=1}^{n} \\mathbf{1}_{\\{k=j+i-1\\}} \\\\ &amp; =\\sum_{k=1}^{n} \\sum_{j=1}^{n-f+1} \\sum_{i=1}^{f} y_{j} w_{i} x_{k} \\mathbf{1}_{\\{k-j+1=i\\}} \\\\ &amp; =\\sum_{k=1}^{n} x_{k} \\sum_{j=1}^{n-f+1} \\sum_{i=1}^{f} w_{k-j+1} y_{j} \\mathbf{1}_{\\{k-j+1=i\\}} \\\\ &amp; =\\sum_{k=1}^{n} x_{k} \\sum_{j=1}^{n-f+1} w_{k-j+1} y_{j} \\sum_{i=1}^{f} \\mathbf{1}_{\\{k-j+1=i\\}} \\\\ &amp; =\\sum_{k=1}^{n} x_{k} \\sum_{j=1}^{n-f+1} w_{k-j+1} y_{j} \\mathbf{1}_{\\{1 \\leq k-j+1 \\leq f\\}} \\\\ &amp; =\\sum_{k=1}^{n} x_{k} \\sum_{j=1}^{n-f+1} w_{k-j+1} y_{j} \\mathbf{1}_{\\{j \\leq k\\}} \\mathbf{1}_{\\{k-f+1 \\leq j\\}} \\\\ &amp; =\\sum_{k=1}^{n} x_{k} \\sum_{j=\\max (k-f+1,1)}^{\\min (n-f+1, k)} w_{k-j+1} y_{j}=\\left(A^{\\top} y\\right)^{\\top} x \\\\ \\end{aligned} \\] <p>Definition 8.15 : Transposed Convolution</p> <p>In transposed convolution, input neurons additively distribute values to the output via the kernel. Before people noticed that this is the transpose of convolution, the names backwards convolution and deconvolution were used.</p> <p>For each input neuron, multiply the kernel and add (accumulate) the value in the output. Can accommodate strides, padding, and multiple channels.</p> <p> </p> <ul> <li>Convolution Visualized</li> </ul>  ![](./assets/8.15.gif){: width=\"50%\"}  <ul> <li>Transpose Convolution Visualized</li> </ul>  ![](./assets/8.16.gif){: width=\"100%\"}  <p>Definition 8.16 : 2D Transpose Convolution Layer (Formal Definition)</p> <ul> <li>\\(B\\) : batch size</li> <li>\\(C_{\\text{in}}\\) : # of input channels</li> <li>\\(C_{\\text{out}}\\) : # of output channels</li> <li>\\(m, n\\) : # of vertical and horizontal indices of input</li> <li>\\(f_1, f_2\\) : # of vertical and horizontal indices of filter</li> </ul> <ul> <li>Input tensor: \\(Y \\in \\mathbb{R}^{B \\times C_{\\mathrm{in}} \\times m \\times n}\\)</li> <li>Output tensor: \\(X \\in \\mathbb{R}^{B \\times C_{\\text {out }} \\times\\left(m+f_{1}-1\\right) \\times\\left(n+f_{2}-1\\right)}\\)</li> <li>Filter \\(w \\in \\mathbb{R}^{C_{\\text {in }} \\times C_{\\text {out }} \\times f_{1} \\times f_{2}}\\)</li> <li>Bias \\(b \\in \\mathbb{R}^{C_{\\text {out }}}\\) (If <code>bias=False</code>, then \\(b=0\\).) </li> </ul> <pre><code>def trans_conv(Y, w, b):\n    c_in, c_out, f1, f2 = w.shape\n    batch, c_in, m, n = Y.shape\n    X = torch.zeros(batch, c_out, m + f1 - 1, n + f2 - 1)\n    for k in range(c_in):\n        for i in range(Y.shape[2]):\n            for j in range(Y.shape[3]):\n                X[:, :, i:i+f1, j:j+f2] += Y[:, k, i, j].view(-1,1,1,1)*w[k, :, :, :].unsqueeze(0)\n    return X + b.view(1,-1,1,1)\n</code></pre> <p>In a matrix representation \\(A\\) of convolution, the dependencies of the inputs and outputs are represented by the non-zeros of \\(A\\), i.e., the sparsity pattern of \\(A\\). If \\(A_{i j}=0\\), then input neuron \\(j\\) does not affect the output neuron \\(i\\). If \\(A_{i j} \\neq 0\\), then \\(\\left(A^{\\top}\\right)_{j i} \\neq 0\\). So if input neuron \\(j\\) affects output neuron \\(i\\) in convolution, then input neuron \\(i\\) affects output neuron \\(j\\) in transposed convolution.</p>  ![](./assets/8.17.png){: width=\"50%\"}  <p>We can combine this reasoning with our visual understanding of convolution. The diagram simultaneously illustrates the dependencies for both convolution and transposed convolution.</p>"},{"location":"books-and-courses/mfdnn/8/#upsampling","title":"Upsampling","text":"<p>Concept 8.17 : Upsampling : Nearest Neighbor</p> <p><code>torch.nn.Upsample</code> with <code>mode='nearest'</code></p> <p> </p> <p>Concept 8.18 : Upsampling : Bilinear Interpolation</p> <p><code>Torch.nn.Upsample</code> with <code>mode='bilinear'</code> <code>linear</code> interpolation is available for 1D data <code>trilinear</code> interpolation is available for 3D data (We won't pay attention to the interpolation formula.)</p> <p> </p>"},{"location":"books-and-courses/mfdnn/8/#semantic-segmentation","title":"Semantic Segmentation","text":"<p>Definition 8.19 : Semantic Segmentation</p> <p>In semantic segmentation, the goal is to segment the image into semantically meaningful regions by classifying each pixel.</p> <p> </p> <p>Definition 8.20 : Object Localization</p> <p>Object localization localizes a single object usually via a bounding box.</p> <p> </p> <p>Definition 8.21 : Object Detection</p> <p>Object detection detects many objects, with the same class often repeated, usually via bounding boxes.</p> <p> </p> <p>Definition 8.22 : Image Segmentation</p> <p>Instance segmentation distinguishes multiple instances of the same object type.</p> <p> </p> <p>We will focus on semantic segmentation.</p> <p>Definition 8.23 : Pascal VOC</p> <p>We will use PASCAL Visual Object Classes (VOC) dataset for semantic segmentation. (Dataset also contains labels for object detection.)</p> <p>There are 21 classes: 20 main classes and 1 \"unlabeled\" class.</p> <p>Data \\(X_{1}, \\ldots, X_{N} \\in \\mathbb{R}^{3 \\times m \\times n}\\) and labels \\(Y_{1}, \\ldots, Y_{N} \\in\\{0,1, \\ldots, 20\\}^{m \\times n}\\), i.e., \\(Y_{i}\\) provides a class label for every pixel of \\(X_{i}\\).</p> <p> </p> <p>Concept 8.24 : Loss for Semantic Segmentation</p> <p>Consider the neural network</p> \\[ f_{\\theta}: \\mathbb{R}^{3 \\times m \\times n} \\rightarrow \\mathbb{R}^{k \\times m \\times n} \\] <p>such that \\(\\mu\\left(f_{\\theta}(X)\\right)_{i j} \\in \\Delta^{k}\\) is the probabilities for the \\(k\\) classes for pixel \\((i, j)\\).</p> <p>We minimize the sum of pixel-wise cross-entropy losses</p> \\[ \\mathcal{L}(\\theta)=\\sum_{l=1}^{N} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\ell^{\\mathrm{CE}}\\left(f_{\\theta}\\left(X_{l}\\right)_{i j},\\left(Y_{l}\\right)_{i j}\\right) \\] <p>where \\(\\ell^{C E}\\) is the cross entropy loss.</p> <p>Definition 8.25 : U-Net</p> <p>The U-Net architecture:</p> <ul> <li>Reduce the spatial dimension to obtain high-level (coarse scale) features</li> <li>Upsample or transpose convolution to restore spatial dimension.</li> <li>Use residual connections across each dimension reduction stage.</li> </ul> <p> </p> <p>Definition 8.26 : Magnetic Resonance Imaging (MRI)</p> <p>Magnetic resonance imaging (MRI) is an inverse problem in which we partially measure the Fourier transform of the patient and the goal is to reconstruct the patient's image.</p> <p>So \\(X_{\\text {true }} \\in \\mathbb{R}^{n}\\) is the true original image (reshaped into a vector) with \\(n\\) pixels or voxels and \\(\\mathcal{A}\\left[X_{\\text {true }}\\right] \\in \\mathbb{C}^{k}\\) with \\(k \\ll n\\). (If \\(k=n\\), MRI scan can take hours.)</p> <p>Classical reconstruction algorithms rely on Fourier analysis, total variation regularization, compressed sensing, and optimization.</p> <p>Recent state-of-the-art use deep neural networks.</p> <p>Definition 8.27 : FastMRI Dataset</p> <p>A team of researchers from Facebook AI Research and NYU released a large MRI dataset to stimulate datadriven deep learning research for MRI reconstruction.</p> <p> </p> <p>(J. Zbontar, F. Knoll, A. Sriram, T. Murrell, Z. Huang, M. J. Muckley, A. Defazio, R. Stern, P. Johnson, M. Bruno, M. Parente, K. J. Geras, J. Katsnelson, H. Chandarana, Z. Zhang, M. Drozdzal, A. Romero, M. Rabbat, P. Vincent, N. Yakubova, J. Pinkerton, D. Wang, E. Owens, C. L. Zitnick, M. P. Recht, D. K. Sodickson, and Y. W. Lui, fastMRI: An open dataset and benchmarks for accelerated MRI, arXiv, 2019.)</p> <p>Definition 8.28 : Computational Tomography (CT)</p> <p>Computational tomography (CT) is an inverse problem in which we partially measure the Radon transform of the patient and the goal is to reconstruct the patient's image.</p> <p>So \\(X_{\\text {true }} \\in \\mathbb{R}^{n}\\) is the true original image (reshaped into a vector) with \\(n\\) pixels or voxels and \\(\\mathcal{A}\\left[X_{\\text {true }}\\right] \\in \\mathbb{R}^{k}\\) with \\(k \\ll n\\). (If \\(k=n\\), the X -ray exposure to perform the CT scan can be harmful.)</p> <p>Recent state-of-the-art use deep neural networks.</p> <p>Concept 8.29 : U-Net is used for inverse problems.</p> <p>Although U-Net was originally proposed as an architecture for semantic segmentation, it is also being used widely as one of the default architectures in inverse problems, including MRI reconstruction.</p> <p> </p> <p>(J. Zbontar, F. Knoll, A. Sriram, T. Murrell, Z. Huang, M. J. Muckley, A. Defazio, R. Stern, P. Johnson, M. Bruno, M. Parente, K. J. Geras, J. Katsnelson, H. Chandarana, Z. Zhang, M. Drozdzal, A. Romero, M. Rabbat, P. Vincent, N. Yakubova, J. Pinkerton, D. Wang, E. Owens, C. L. Zitnick, M. P. Recht, D. K. Sodickson, and Y. W. Lui, fastMRI: An open dataset and benchmarks for accelerated MRI, arXiv, 2019.)</p> <p>U-Net is also used as one of the default architectures in CT reconstruction.</p> <p> </p> <p>(K. H. Jin, M. T. McCann, E. Froustey, and M. Unser, Deep convolutional neural network for inverse problems in imaging, IEEE TIP, 2017.)</p>"},{"location":"books-and-courses/mfdnn/9/","title":"\u00a7 9. Autoencoder","text":""},{"location":"books-and-courses/mfdnn/9/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>Definition 9.1 : Unsupervised Learning</p> <p>Unsupervised learning utilizes data \\(X_{1}, \\ldots, X_{N}\\) to learn the \"structure\" of the data. No labels are utilized.</p> <p>There are a wide range of unsupervised learning tasks. In this class, we discuss just a few.</p> <p>Generally, unsupervised learning tasks tend to have more mathematical complexity.</p> <p>Concept 9.2 : Many data has low-dimensional latent representation, and the task is to find it.</p> <p>Many high-dimensional data has some underlying low-dimensional structure. (One can model this assumption as data residing in a low dimensional manifold and utilize ideas from differential geometry. We won\u2019t pursue this direction)</p> <p>If you randomly generate the pixels of a color image \\(X \\in \\mathbb{R}^{3 \\times m \\times n}\\), it will likely make no sense. Only a very small subset of pixel values correspond to meaningful images.</p> <p>In machine learning, especially in unsupervised learning, finding a \"meaningful\" low dimensional latent representation is of interest.</p> <p>A good lower-dimensional representation of the data implies you have a good understanding of the data.</p>"},{"location":"books-and-courses/mfdnn/9/#definition-of-autoencoder","title":"Definition of Autoencoder","text":"<p>Definition 9.3 : Autoencoder</p> <p>An autoencoder (AE) has encoder \\(E_{\\theta}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{r}\\) and decoder \\(D_{\\varphi}: \\mathbb{R}^{r} \\rightarrow \\mathbb{R}^{n}\\) networks, where \\(r \\ll n\\). (If \\(r \\geq n\\), AE learns identity mapping, so pointless.) The two networks are trained through the loss</p> \\[ \\mathcal{L}(\\theta, \\varphi)=\\sum_{i=1}^{N}\\left\\|X_{i}-D_{\\varphi}\\left(E_{\\theta}\\left(X_{i}\\right)\\right)\\right\\|^{2} \\] <p>The low-dimensional output \\(E_{\\theta}(X)\\) is the latent vector. The encoder performs dimensionality reduction.</p> <p>The autoencoder can be thought of as a deep non-linear generalization of the principle component analysis (PCA).</p> <p> </p> <p>(G. E. Hinton and R. R. Salakhutdinov, Reducing the dimensionality of data with neural networks, Science, 2006.)</p>"},{"location":"books-and-courses/mfdnn/9/#applications-of-autoencoder","title":"Applications of Autoencoder","text":"<p>Concept 9.4 : Applications of AE: Denoising</p> <p>Autoencoders can be used to denoise or reconstruct corrupted images.</p> <p> </p> <p>(P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol, Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion, JMLR, 2010. G. Nishad, Reconstruct corrupted data using Denoising Autoencoder, Medium, 2020.)</p> <p>Concept 9.5 : Applications of AE: Compression</p> <p>Once an AE has been trained, storing the latent variable representation, rather than the original image can be used as a compression mechanism.</p> <p>More generally, latent variable representations can be used for video compression. (link)</p> <p>Concept 9.6 : Applications of AE: Clustering</p> <p>Train an AE and then perform clustering on the latent variables. For the clustering algorithm, one can use things like k-means, which groups together.</p> <p>Clustering is also referred to as unsupervised classification. Without labels, we want the group \"similar\" data.</p> <p> </p> <p>(J. Xie, R. Girshick, and A. Farhadi, Unsupervised deep embedding for clustering analysis, ICML, 2016.)</p> <p>Concept 9.7 : Anomaly/Outlier Detection</p> <p>Problem: detecting data that is significantly different from the data seen during training.</p> <p>Insight: AE should not be able to faithfully reconstruct novel data.</p> <p>Solution: Train an AE and define the score function to be the reconstruction loss:</p> \\[ s(X)=\\left\\|X-D_{\\varphi}\\left(E_{\\theta}(X)\\right)\\right\\|^{2} \\] <p>If score is high, determine the datapoint to be an outliner.</p> <p>(S. Hawkins, H. He, G. Williams, and R. Baxter, Outlier detection using replicator neural networks, DaWaK, 2002.)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/","title":"Chapter 1:  Optimization and Stochastic Gradient Descent","text":"<p>Mathematical Foundations of Deep Neural Networks Spring 2024 Department of Mathematical Sciences Ernest K. Ryu Seoul National University</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#optimization-problem","title":"Optimization problem","text":"<p>In an optimization problem, we minimize or maximize a function value, possibly subject to constraints.</p> \\[ \\begin{array}{ll} \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} &amp; f(\\theta) \\\\ \\text { subject to } &amp; h_{1}(\\theta)=0 \\end{array} \\] <p>Decision variable: \\(\\theta\\) Objective function: \\(f\\) Equality constraint: \\(h_{i}(\\theta)=0\\) for \\(i=1, \\ldots, m\\) Inequality constraint: \\(g_{j}(\\theta) \\leq 0\\) for \\(j=1, \\ldots, n\\)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#minimization-vs-maximization","title":"Minimization vs. maximization","text":"<p>In machine learning (ML), we often minimize a \"loss\", but sometimes we maximize the \"likelihood\".</p> <p>In any case, minimization and maximization are equivalent since</p> \\[ \\text { maximize } f(\\theta) \\quad \\Leftrightarrow \\quad \\text { minimize }-f(\\theta) \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#feasible-point-and-constraints","title":"Feasible point and constraints","text":"<p>\\(\\theta \\in \\mathbb{R}^{p}\\) is a feasible point if it satisfies all constraints:</p> \\[ \\begin{array}{cc} h_{1}(\\theta)=0 &amp; g_{1}(\\theta) \\leq 0 \\\\ \\vdots &amp; \\vdots \\\\ h_{m}(\\theta)=0 &amp; g_{n}(\\theta) \\leq 0 \\end{array} \\] <p>Optimization problem is infeasible if there is no feasible point.</p> <p>An optimization problem with no constraint is called an unconstrained optimization problem. Optimization problems with constraints is called a constrained optimization problem.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#optimal-value-and-solution","title":"Optimal value and solution","text":"<p>Optimal value of an optimization problem is</p> \\[ p^{\\star}=\\inf \\left\\{f(\\theta) \\mid \\theta \\in \\mathbb{R}^{n}, \\theta \\text { feasible }\\right\\} \\] <ul> <li>\\(p^{\\star}=\\infty\\) if problem is infeasible</li> <li>\\(p^{\\star}=-\\infty\\) is possible</li> <li>In ML, it is often a priori clear that \\(0 \\leq p^{\\star}&lt;\\infty\\).</li> </ul> <p>If \\(f\\left(\\theta^{\\star}\\right)=p^{\\star}\\), we say \\(\\theta^{\\star}\\) is a solution or \\(\\theta^{\\star}\\) is optimal.</p> <ul> <li>A solution may or may not exist.</li> <li>A solution may or may not be unique.</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#example-curve-fitting","title":"Example: Curve fitting","text":"<p>Consider setup with data \\(X_{1}, \\ldots, X_{N}\\) and corresponding labels \\(Y_{1}, \\ldots, Y_{N}\\) satisfying the relationship</p> \\[ Y_{i}=f_{\\star}\\left(X_{i}\\right)+\\text { error } \\] <p>for \\(i=1, \\ldots, N\\). Hopefully, \"error\" is small. True function \\(f_{\\star}\\) is unknown.</p> <p>Goal is to find a function (curve) \\(f\\) such that \\(f \\approx f_{\\star}\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#example-least-squares","title":"Example: Least-squares","text":"<p>In least-squares minimization, we solve</p> \\[ \\operatorname{minimize}_{\\theta \\in \\mathbb{R} P} \\quad \\frac{1}{2}\\|X \\theta-Y\\|^{2} \\] <p>where \\(X \\in \\mathbb{R}^{N \\times p}\\) and \\(Y \\in \\mathbb{R}^{N}\\). Equivalent to</p> \\[ \\operatorname{minimize}_{\\theta \\in \\mathbb{R}^{p}} \\frac{1}{2} \\sum_{i=1}^{N}\\left(X_{i}^{\\top} \\theta-Y_{i}\\right)^{2} \\] <p>where \\(X=\\left[\\begin{array}{c}X_{1}^{\\top} \\\\ \\vdots \\\\ X_{N}^{\\top}\\end{array}\\right]\\) and \\(Y=\\left[\\begin{array}{c}Y_{1} \\\\ \\vdots \\\\ Y_{N}\\end{array}\\right]\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#example-least-squares_1","title":"Example: Least-squares","text":"<p>To solve</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\frac{1}{2}\\|X \\theta-Y\\|^{2} \\] <p>take grad and set it to 0 :</p> \\[ \\begin{gathered} X^{\\top}\\left(X \\theta^{\\star}-Y\\right)=0 \\\\ \\theta^{\\star}=\\left(X^{\\top} X\\right)^{-1} X^{\\top} Y \\end{gathered} \\] <p>Here, we assume \\(X^{\\top} X\\) is invertible.</p> <p>Make sure you understand why</p> \\[ \\nabla_{\\theta} \\frac{1}{2}\\|X \\theta-Y\\|^{2}=X^{\\top}(X \\theta-Y) \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#ls-is-an-instance-of-curve-fitting","title":"LS is an instance of curve fitting","text":"<p>How is LS curve fitting? Define \\(f_{\\theta}(x)=x^{\\top} \\theta\\). Then LS becomes</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\frac{1}{2} \\sum_{i=1}^{N}\\left(f_{\\theta}\\left(X_{i}\\right)-Y_{i}\\right)^{2} \\] <p>and the solution hopefully satisfies</p> \\[ Y_{i}=f_{\\theta}\\left(X_{i}\\right)+\\text { small. } \\] <p>Since \\(X_{i}\\) and \\(Y_{i}\\) is assumed to satisfy</p> \\[ Y_{i}=f_{\\star}\\left(X_{i}\\right)+\\text { error } \\] <p>we are searching over linear functions (linear curves) \\(f_{\\theta}\\) that best fit (approximate) \\(f_{\\star}\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#local-vs-global-minima","title":"Local vs. global minima","text":"<p>\\(\\theta^{\\star}\\) is a local minimum if \\(f(\\theta) \\geq f\\left(\\theta^{\\star}\\right)\\) for all feasible \\(\\theta\\) within a small neighborhood. \\(\\theta^{\\star}\\) is a global minimum if \\(f(\\theta) \\geq f\\left(\\theta^{\\star}\\right)\\) for all feasible \\(\\theta\\).</p> <p>In the worst case, finding the global minimum of an optimization problem is difficult*. </p> <p>However, in deep learning, optimization problems are often \"solved\" without any guarantee of global optimality.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#gradient-descent","title":"Gradient descent","text":"<p>Consider the unconstrained optimization problem</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{P}}{\\operatorname{minimize}} f(\\theta) \\] <p>where \\(f\\) is differentiable.</p> <p>Gradient Descent (GD) algorithm:</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\nabla f\\left(\\theta^{k}\\right) \\quad \\text { for } k=0,1, \\ldots, \\] <p>where \\(\\theta^{0} \\in \\mathbb{R}^{p}\\) is the initial point and \\(\\alpha_{k}&gt;0\\) is the learning rate or the stepsize.</p> <p>The terminology learning rate is common the machine learning literature while stepsize is more common in the optimization literature.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#definition-of-differentiability","title":"Definition of \"differentiability\"","text":"<p>In math, a function is \"differentiable\" if its derivative exists everywhere.</p> <p>In deep learning (DL), a function is often said to be differentiable if its derivative exists almost everywhere and the function is nice*. ReLU activation functions are said to be differentiable.</p> <p>We won't be too concerned with this distinction.</p> <p>Differentiable in DL \\&amp; Math</p> <p>Differentiable in DL but not in Math </p> <p>Not differentiable in DL or Math  \\(\\longrightarrow\\)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#why-does-gd-converge","title":"Why does GD converge?","text":"\\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\nabla f\\left(\\theta^{k}\\right) \\] <p>Taylor expansion of \\(f\\) about \\(\\theta^{k}\\) :</p> \\[ f(\\theta)=f\\left(\\theta^{k}\\right)+\\nabla f\\left(\\theta^{k}\\right)^{\\top}\\left(\\theta-\\theta^{k}\\right)+\\mathcal{O}\\left(\\left\\|\\theta-\\theta^{k}\\right\\|^{2}\\right) \\] <p>Plug in \\(\\theta^{k+1}\\) :</p> \\[ f\\left(\\theta^{k+1}\\right)=f\\left(\\theta^{k}\\right)-\\alpha_{k}\\left\\|\\nabla f\\left(\\theta^{k}\\right)\\right\\|^{2}+\\mathcal{O}\\left(\\alpha_{k}^{2}\\right) \\] <p>\\(-\\nabla f\\left(\\theta^{k}\\right)\\) is steepest descent direction. For small (cautious) \\(\\alpha_{k}\\), GD step reduces function value.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#is-gd-a-descent-method","title":"Is GD a \"descent method\"?","text":"\\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\nabla f\\left(\\theta^{k}\\right) \\] <p>Without further assumptions, \\(-\\nabla f\\left(\\theta^{k}\\right)\\) only gives you directional information. How far should you go? How large should \\(\\alpha_{k}\\) be?</p> <p>A step of GD need not result in descent, i.e., \\(f\\left(\\theta^{k+1}\\right)&gt;f\\left(\\theta^{k}\\right)\\) is possible.</p> <p>We need an assumption that ensures the first-order Taylor expansion is a good approximation within a sufficiently large neighborhood. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#what-can-we-prove","title":"What can we prove?","text":"<p>Without further assumptions, there is no hope of finding the global minimum.</p> <p>We cannot prove the function value converges to global optimum. We instead prove \\(\\nabla f\\left(\\theta^{k}\\right) \\rightarrow 0\\). Roughly speaking, this is similar, but weaker than proving that \\(\\theta^{k}\\) converges to a local minimum.*  \\({ }^{*}\\) Without further assumptions, we cannot show that \\(\\theta^{k}\\) converges to a limit, and even \\(\\theta^{k}\\) does converge to a limit, we cannot guarantee that that limit is not a saddle point or even a local maximum. Nevertheless, people commonly use the argument that \\(\\theta^{k}\\) usually converges and that it is unlikely that the</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#convergence-of-gd","title":"Convergence of GD","text":"<p>Theorem) Assume \\(f: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}\\) is differentiable, \\(\\nabla f\\) is \\(L\\)-Lipschitz continuous, and \\(\\inf _{\\theta \\in \\mathbb{R}^{p}} f(\\theta)&gt;-\\infty\\). Then</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha \\nabla f\\left(\\theta^{k}\\right) \\] <p>with \\(\\alpha \\in\\left(0, \\frac{2}{L}\\right)\\) satisfies \\(\\nabla f\\left(\\theta^{k}\\right) \\rightarrow 0\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#lipschitz-gradient-lemma","title":"Lipschitz gradient lemma","text":"<p>We say \\(\\nabla f: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}^{p}\\) is \\(L\\)-Lipschitz if</p> \\[ \\|\\nabla f(x)-\\nabla f(y)\\| \\leq L\\|x-y\\| \\quad \\forall x, y \\in \\mathbb{R}^{p} . \\] <p>Roughly, this means \\(\\nabla f\\) does not change rapidly. As a consequence, we can trust the first-order Taylor expansion on a non-infinitesimal neighborhood.</p> <p>Lemma) Let \\(f: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}\\) be differentiable and \\(\\nabla f: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}^{p}\\) be L-Lipschitz. Then</p> \\[ f(\\theta+\\delta) \\leq f(\\theta)+\\nabla f(\\theta)^{\\top} \\delta+\\frac{L}{2}\\|\\delta\\|^{2} \\quad \\forall \\theta, \\delta \\in \\mathbb{R}^{p} \\] <p>\\(f(\\theta)+\\nabla f(\\theta)^{\\top} \\delta-\\frac{L}{2}\\|\\delta\\|^{2} \\leq f(\\theta+\\delta)\\) is also true, but we do not need this other direction. Together the inequalities imply</p> \\[ \\left|f(\\theta+\\delta)-\\left(f(\\theta)+\\nabla f(\\theta)^{\\top} \\delta\\right)\\right| \\leq \\frac{L}{2}\\|\\delta\\|^{2} \\quad \\forall \\theta, \\delta \\in \\mathbb{R}^{p} \\] <p>(I don't think this proof is important enough to cover in class, but I provide it here for completeness.)</p> <p>Proof) Define \\(g: \\mathbb{R} \\rightarrow \\mathbb{R}\\) as \\(g(t)=f(\\theta+t \\delta)\\). Then \\(g\\) is differentiable and</p> \\[ g^{\\prime}(t)=\\nabla f(\\theta+t \\delta)^{\\top} \\delta \\] <p>Note \\(g^{\\prime}\\) is \\(\\left(L\\|\\delta\\|^{2}\\right)\\)-Lipschitz continuous since</p> \\[ \\begin{gathered} \\left|g^{\\prime}\\left(t_{1}\\right)-g^{\\prime}\\left(t_{0}\\right)\\right|=\\left|\\left(\\nabla f\\left(\\theta+t_{1} \\delta\\right)-\\nabla f\\left(\\theta+t_{0} \\delta\\right)\\right)^{\\top} \\delta\\right| \\\\ \\leq\\left\\|\\nabla f\\left(\\theta+t_{1} \\delta\\right)-\\nabla f\\left(\\theta+t_{0} \\delta\\right)\\right\\|\\| \\| \\delta \\| \\\\ \\leq L\\left\\|t_{1} \\delta-t_{0} \\delta\\right\\|\\|\\delta\\| \\\\ =L\\|\\delta\\|^{2}\\left|t_{1}-t_{0}\\right| \\end{gathered} \\] <p>Finally, we conclude with</p> \\[ \\begin{gathered} f(\\theta+\\delta)=g(1)=g(0)+\\int_{0}^{1} g^{\\prime}(t) \\mathrm{d} t \\\\ \\leq f(\\theta)+\\int_{0}^{1}\\left(g^{\\prime}(0)+L\\|\\delta\\|^{2} t\\right) \\mathrm{d} t \\\\ =f(\\theta)+\\nabla f(\\theta)^{\\top} \\delta+\\frac{L}{2}\\|\\delta\\|^{2} \\end{gathered} \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#summability-lemma","title":"Summability Lemma","text":"<p>Lemma) Let \\(V^{0}, V^{1}, \\ldots \\in \\mathbb{R}\\) and \\(S^{0}, S^{1}, \\ldots \\in \\mathbb{R}\\) be nonnegative sequences satisfying</p> \\[ V^{k+1} \\leq V^{k}-S^{k} \\] <p>for \\(k=0,1,2, \\ldots\\) Then \\(S^{k} \\rightarrow 0\\). Key idea. \\(S^{k}\\) measures progress (decrease) made in iteration \\(k\\). Since \\(V^{k} \\geq 0, V^{k}\\) cannot decrease forever, so the progress (magnitude of \\(S^{k}\\) ) must diminish to 0. Proof) Sum the inequality from \\(i=0\\) to \\(k\\)</p> \\[ V^{k+1}+\\sum_{i=0}^{k} S^{i} \\leq V^{0} \\] <p>Let \\(k \\rightarrow \\infty\\)</p> \\[ \\sum_{i=0}^{\\infty} S^{i} \\leq V^{0}-\\lim _{k \\rightarrow \\infty} V^{k} \\leq V^{0} \\] <p>Since \\(\\sum_{i=0}^{\\infty} S^{i}&lt;\\infty, S^{i} \\rightarrow 0\\)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#convergence-of-gd-proof","title":"Convergence of GD: Proof","text":"<p>Theorem) Under the assumptions, if \\(\\theta^{k+1}=\\theta^{k}-\\alpha \\nabla f\\left(\\theta^{k}\\right)\\) and \\(\\alpha \\in\\left(0, \\frac{2}{L}\\right)\\), then \\(\\nabla f\\left(\\theta^{k}\\right) \\rightarrow 0\\). Proof) Use Lipschitz gradient lemma with \\(\\theta=\\theta^{k}\\) and \\(\\delta=-\\alpha \\nabla f\\left(\\theta^{k}\\right)\\) to get</p> \\[ f\\left(\\theta^{k+1}\\right) \\leq f\\left(\\theta^{k}\\right)-\\alpha\\left(1-\\frac{\\alpha L}{2}\\right)\\left\\|\\nabla f\\left(\\theta^{k}\\right)\\right\\|^{2} \\] <p>and</p> \\[ \\begin{array}{r} \\left(f\\left(\\theta^{k+1}\\right)-\\inf _{\\theta} f(\\theta)\\right) \\leq\\left(f\\left(\\theta^{k}\\right)-\\inf _{\\theta} f(\\theta)\\right)-\\alpha\\left(1-\\frac{\\alpha L}{2}\\right)\\left\\|\\nabla f\\left(\\theta^{k}\\right)\\right\\|^{2} \\\\ \\geq 0 \\end{array} \\] <p>By the summability lemma, \\(\\left\\|\\nabla f\\left(\\theta^{k}\\right)\\right\\|^{2} \\rightarrow 0\\) and thus \\(\\nabla f\\left(\\theta^{k}\\right) \\rightarrow 0\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#purpose-of-gd-convergence-analysis","title":"Purpose of GD convergence analysis","text":"<p>In deep learning, the condition that \\(\\nabla f\\) is \\(L\\)-Lipschitz is usually not true*.</p> <p>Rather, the purpose of these mathematical analyses is to obtain qualitative insights; this convergence proof and the exercises of hw1 are meant to provide you with intuition on the training dynamics of GD and SGD.</p> <p>Because analyzing deep learning systems as is rigorously is usually difficult, people usually</p> <ul> <li>analyze modified (simplified) setups rigorously or</li> <li>analyze the full setup heuristically.</li> </ul> <p>In both cases, the goal is to obtain qualitative insights, rather than theoretical guarantees.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#finite-sum-optimization-problems","title":"Finite-sum optimization problems","text":"<p>A finite-sum optimization problem has the structure</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} f_{i}(\\theta):=F(\\theta) \\] <p>Finite-sum is ubiquitous in ML. \\(N\\) usually corresponds to the number of data points.</p> <p>Using GD</p> \\[ \\theta^{k+1}=\\theta^{k}-\\frac{\\alpha_{k}}{N} \\sum_{i=1}^{N} \\nabla f_{i}\\left(\\theta^{k}\\right) \\] <p>is impractical when \\(N\\) is large since \\(\\frac{1}{N} \\sum_{i=1}^{N} \\cdot\\) takes too long to compute.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#finite-sum-cong-expectation","title":"Finite-sum \\(\\cong\\) Expectation","text":"<p>Although the finite-sum optimization problem has no inherent randomness, we can reformulate this problem with randomness:</p> \\[ \\operatorname{minimize}_{\\theta \\in \\mathbb{R}^{p}} \\quad \\mathbb{E}_{I}\\left[f_{I}(\\theta)\\right] \\] <p>where \\(I \\sim\\) Uniform \\(\\{1, \\ldots, N\\}\\). To see the equivalence,</p> \\[ \\mathbb{E}_{I}\\left[f_{I}(\\theta)\\right]=\\sum_{i=1}^{N} f_{i}(\\theta) \\mathbb{P}(I=i)=\\frac{1}{N} \\sum_{i=1}^{N} f_{i}(\\theta)=F(\\theta) \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#stochastic-gradient-descent-sgd","title":"Stochastic gradient descent (SGD)","text":"<p>Stochastic gradient descent (SGD)</p> \\[ \\begin{gathered} i(k) \\sim \\operatorname{Uniform}\\{1, \\ldots, N\\} \\\\ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\nabla f_{i(k)}\\left(\\theta^{k}\\right) \\end{gathered} \\] <p>for \\(k=0,1, \\ldots\\), where \\(\\theta^{0} \\in \\mathbb{R}^{p}\\) is the initial point and \\(\\alpha_{k}&gt;0\\) is the learning rate. \\(\\nabla f_{i(k)}\\left(\\theta^{k}\\right)\\) is a stochastic gradient of \\(F\\) at \\(\\theta^{k}\\), i.e.,</p> \\[ \\mathbb{E}\\left[\\nabla f_{i(k)}\\left(\\theta^{k}\\right)\\right]=\\nabla \\mathbb{E}\\left[f_{i(k)}\\left(\\theta^{k}\\right)\\right]=\\nabla F\\left(\\theta^{k}\\right) \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#gd-vs-sgd","title":"GD vs. SGD","text":"<p>GD uses all indices \\(i=1, \\ldots, N\\) every iteration</p> \\[ \\theta^{k+1}=\\theta^{k}-\\frac{\\alpha_{k}}{N} \\sum_{i=1}^{N} \\nabla f_{i}\\left(\\theta^{k}\\right) \\] <p>SGD uses only a single random index \\(i(k)\\) every iteration</p> \\[ \\begin{gathered} i(k) \\sim \\text { Uniform }\\{1, \\ldots, N\\} \\\\ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\nabla f_{i(k)}\\left(\\theta^{k}\\right) \\end{gathered} \\] <p>When size of the data \\(N\\) is large, SGD is often more effective than GD.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#digression-randomized-algorithms","title":"Digression: Randomized algorithms","text":"<p>A randomized algorithm utilizes artificial randomness to solve an otherwise deterministic problem.</p> <p>There are problems* for which a randomized algorithm is faster than the best known deterministic algorithm.</p> <p>The most famous example of this is SGD in deep learning.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#why-does-sgd-converge","title":"Why does SGD converge?","text":"<p>Plug \\(\\theta^{k+1}\\) into Taylor expansion of \\(F\\) about \\(\\theta^{k}\\) :</p> \\[ F\\left(\\theta^{k+1}\\right)=F\\left(\\theta^{k}\\right)-\\alpha_{k} \\nabla F\\left(\\theta^{k}\\right)^{\\top} \\nabla f_{i(k)}\\left(\\theta^{k}\\right)+\\mathcal{O}\\left(\\alpha_{k}^{2}\\right) \\] <p>Take expectation on both sides:</p> \\[ \\mathbb{E}_{k}\\left[F\\left(\\theta^{k+1}\\right)\\right]=F\\left(\\theta^{k}\\right)-\\alpha_{k}\\left\\|\\nabla F\\left(\\theta^{k}\\right)\\right\\|^{2}+\\mathcal{O}\\left(\\alpha_{k}^{2}\\right) \\] <p>( \\(\\mathbb{E}_{k}\\) is expectation conditioned on \\(\\theta^{k}\\) ) \\(-\\nabla f_{i(k)}\\left(\\theta^{k}\\right)\\) is descent direction in expectation. For small (cautious) \\(\\alpha_{k}\\), SGD step reduces function value in expectation.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#variants-of-sgd-for-finite-sum-problems","title":"Variants of SGD for finite-sum problems","text":"<p>Consider</p> \\[ \\operatorname{minimize}_{\\theta \\in \\mathbb{R}^{p}} \\frac{1}{N} \\sum_{i=1}^{N} f_{i}(\\theta) \\] <p>SGD can be generalized to</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} g^{k} \\] <p>where \\(g^{k}\\) is a stochastic gradient. The choice \\(g^{k}=\\nabla f_{i(k)}\\left(\\theta^{k}\\right)\\) is just one option.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#sampling-with-replacement-lemma","title":"Sampling with replacement lemma","text":"<p>Lemma) Let \\(X_{1}, \\ldots, X_{N} \\in \\mathbb{R}^{p}\\) be given (non-random) vectors. Let \\(\\frac{1}{N} \\sum_{i=1}^{N} X_{i}=\\mu\\). Let \\(i(1), \\ldots, i(B) \\subseteq\\{1, \\ldots, N\\}\\) be random indices. Then</p> \\[ \\mathbb{E} \\frac{1}{B} \\sum_{b=1}^{B} X_{i(b)}=\\mu \\] <p>Proof)</p> \\[ \\mathbb{E} \\frac{1}{B} \\sum_{b=1}^{B} X_{i(b)}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{E} X_{i(b)}=\\frac{1}{B} \\sum_{b=1}^{B} \\mu=\\mu \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#minibatch-sgd-with-replacement","title":"Minibatch SGD with replacement","text":"<p>Minibatch SGD with replacement</p> \\[ \\begin{gathered} i(k, 1), \\ldots, i(k, B) \\sim \\text { Uniform }\\{1, \\ldots, N\\} \\\\ \\theta^{k+1}=\\theta^{k}-\\frac{\\alpha_{k}}{B} \\sum_{b=1}^{B} \\nabla f_{i(k, b)}\\left(\\theta^{k}\\right) \\end{gathered} \\] <p>To clarify, we sample \\(B\\) out of \\(N\\) indices with replacement, i.e., the same index can be sampled multiple times.</p> <p>By previous lemma, \\(\\frac{1}{B} \\sum_{b=1}^{B} \\nabla f_{i(k, b)}\\left(\\theta^{k}\\right)\\) is a stochastic gradient of \\(F\\) at \\(\\theta^{k}\\)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#random-permutations","title":"Random permutations","text":"<p>A permutation \\(\\sigma\\) is a list of length \\(N\\) containing integers \\(1, \\ldots, N\\) all exactly once. We write \\(S_{n}\\) for the set of permutations of length \\(N\\).</p> <p>There are \\(N\\) ! possible permutations of length \\(N\\).</p> <p>A random permutation is a permutation chosen randomly with uniform probability; each of the \\(N!\\) permutations are chosen with probability \\(\\frac{1}{N}\\) :</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#digression-0-based-indexing-and-random-permutations-in-python","title":"Digression: 0-based indexing and random permutations in Python","text":"<p>In Python, generate random permutations with</p> <pre><code>np.random.permutation(np.arange(N))\n</code></pre> <p>In Python, array indices start at 0, although in math and in human language, counting starts at 1 . We use permutations containing \\(0,1, \\ldots, N-1\\) in our Python code.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#sampling-without-replacement-lemma","title":"Sampling without replacement lemma","text":"<p>Lemma) Let \\(X_{1}, \\ldots, X_{N} \\in \\mathbb{R}^{p}\\) be given (non-random) vectors. Let \\(\\frac{1}{N} \\sum_{i=1}^{N} X_{i}=\\mu\\). Let \\(\\sigma\\) be a random permutation. Then</p> \\[ \\mathbb{E} \\frac{1}{B} \\sum_{b=1}^{B} X_{\\sigma(b)}=\\mu \\] <p>Proof)</p> \\[ \\mathbb{E} \\frac{1}{B} \\sum_{b=1}^{B} X_{\\sigma(b)}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{E} X_{\\sigma(b)}=\\frac{1}{B} \\sum_{b=1}^{B} \\mu=\\mu \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#minibatch-sgd-without-replacement","title":"Minibatch SGD without replacement","text":"<p>Minibatch SGD without replacement</p> \\[ \\begin{gathered} \\sigma^{k} \\sim \\operatorname{permutation}(N) \\\\ \\theta^{k+1}=\\theta^{k}-\\frac{\\alpha_{k}}{B} \\sum_{b=1}^{B} \\nabla f_{\\sigma^{k}(b)}\\left(\\theta^{k}\\right) \\end{gathered} \\] <p>We assume \\(B \\leq N\\). To clarify, we sample \\(B\\) out of \\(N\\) indices without replacement, i.e., the same index cannot be sampled multiple times.</p> <p>By previous lemma, \\(\\frac{1}{B} \\sum_{b=1}^{B} \\nabla f_{\\sigma^{k}(b)}\\left(\\theta^{k}\\right)\\) is a stochastic gradient of \\(F\\) at \\(\\theta^{k}\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#how-to-choose-batch-size-b","title":"How to choose batch size \\(B\\) ?","text":"<p>Note \\(B=1\\) minibatch SGD becomes SGD.</p> <p>Mathematically (measuring performance per iteration)</p> <ul> <li>Use large batch is when noise/randomness is large.</li> <li>Use small batch is when noise/randomness is small.</li> </ul> <p>Practically (measuring performance per unit time)</p> <ul> <li>Large batch allows more efficient computation on GPUs.</li> <li>Often best to increase batch size up to the GPU memory limit.</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#gd-and-sgd-without-differentiability","title":"GD and SGD without differentiability","text":"<p>In DL, SGD is applied to nice continuous but non-differentiable* functions that are differentiable almost everywhere.</p> <p>In this case, if we choose \\(\\theta^{0} \\in \\mathbb{R}^{n}\\) randomly and run</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\nabla f\\left(\\theta^{k}\\right) \\] <p>the algorithm is usually well-defined, i.e., \\(\\theta^{k}\\) never hits a point of non-differentiability.</p> <p>With a proof or not, GD and SGD are applied to non-differentiable minimization in ML. The absence of differentiability \\({ }^{*}\\) does not seem to cause serious problems.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#cyclic-sgd","title":"Cyclic SGD","text":"<p>Consider the sequence of indices</p> \\[ \\{\\bmod (k, N)+1\\}_{k=0,1, \\ldots}=1,2, \\ldots, N, 1,2, \\ldots, N, \\ldots \\] <p>Here, \\(\\bmod (k, N)\\) is the remainder of \\(k\\) when divided by \\(N\\). In Python, this is written with \\(k \\% N\\).</p> <p>Cyclic SGD:</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{\\mathbf{k}} \\nabla f_{\\bmod (k, N)+1}\\left(\\theta^{k}\\right) \\] <p>To clarify, this samples the indices in a (deterministic) cyclic order.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#cyclic-mini-batch-sgd","title":"Cyclic (mini-batch) SGD","text":"<p>Strictly speaking, cyclic SGD is not an instance of SGD as unbiased estimation property lost.</p> <p>Advantage:</p> <ul> <li>Uses all indices (data) every \\(N\\) iterations.</li> </ul> <p>Disadvantage:</p> <ul> <li>Worse than SGD in some cases, theoretically and empirically.</li> <li>In DL, neural networks can learn to anticipate cyclic order.</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#shuffled-cyclic-sgd","title":"Shuffled Cyclic SGD","text":"<p>Shuffled Cyclic SGD:</p> \\[ \\theta^{k+1}=\\theta^{k}-\\left.\\alpha_{k} \\nabla f\\right|_{\\left.\\left.\\right|^{\\frac{k}{N}}\\right|_{(\\bmod (k, N)+1)}}\\left(\\theta^{k}\\right) \\] <p>where \\(\\sigma^{0}, \\sigma^{1}, \\ldots\\) is a sequence of random permutations, i.e., we shuffle the order every cycle. Again, strictly speaking, shuffled cyclic SGD is not an instance of SGD as unbiased estimation property lost.</p> <p>Advantages :</p> <ul> <li>Uses all indices (data) every \\(N\\) iterations.</li> <li>Neural network cannot learn to anticipate data order.</li> <li>Empirically best performance.</li> </ul> <p>Disadvantages:</p> <ul> <li>Theory not as strong as regular SGD.</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#which-variant-of-sgd-to-use","title":"Which variant of SGD to use?","text":"<p>Theoretical comparison of SGD variants:</p> <ul> <li>Not that easy.</li> <li>Result does not strongly correlate with practical performance in DL.</li> </ul> <p>In DL, the most common choice is</p> <ul> <li>shuffled cyclic minibatch SGD (without replacement) and</li> <li>batchsize \\(B\\) is as large as possible within the GPU memory limit.</li> </ul> <p>One can generally consider this to be the default option.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#epoch-in-finite-sum-optimization-and-machine-learning-training","title":"Epoch in finite-sum optimization and machine learning training","text":"<p>An epoch is loosely defined as the unit of optimization or training progress of processing all indices or data once.</p> <ul> <li>1 iteration of GD constitutes an epoch.</li> <li>\\(N\\) iterations of SGD, cyclic SGD, or shuffled cyclic SGD constitute an epoch.</li> <li>\\(N / B\\) iterations of minibatch SGD constitute an epoch.</li> </ul> <p>Epoch is often a convenient unit for counting iterations compared to directly counting the iteration number.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#sgd-with-general-expectation","title":"SGD with general expectation","text":"<p>Consider an optimization problem with its objective defined with a general expectation</p> \\[ \\operatorname{minimize}_{\\theta \\in \\mathbb{R}^{p}} \\quad \\mathbb{E}_{\\omega}\\left[f_{\\omega}(\\theta)\\right]:=F(\\theta) \\] <p>Here, \\(\\omega\\) is a random variable. We will encounter these expectations (non-finite sum) when we talk about generative models.</p> <p>For this setup, the SGD algorithm is</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\nabla f_{\\omega^{k}}\\left(\\theta^{k}\\right) \\] <p>where \\(\\omega^{0}, \\omega^{1}, \\ldots\\) are IID random samples of \\(\\omega\\). If \\(\\nabla_{\\theta} \\mathbb{E}_{\\omega}\\left[f_{\\omega}(\\theta)\\right]=\\mathbb{E}_{\\omega}\\left[\\nabla_{\\theta} f_{\\omega}(\\theta)\\right]\\), then \\(\\nabla f_{\\omega^{k}}\\left(\\theta^{k}\\right)\\) is a stochastic gradient of \\(F(\\theta)\\) at \\(\\theta^{k}\\). (Make sure you understand why the previous SGD for the finite-sum setup is a special case of this.)</p> <p>GD for this setup is</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\mathbb{E}_{\\omega}\\left[\\nabla_{\\theta} f_{\\omega}\\left(\\theta^{k}\\right)\\right] \\] <p>However, if the expectation is difficult to compute GD is impractical and SGD is preferred.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#chapter-2-shallow-neural-networks-to-multilayer-perceptrons","title":"Chapter 2:  Shallow Neural Networks to Multilayer Perceptrons","text":"<p>Mathematical Foundations of Deep Neural Networks</p> <p>Spring 2024 Department of Mathematical Sciences Ernest K. Ryu Seoul National University</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#supervised-learning-setup","title":"Supervised learning setup","text":"<p>We have data \\(X_{1}, \\ldots, X_{N} \\in X\\) and corresponding labels \\(Y_{1}, \\ldots, Y_{N} \\in \\mathcal{Y}\\).</p> <p>Example) \\(X_{i}\\) is the \\(i\\) th email and \\(Y_{i} \\in\\{-1,+1\\}\\) denotes whether \\(X_{i}\\) is a spam email. Example) \\(X_{i}\\) is the \\(i\\) th image and \\(Y_{i} \\in\\{0, \\ldots, 9\\}\\) denotes handwritten digit.</p> <p>Assume there is a true unknown function</p> \\[ f_{\\star}: x \\rightarrow y \\] <p>mapping data to its label. In particular, \\(Y_{i}=f_{\\star}\\left(X_{i}\\right)\\) for \\(i=1, \\ldots, N\\). </p> <p>The goal of supervised learning is to use \\(X_{1}, \\ldots, X_{N}\\) and \\(Y_{1}, \\ldots, Y_{N}\\) to find \\(f \\approx f_{\\star}\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#formulating-the-right-objective","title":"Formulating the right objective","text":"<p>The goal of \"finding \\(f \\approx f_{\\star}\\) \" must be further quantified.</p> <p>Assume a loss function such that \\(\\ell\\left(y_{1}, y_{2}\\right)=0\\) if \\(y_{1}=y_{2}\\) and \\(\\ell\\left(y_{1}, y_{2}\\right)&gt;0\\) if \\(y_{1} \\neq y_{2}\\).</p> <p>Attempt 1)</p> \\[ \\underset{f}{\\operatorname{minimize}} \\sup _{x \\in \\mathcal{X}} \\ell\\left(f(x), f_{\\star}(x)\\right) \\] <p>Problems:</p> <ul> <li>There is a trivial solution \\(f=f_{\\star}\\).</li> <li>Minimization over all functions \\(f\\) is in general algorithmically intractable \\({ }^{1}\\). How would one represent a \\(f\\) on a computer?</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#formulating-the-right-objective_1","title":"Formulating the right objective","text":"<p>Attempt 2) Restrict search to a class of parametrized functions \\(f_{\\theta}(x)\\) where \\(\\theta \\in \\Theta \\subseteq \\mathbb{R}^{p}\\), i.e., only consider \\(f \\in\\left\\{f_{\\theta} \\mid \\theta \\in \\Theta\\right\\}\\) where \\(\\Theta \\subseteq \\mathbb{R}^{p}\\). Then solve</p> \\[ \\operatorname{minimize}_{f \\in\\left\\{f_{\\theta} \\mid \\theta \\in \\Theta\\right\\}} \\sup _{x \\in \\mathcal{X}} \\ell\\left(f(x), f_{\\star}(x)\\right) \\] <p>which is equivalent to</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\sup _{x \\in \\mathcal{X}} \\ell\\left(f_{\\theta}(x), f_{\\star}(x)\\right) \\] <p>Problems:</p> <ul> <li>The supremum \\(\\sup _{x \\in \\mathcal{X}}\\) is computationally inconvenient to deal with.</li> <li>Objective is too pessimistic. We do not need to do well all the time, we just need to do well on average.</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#formulating-the-right-objective_2","title":"Formulating the right objective","text":"<p>Attempt 3) Take a finite sample \\({ }^{*} X_{1}, \\ldots, X_{N} \\in \\mathcal{X}\\) and corresponding labels \\(Y_{1}, \\ldots, Y_{N} \\in \\mathcal{Y}\\). Then solve</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(f_{\\theta}\\left(X_{i}\\right), f_{\\star}\\left(X_{i}\\right)\\right) \\] <p>which is equivalent to</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(f_{\\theta}\\left(X_{i}\\right), Y_{i}\\right) \\] <p>This is the standard form of the optimization problem (except regularizers) we consider in the supervised learning. We will talk about regularizers later.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#aside-minimum-vs-infimum","title":"Aside: Minimum vs. infimum","text":"<p>We clarify terminology.</p> <ul> <li>\"Minimize\": Used to specify an optimization problem.</li> <li>\"Minimizer\": A solution to a minimization problem.</li> <li>\"Minimum\": Used to specify the smallest objective value and asserts a minimizer exists.</li> <li>\"Infimum\": Used to specify the limiting smallest objective value, but a minimizer may not exist.</li> </ul> <p>Analogous definitions with \"maximize\", \"maximizer\", \"maximum\", and \"supremum\"</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#training-is-optimization","title":"Training is optimization","text":"<p>In machine learning, the anthropomorphized word \"training\" refers to solving an optimization problem such as</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(f_{\\theta}\\left(X_{i}\\right), Y_{i}\\right) \\] <p>In most cases, SGD or variants of SGD are used.</p> <p>We call \\(f_{\\theta}\\) the machine learning model or the neural network.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#least-squares-regression","title":"Least-squares regression","text":"<p>\\(\\ln \\mathrm{LS}, X=\\mathbb{R}^{p}, \\mathcal{Y}=\\mathbb{R}, \\Theta=\\mathbb{R}^{p}, f_{\\theta}(x)=x^{\\top} \\theta\\), and \\(\\ell\\left(y_{1}, y_{2}\\right)=\\frac{1}{2}\\left(y_{1}-y_{2}\\right)^{2}\\). So we solve</p> \\[ \\operatorname{minimize}_{\\theta \\in \\mathbb{R}^{p}} \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{2}\\left(f_{\\theta}\\left(X_{i}\\right)-Y_{i}\\right)^{2}=\\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{2}\\left(X_{i}^{\\top} \\theta-Y_{i}\\right)^{2}=\\frac{1}{2 N}\\|X \\theta-Y\\|^{2} \\] <p>where \\(X=\\left[\\begin{array}{c}X_{1}^{\\top} \\\\ \\vdots \\\\ X_{N}^{\\top}\\end{array}\\right]\\) and \\(Y=\\left[\\begin{array}{c}Y_{1} \\\\ \\vdots \\\\ Y_{N}\\end{array}\\right]\\).</p> <p>The model \\(f_{\\theta}(x)=x^{\\top} \\theta\\) is a shallow neural network. (The terminology will makes sense when contrasted with deep neural networks.)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#binary-classification-and-linear-separability","title":"Binary classification and linear separability","text":"<p>In binary classification, we have \\(X=\\mathbb{R}^{p}\\) and \\(\\mathcal{Y}=\\{-1,+1\\}\\).</p> <p>The data is linearly separable if there is a hyperplane defined by ( \\(a_{\\text {true }}, b_{\\text {true }}\\) ) such that</p> \\[ y=\\left\\{\\begin{array}{cl} 1 &amp; \\text { if } a_{\\text {true }}^{\\top} x+b_{\\text {true }}&gt;0 \\\\ -1 &amp; \\text { otherwis. } \\end{array}\\right. \\] <p></p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#linear-classification","title":"Linear classification","text":"<p>Consider linear (affine) models</p> \\[ f_{a, b}(x)= \\begin{cases}+1 &amp; \\text { if } a^{\\top} x+b&gt;0 \\\\ -1 &amp; \\text { otherwise }\\end{cases} \\] <p>Consider the loss function</p> \\[ \\ell\\left(y_{1}, y_{2}\\right)=\\frac{1}{2}\\left|1-y_{1} y_{2}\\right|= \\begin{cases}0 &amp; \\text { if } y_{1}=y_{2} \\\\ 1 &amp; \\text { if } y_{1} \\neq y_{2}\\end{cases} \\] <p>The optimization problem</p> \\[ \\operatorname{minimize}_{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(f_{a, b}\\left(X_{i}\\right), Y_{i}\\right) \\] <p>has a solution with optimal value 0 when the data is linearly separable. Problem: Optimization problem is discontinuous and thus cannot be solved with SGD.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#relaxing-into-continuous-formulation","title":"Relaxing into continuous formulation","text":"<p>Even if the underlying function or phenomenon to approximate is discontinuous, the model needs to be continuous* in its parameters. The loss function also needs to be continuous. (The prediction need not be continuous.)</p> <p>We consider a relaxation, is a continuous proxy of the discontinuous thing. Specifically, consider</p> \\[ f_{a, b}(x)=a^{\\top} x+b \\] <p>Once trained, \\(f_{a, b}(x)&gt;0\\) means the neural network is predicting \\(y=+1\\) to be \"more likely\", and \\(f_{a, b}(x)&lt;0\\) means the neural network is predicting \\(y=-1\\) to be \"more likely\".</p> <p>Therefore, we train the model to satisfy</p> \\[ Y_{i} f_{a, b}\\left(X_{i}\\right)&gt;0 \\text { for } i=1, \\ldots, N . \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#relaxing-into-continuous-formulation_1","title":"Relaxing into continuous formulation","text":"<p>Problem with strict inequality \\(Y_{i} f_{a, b}\\left(X_{i}\\right)&gt;0\\) :</p> <ul> <li>Strict inequality has numerical problems with round-off error.</li> <li>The magnitude \\(\\left|f_{a, b}(x)\\right|\\) can be viewed as the confidence* of the prediction, but having a small positive value for \\(Y_{i} f_{a, b}\\left(X_{i}\\right)\\) indicates small confidence of the neural network.</li> </ul> <p>We modify our model's desired goal to be \\(Y_{i} f_{a, b}\\left(X_{i}\\right) \\geq 1\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#support-vector-machine-svm","title":"Support vector machine (SVM)","text":"<p>To train the neural network to satisfy</p> \\[ 0 \\geq 1-Y_{i} f_{a, b}\\left(X_{i}\\right) \\text { for } i=1, \\ldots, N . \\] <p>we minimize the excess positive component of the RHS</p> \\[ \\operatorname{minimize}_{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}} \\frac{1}{N} \\sum_{i=1}^{N} \\max \\left\\{0,1-Y_{i} f_{a, b}\\left(X_{i}\\right)\\right\\} \\] <p>which is equivalent to</p> \\[ \\operatorname{minimize}_{a \\in \\mathbb{R}^{P}, b \\in \\mathbb{R}} \\frac{1}{N} \\sum_{i=1}^{N} \\max \\left\\{0,1-Y_{i}\\left(a^{\\top} X_{i}+b\\right)\\right\\} \\] <p>If the optimal value is 0 , then the data is linearly separable.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#support-vector-machine-svm_1","title":"Support vector machine (SVM)","text":"<p>This formulation is called the support vector machine (SVM)*</p> \\[ \\operatorname{minimize}_{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}} \\frac{1}{N} \\sum_{i=1}^{N} \\max \\left\\{0,1-Y_{i}\\left(a^{\\top} X_{i}+b\\right)\\right\\} \\] <p>It is also common to add a regularizer</p> \\[ \\operatorname{minimize}_{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}} \\frac{1}{N} \\sum_{i=1}^{N} \\max \\left\\{0,1-Y_{i}\\left(a^{\\top} X_{i}+b\\right)\\right\\}+\\frac{\\lambda}{2}\\|a\\|^{2} \\] <p>We will talk about regularizers later.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#prediction-with-svm","title":"Prediction with SVM","text":"<p>Once the SVM is trained, make predictions with</p> \\[ \\operatorname{sign}\\left(f_{a, b}(x)\\right)=\\operatorname{sign}\\left(a^{\\top} x+b\\right) \\] <p>when \\(f_{a, b}(x)=0\\), we assign \\(\\operatorname{sign}\\left(f_{a, b}(x)\\right)\\) arbitrarily.</p> <p>Note that the prediction is discontinuous, but predictions are in \\(\\{-1,+1\\}\\) so it must be discontinuous.</p> <p>If \\(\\sum_{i=1}^{N} \\max \\left\\{0,1-Y_{i} f_{a, b}\\left(X_{i}\\right)\\right\\}=0\\), then \\(\\operatorname{sign}\\left(f_{a, b}\\left(X_{i}\\right)\\right)=Y_{i}\\) for \\(i=1, \\ldots, N\\), i.e., the neural network predicts the known labels perfectly. (Make sure you understand this.) Of course, it is a priori not clear how accurate the prediction will be for new unseen data.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#svm-is-a-relaxation","title":"SVM is a relaxation","text":"<p>Directly minimizing the prediction error on the data is</p> \\[ \\operatorname{minimize}_{a \\in \\mathbb{R}^{P}, b \\in \\mathbb{R}} \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{2}\\left|1-Y_{i} \\operatorname{sign}\\left(f_{a, b}\\left(X_{i}\\right)\\right)\\right| \\] <p>The optimization we instead solve is</p> \\[ \\operatorname{minimize}_{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}} \\frac{1}{N} \\sum_{i=1}^{N} \\max \\left\\{0,1-Y_{i} f_{a, b}\\left(X_{i}\\right)\\right\\} \\] <p>Let the optimal values be \\(p_{1}^{\\star}\\) and \\(p_{2}^{\\star}\\). Again, SVM is of as a relaxation of the first. The two are not equivalent. (An equivalent formulation is not referred to as a relaxation.)</p> <ul> <li>It is possible to show \\({ }^{\\star}\\) that \\(p_{1}^{\\star}=0\\) if and only if \\(p_{2}^{\\star}=0\\).</li> <li>If \\(p_{1}^{\\star}&gt;0\\) and \\(p_{2}^{\\star}&gt;0\\), a solution to the first problem need not correspond to a solution to the second problem, i.e., there solutions may be completely different.</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#relaxed-supervised-learning-setup","title":"Relaxed supervised learning setup","text":"<p>We relax the supervised learning setup to predict probabilities, rather than make point predictions*. So, labels are generated based on data, perhaps randomly. Consider data \\(X_{1}, \\ldots, X_{N} \\in \\mathcal{X}\\) and labels \\(Y_{1}, \\ldots, Y_{N} \\in \\mathcal{Y}\\). Assume there exists a function</p> \\[ f_{\\star}: \\mathcal{X} \\rightarrow \\mathcal{P}(\\mathcal{Y}) \\] <p>where \\(\\mathcal{P}(\\mathcal{Y})\\) denotes the set of probability distributions on \\(\\mathcal{Y}\\). Assume the generation of \\(Y_{i}\\) given \\(X_{i}\\) is independent of \\(Y_{j}\\) and \\(X_{j}\\) for \\(j \\neq i\\). </p> <p>Example) \\(f(X)=\\left[\\begin{array}{l}0.8 \\\\ 0.2\\end{array}\\right]\\) in dog vs. cat classifier. Example) An email saying \"Buy this thing at our store!\" may be spam to some people, but it may not be spam to others.</p> <p>The relaxed SL setup is more general and further realistic.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#kl-divergence","title":"KL-divergence","text":"<p>Let \\(p, q \\in \\mathbb{R}^{n}\\) represent probability masses, i.e., \\(p_{i} \\geq 0\\) for \\(i=1, \\ldots, n\\) and \\(\\sum_{i=1}^{n} p_{i}=1\\) and the same for \\(q\\). The Kullback-Leibler-divergence (KL-divergence) from \\(q\\) to \\(p\\) is</p> \\[ D_{\\mathrm{KL}}(p \\| q)=\\sum_{i=1}^{n} p_{i} \\log \\left(\\frac{p_{i}}{q_{i}}\\right)=-\\sum_{i=1}^{n} p_{i} \\log \\left(q_{i}\\right)+\\sum_{i=1}^{n} p_{i} \\log \\left(p_{i}\\right) \\] <p>Properties:</p> \\[ \\begin{array}{ll} \\quad=H(p, q) &amp; =-H(p) \\\\ \\text { cross entropy of } q &amp; =-H \\\\ \\text { relative to } p &amp; \\text { entropy of } p \\end{array} \\] <ul> <li>Not symmetric, i.e., \\(D_{\\mathrm{KL}}(p \\| q) \\neq D_{\\mathrm{KL}}(q \\| p)\\).</li> <li>\\(D_{\\mathrm{KL}}(p \\| q)&gt;0\\) if \\(p \\neq q\\) and \\(D_{\\mathrm{KL}}(p \\| q)=0\\) if \\(p=q\\).</li> <li>\\(D_{\\mathrm{KL}}(p \\| q)=\\infty\\) is possible. (Further detail on the next slide.)</li> </ul> <p>Often used as a \"distance\" between \\(p\\) and \\(q\\) despite not being a metric.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#kl-divergence_1","title":"KL-divergence","text":"\\[ D_{\\mathrm{KL}}(p \\| q)=\\sum_{i=1}^{n} p_{i} \\log \\left(\\frac{p_{i}}{q_{i}}\\right) \\] <p>Clarification: Use the convention</p> <ul> <li>\\(0 \\log \\left(\\frac{0}{0}\\right)=0\\left(\\right.\\) when \\(\\left.p_{i}=q_{i}=0\\right)\\)</li> <li>\\(0 \\log \\left(\\frac{0}{q_{i}}\\right)=0\\) if \\(q_{i}&gt;0\\)</li> <li>\\(p_{i} \\log \\left(\\frac{p_{i}}{0}\\right)=\\infty\\) if \\(p_{i}&gt;0\\)</li> </ul> <p>Probabilistic interpretation:</p> \\[ D_{\\mathrm{KL}}(p \\| q)=\\mathbb{E}_{I}\\left[\\log \\left(\\frac{p_{I}}{q_{I}}\\right)\\right] \\] <p>with the random variable \\(I\\) such that \\(\\mathbb{P}(I=i)=p_{i}\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#empirical-distribution-for-binary-classification","title":"Empirical distribution for binary classification","text":"<p>In basic binary classification, define the empirical distribution</p> \\[ \\mathcal{P}(y)= \\begin{cases}{\\left[\\begin{array}{l} 1 \\\\ 0 \\end{array}\\right]} &amp; \\text { if } y=-1 \\\\ {\\left[\\begin{array}{l} 0 \\\\ 1 \\end{array}\\right]} &amp; \\text { if } y=+1\\end{cases} \\] <p>More generally, the empirical distribution describes the data we have seen. In this context, we have only seen one label per datapoint, so our empirical distributions are one-hot vectors. (If there are multiple annotations per data point \\(x\\) and they don't agree, then the empirical distribution may not be one-hot vectors. For example, given the same email, some users may flag it as spam while others consider it useful information.)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#logistic-regression","title":"Logistic regression","text":"<p>Logistic regression (LR), is another model for binary classification:</p> <ol> <li>Use the model</li> </ol> \\[ f_{a, b}(x)=\\left[\\begin{array}{c} \\frac{1}{1+e^{a^{\\top} x+b}} \\\\ \\frac{e^{a^{\\top} x+b}}{1+e^{a^{\\top} x+b}} \\end{array}\\right]=\\left[\\begin{array}{c} \\frac{1}{1+e^{a^{\\top} x+b}} \\\\ \\frac{1}{1+e^{-\\left(a^{\\top} x+b\\right)}} \\end{array}\\right]=\\mathbb{P}(y=-1) \\] <ol> <li>Minimize KL-Divergence (or cross entropy) from the model \\(f_{a, b}\\left(X_{i}\\right)\\) output probabilities to the empirical distribution \\(\\mathcal{P}\\left(Y_{i}\\right)\\).</li> </ol> \\[ \\operatorname{minimize}_{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}} \\sum_{i=1}^{N} D_{\\mathrm{KL}}\\left(\\mathcal{P}\\left(Y_{i}\\right) \\| f_{a, b}\\left(X_{i}\\right)\\right) \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#logistic-regression_1","title":"Logistic regression","text":"<p>Note:</p> \\[ \\begin{gathered} \\operatorname{minimize}_{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}} \\sum_{i=1}^{N} D_{\\mathrm{KL}}\\left(\\mathcal{P}\\left(Y_{i}\\right) \\| f_{a, b}\\left(X_{i}\\right)\\right) \\\\ \\mathbb{\\Downarrow} \\\\ \\operatorname{minimize}_{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}} \\sum_{i=1}^{N} H\\left(\\mathcal{P}\\left(Y_{i}\\right), f_{a, b}\\left(X_{i}\\right)\\right)+(\\text { terms independent of } a, b) \\\\ \\operatorname{minimize}_{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}} \\sum_{i=1}^{N} \\log \\left(1+\\exp \\left(-Y_{i}\\left(a^{\\top} X_{i}+b\\right)\\right)\\right) \\\\ \\operatorname{minimize}_{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(Y_{i}\\left(a^{\\top} X_{i}+b\\right)\\right) \\end{gathered} \\] <p>where \\(\\ell(z)=\\log \\left(1+e^{-z}\\right)\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#point-prediction-with-logistic-regression","title":"Point prediction with logistic regression","text":"<p>When performing point prediction with \\(\\mathrm{LR}, a^{\\top} x+b&gt;0\\) means \\(\\mathbb{P}(y=+1)&gt;0.5\\) and vice versa.</p> <p>Once the LR is trained, make predictions with</p> \\[ \\operatorname{sign}\\left(a^{\\top} x+b\\right) \\] <p>when \\(a^{\\top} x+b=0\\), we assign \\(\\operatorname{sign}\\left(a^{\\top} x+b\\right)\\) arbitrarily. This is the same as SVM.</p> <p>Again, it is a priori not clear how accurate the prediction will be for new unseen data.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#svm-vs-lr","title":"SVM vs. LR","text":"<p>Both support vector machine and logistic regression can be written as</p> \\[ \\operatorname{minimize}_{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(Y_{i}\\left(a^{\\top} X_{i}+b\\right)\\right) \\] <ul> <li>SVM uses \\(\\ell(z)=\\max \\{0,1-z\\}\\). Obtained from relaxing the discontinuous prediction loss.</li> <li>LR uses \\(\\ell(z)=\\log \\left(1+e^{-z}\\right)\\). Obtained from relaxing the supervised learning setup from predicting the label to predicting the label probabilities. </li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#svm-vs-lr_1","title":"SVM vs. LR","text":"<p>SVM and LR are both \"linear\" classifiers:</p> <ul> <li>Decision boundary \\(a^{\\top} x+b=0\\) is linear.</li> <li>Model completely ignores information perpendicular to \\(a\\).</li> </ul> <p>LR naturally generalizes to multi-class classification via softmax regression. Generalizing SVM to multi-class classification is trickier and less common.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#estimation-vs-prediction","title":"Estimation vs. Prediction","text":"<p>Finding \\(f \\approx f_{\\star}\\) for unknown</p> \\[ f_{\\star}: \\mathcal{X} \\rightarrow \\mathcal{P}(\\mathcal{Y}) \\] <p>is called estimation*. When we consider a parameterized model \\(f_{\\theta}\\), finding \\(\\theta\\) is the estimation. However, estimation is usually not the end goal.</p> <p>The end goal is prediction. It is to use \\(f_{\\theta} \\approx f_{\\star}\\) on new data \\(X_{1}^{\\prime}, \\ldots, X_{M}^{\\prime} \\in X\\) to find labels \\(Y_{1}^{\\prime}, \\ldots, Y_{M}^{\\prime} \\in \\mathcal{Y}\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#is-prediction-possible","title":"Is prediction possible?","text":"<p>In the worst hypotheticals, prediction is impossible.</p> <ul> <li>Even though smoking is harmful for every other human being, how can we be \\(100 \\%\\) sure that this one person is not a mutant who benefits from the chemicals of a cigarette?</li> <li>Water freezes at \\(0^{\\circ}\\), but will the same be true tomorrow? How can we be \\(100 \\%\\) sure that the laws of physics will not suddenly change tomorrow?</li> </ul> <p>Of course, prediction is possible in practice.</p> <p>Theoretically, prediction requires assumptions on the distribution of \\(X\\) and the model of \\(f_{\\star}\\) is needed. This is in the realm of statistics of statistical learning theory.</p> <p>For now, we will take the view that if we predict known labels of the training data, we can reasonably hope to do well on the new data. (We will discuss the issue of generalization and overfitting later.)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#training-data-vs-test-data","title":"Training data vs. test data","text":"<p>When testing a machine learning model, it is essential that one separates the training data with the test data.</p> <p>In other classical disciplines using data, one performs a statistical hypothesis test to obtain a \\(p\\)-value. In ML, people do not do that.</p> <p>The only sure way to ensure that the model is doing well is to assess its performance on new data.</p> <p>Usually, training data and test data is collected together. This ensures that they have the same statistical properties. The assumption is that this test data will be representative of the actual data one intends to use machine learning on.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#aside-maximum-likelihood-estimation-cong-minimizing-kl-divergence","title":"Aside: Maximum likelihood estimation \\(\\cong\\) minimizing KL divergence","text":"<p>Consider the setup where you have IID discrete random variables \\(X_{1}, \\ldots, X_{N}\\) that can take values \\(1, \\ldots, k\\). We model the probability masses with \\(\\mathbb{P}_{\\theta}(X=1), \\ldots, \\mathbb{P}_{\\theta}(X=k)\\). The maximum likelihood estimation (MLE) is obtained by solving</p> \\[ \\underset{\\theta}{\\operatorname{maximize}} \\frac{1}{N} \\sum_{i=1}^{N} \\log \\left(\\mathbb{P}_{\\theta}\\left(X_{i}\\right)\\right) \\] <p>Next, define</p> \\[ f_{\\theta}=\\left[\\begin{array}{c} \\mathbb{P}_{\\theta}(X=1) \\\\ \\vdots \\\\ \\mathbb{P}_{\\theta}(X=k) \\end{array}\\right], \\quad \\mathcal{P}\\left(X_{1}, \\ldots, X_{N}\\right)=\\frac{1}{N}\\left[\\begin{array}{c} \\#\\left(X_{i}=1\\right) \\\\ \\vdots \\\\ \\#\\left(X_{i}=k\\right) \\end{array}\\right] . \\] <p>Then MLE is equivalent to minimizing the KL divergence from the model to the empirical distribution.</p> MLE \\(\\underset{\\theta}{\\operatorname{minimize}}\\) $\\mathbb{\\ \\(\\left.\\underset{\\theta}{\\operatorname{minimize}}\\left(X_{1}, \\ldots, X_{N}\\right), f_{\\theta}\\right)\\) $D_{\\mathrm{KL}}\\left(\\mathcal{P}\\left(X_{1}, \\ldots, X_{N}\\right) \\"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#aside-maximum-likelihood-estimation-cong-minimizing-kl-divergence_1","title":"Aside: Maximum likelihood estimation \\(\\cong\\) minimizing KL divergence","text":"<p>One can also derive LR equivalently as the MLE.</p> <p>Generally, one can view the MLE as minimizing the KL divergence between the model and the empirical distribution. (For continuous random variables like the Gaussian, this requires extra work, since we haven't defined the KL divergence for continuous random variables.)</p> <p>In deep learning, the distance measure need not be KL divergence.</p> <p>Dataset: MNIST</p> <p>Images of hand-written digits with \\(28 \\times 28=784\\) pixels and integervalued intensity between 0 and 255 . Every digit has a label in \\(\\{0,1, \\ldots, 8,9\\}\\).</p> <p>70,000 images (60,000 for training 10,000 testing) of 10 almost balanced classes.</p> <p>One of the simplest data set used in machine learning.</p> 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 <p> </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#dataset-mnist","title":"Dataset: MNIST","text":"<p>The USA government needed a standardized test to assess handwriting recognition software being sold to the government. So the NIST (National Institute of Standards and Technology) created the dataset in the 1990s. In 1990, NIST Special Database 1 distributed on CD-ROMs by mail. NIST SD 3 (1992) and SD 19 (1995) were improvements.</p> <p>Humans were instructed to fill out handwriting sample forms. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#dataset-mnist_1","title":"Dataset: MNIST","text":"<p>However, humans cannot be trusted to follow instructions, so a lab technician performed \"human ground truth adjudication\".</p> <p>In 1998, Man LeCun, Corinna Cortes, Christopher J. C. Barges took the NIST dataset and modified it to create the MNIST dataset. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#role-of-datasets-in-ml-research","title":"Role of Datasets in ML Research","text":"<p>An often underappreciated contribution.</p> <p>Good datasets play a crucial role in driving progress in ML research.</p> <p>Thinking about the dataset is the essential first step of understanding the feasibility of a ML task.</p> <p>Accounting for the cost of producing datasets and leveraging freely available data as much as possible (semi-supervised learning) is a recent trend in machine learning.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#dataset-cifar10","title":"Dataset: CIFAR10","text":"<p>\\(60,00032 \\times 32\\) color images in 10 (perfectly) balanced classes. airplane automobile bird cat deer dog frog horse ship truck </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#dataset-cifar10_1","title":"Dataset: CIFAR10","text":"<p>In 2008, a MIT and NYU team created the 80 million tiny images data set by searching on Google, Flickr, and Altavista for every non-abstract English noun and downscaled the images to \\(32 \\times 32\\). The search term provided an unreliable label for the image. This dataset was not very easy to use since the classes were too numerous.</p> <p>In 2009, Alex Krizhevsky published the CIFAR10, by distilling just a few classes and cleaning up the labels. Students were paid to verify the labels.</p> <p>The dataset was named CIFAR-10 after the funding agency Canadian Institute For Advanced Research.  There is also a CIFAR-100 with 100 classes.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#shallow-learning-with-pytorch","title":"Shallow learning with PyTorch","text":"<p>PyTorch demo</p> <p>We follow the following steps</p> <ol> <li>Load data</li> <li>Define model</li> <li> <p>Miscellaneous setup</p> </li> <li> <p>Instantiate model</p> </li> <li>Choose loss function</li> <li> <p>Choose optimizer</p> </li> <li> <p>Train with SGD</p> </li> <li> <p>Clear previously computed gradients</p> </li> <li>Compute forward pass</li> <li>Compute gradient via backprop</li> <li> <p>SGD update</p> </li> <li> <p>Evaluate trained model</p> </li> <li>Visualize results of trained model</li> </ol>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#lr-as-a-1-layer-neural-network","title":"LR as a 1-layer neural network","text":"<p>In LR, we solve</p> \\[ \\operatorname{minimize}_{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(f_{\\theta}\\left(X_{i}\\right), Y_{i}\\right) \\] <p>where \\(\\ell\\left(y_{1}, y_{2}\\right)=\\log \\left(1+e^{-y_{1} y_{2}}\\right)\\) and \\(f_{\\theta}\\) is linear.</p> <p>We can view \\(f_{\\theta}(x)=0=a^{\\top} x+b\\) as a 1-layer (shallow) neural network.</p> <p>Output layer </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#linear-deep-networks-make-no-sense","title":"Linear deep networks make no sense","text":"<p>What happens if we stack multiple linear layers? Problem: This is pointless because composition of linear functions is linear. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#deep-neural-networks-with-nonlinearities","title":"Deep neural networks with nonlinearities","text":"<p>Solution: use a nonlinear activation function \\(\\sigma\\) to inject nonlinearities. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#common-activation-functions","title":"Common activation functions","text":"<p>Rectified Linear Unit (ReLU) \\(\\operatorname{ReLU}(z)=\\max (z, 0)\\)</p> <p>Sigmoid \\(\\operatorname{Sigmoid}(z)=\\frac{1}{1+e^{-z}}\\)</p> <p>Hyperbolic tangent \\(\\tanh (z)=\\frac{1-e^{-2 z}}{1+e^{-2 z}}\\)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#multilayer-perceptron-mlp","title":"Multilayer perceptron (MLP)","text":"<p>The multilayer perceptron, also called fully connected neural network, has the form</p> \\[ \\begin{aligned} y_{L}= &amp; W_{L} y_{L-1}+b_{L} \\\\ y_{L-1}= &amp; \\sigma\\left(W_{L-1} y_{L-2}+b_{L-1}\\right) \\\\ &amp; \\vdots \\\\ y_{2}= &amp; \\sigma\\left(W_{2} y_{1}+b_{2}\\right) \\\\ y_{1}= &amp; \\sigma\\left(W_{1} x+b_{1}\\right), \\end{aligned} \\] <p>where \\(x \\in \\mathbb{R}^{n_{0}}, W_{\\ell} \\in \\mathbb{R}^{n_{\\ell} \\times n_{\\ell-1}}, b_{\\ell} \\in \\mathbb{R}^{n_{\\ell}}\\), and \\(n_{L}=1\\). To clarify, \\(\\sigma\\) is applied element-wise.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#mlp-for-cifar10-binary-classification","title":"MLP for CIFAR10 binary classification","text":"<p> activation function \\(\\sigma=\\operatorname{ReLU}\\)</p> <p>PyTorch demo</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#linear-layer-formal-definition","title":"Linear layer: Formal definition","text":"<p>Input tensor: \\(X \\in \\mathbb{R}^{B \\times n}, B\\) batch size, \\(n\\) number of indices. Output tensor: \\(Y \\in \\mathbb{R}^{B \\times m}, B\\) batch size, \\(m\\) number of indices.</p> <p>With weight \\(A \\in \\mathbb{R}^{m \\times n}\\), bias \\(b \\in \\mathbb{R}^{m}, k=1, \\ldots B\\), and \\(i=1, \\ldots, m\\) :</p> \\[ Y_{k, i}=\\sum_{j=1}^{n} A_{i, j} X_{k, j}+b_{i} \\] <p>Operation is independent across elements of the batch.</p> <p>If bias=False, then \\(b=0\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#weight-initialization","title":"Weight initialization","text":"<p>Remember, SGD is</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha g^{k} \\] <p>where \\(\\theta^{0} \\in \\mathbb{R}^{p}\\) is an initial point.</p> <p>In nice (convex) optimization problems, the initial point \\(\\theta^{0}\\) is not important; you converge to the global solution no matter how you initialize.</p> <p>In deep learning, it is very important to initialize \\(\\theta^{0}\\) well. In fact, \\(\\theta^{0}=0\\) is a terrible idea.</p> <p>Example) With an MLP with ReLU activations functions, if all weights and biases are initialized to be zero, then only the output layer's bias is trained and all other parameters do not move. So the training is stuck at a trivial network setting with \\(f_{\\theta}(x)=\\) constant.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#weight-initialization_1","title":"Weight initialization","text":"<p>PyTorch layers have default initialization schemes. (The default is not to initialize everything to 0 .) Sometimes this default initialization scheme is sufficient (eg. Chapter 2 code.ipynb) sometimes it is not sufficient (eg. Hw3 problem 1).</p> <p>How to initialize weights is tricky. More on this later.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#gradient-computation-via-backprop","title":"Gradient computation via backprop","text":"<p>PyTorch and other deep learning libraries allows users to specify how to evaluate a function then compute derivatives (gradients) automatically.</p> <p>No need to work out gradient computation by hand (even though I make you do it in homework assignments). This feature is called, automatic differentiation, back propagation, or just the chain rule. This is implemented in the torch. autograd module.</p> <p>More on this later.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#multi-class-classification-problem","title":"Multi-class classification problem","text":"<p>Consider supervised learning with data \\(X_{1}, \\ldots, X_{N} \\in \\mathbb{R}^{n}\\) and labels \\(Y_{1}, \\ldots, Y_{N} \\in\\{1, \\ldots, k\\}\\). (A \\(k\\) class classification problem.) Assume there exists a function \\(f_{\\star}: \\mathbb{R}^{n} \\rightarrow \\Delta^{k}\\) mapping from data to label probabilities. Here, \\(\\Delta^{k} \\subset \\mathbb{R}^{k}\\) denotes the set of probability mass functions on \\(\\{1, \\ldots, k\\}\\).</p> <p>Define the empirical distribution \\(\\mathcal{P}(y) \\in \\mathbb{R}^{k}\\) as the one-hot vector:</p> \\[ (\\mathcal{P}(y))_{i}=\\left\\{\\begin{array}{cc} 1 &amp; \\text { if } y=i \\\\ 0 &amp; \\text { otherwise } \\end{array}\\right. \\] <p>for \\(i=1, \\ldots, k\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#softmax-function","title":"Softmax function","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#examples","title":"Examples:","text":"<p>Softmax function \\(\\mu: \\mathbb{R}^{k} \\rightarrow \\Delta^{k}\\) is defined by</p> \\[ \\mu_{i}(z)=(\\mu(z))_{i}=\\frac{e^{z_{i}}}{\\sum_{j=1}^{k} e^{z_{j}}} \\] <p>for \\(i=1, \\ldots, k\\), where \\(z=\\left(z_{1}, \\ldots, z_{k}\\right) \\in \\mathbb{R}^{k}\\). Since</p> \\[ \\sum_{i=1}^{k} \\mu_{i}(z)=1, \\quad \\mu&gt;0 \\] \\[ \\begin{aligned} &amp; \\mu\\left(\\left[\\begin{array}{l} 1 \\\\ 2 \\\\ 3 \\end{array}\\right]\\right)=\\left[\\begin{array}{l} 0.09 \\\\ 0.24 \\\\ 0.6 \\end{array}\\right] \\\\ &amp; \\mu\\left(\\left[\\begin{array}{c} 999 \\\\ 0 \\\\ -2 \\end{array}\\right]\\right) \\approx\\left[\\begin{array}{l} 1 \\\\ 0 \\\\ 0 \\end{array}\\right] \\\\ &amp; \\mu\\left(\\left[\\begin{array}{c} -2 \\\\ -2 \\\\ -99 \\end{array}\\right]\\right) \\approx\\left[\\begin{array}{c} 0.5 \\\\ 0.5 \\\\ 0 \\end{array}\\right] \\end{aligned} \\] <p>Name \"softmax\" is a misnomer. \"Softargmax\" would be more accurate</p> <ul> <li>\\(\\mu(z) \\not \\approx \\max (z)\\)</li> <li>\\(\\mu(z) \\approx \\operatorname{argmax}(z)\\)</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#softmax-regression","title":"Softmax regression","text":"<p>In softmax regression (SR):</p> <ol> <li>Choose the model</li> </ol> \\[ \\mu\\left(f_{A, b}(x)\\right)=\\frac{1}{\\sum_{i=1}^{k} e^{a_{i}^{\\top} x+b_{i}}}\\left[\\begin{array}{c} e^{a_{1}^{\\top} x+b_{1}} \\\\ e^{a_{2}^{\\top} x+b_{2}} \\\\ \\vdots \\\\ e^{a_{k}^{\\top} x+b_{k}} \\end{array}\\right], \\quad f_{A, b}(x)=A x+b, A=\\left[\\begin{array}{c} a_{1}^{\\top} \\\\ a_{2}^{\\top} \\\\ \\vdots \\\\ a_{k}^{\\top} \\end{array}\\right] \\in \\mathbb{R}^{k \\times n}, \\quad b=\\left[\\begin{array}{c} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{k} \\end{array}\\right] \\in \\mathbb{R}^{k} . \\] <ol> <li>Minimize KL-Divergence (or cross entropy) from the model \\(\\mu\\left(f_{A, b}\\left(X_{i}\\right)\\right)\\) output probabilities to the empirical distribution \\(\\mathcal{P}\\left(Y_{i}\\right)\\). \\(\\operatorname{minimize}_{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}} \\sum_{i=1}^{N} D_{\\mathrm{KL}}\\left(\\mathcal{P}\\left(Y_{i}\\right) \\| \\mu\\left(f_{A, b}\\left(X_{i}\\right)\\right)\\right) \\Leftrightarrow \\operatorname{minimize}_{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}} \\sum_{i=1}^{N} H\\left(\\mathcal{P}\\left(Y_{i}\\right), \\mu\\left(f_{A, b}\\left(X_{i}\\right)\\right)\\right)\\)</li> </ol>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#softmax-regression_1","title":"Softmax regression","text":"\\[ \\begin{aligned} &amp; \\operatorname{minimize}_{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}} \\sum_{i=1}^{N} H\\left(\\mathcal{P}\\left(Y_{i}\\right), \\mu\\left(f_{A, b}\\left(X_{i}\\right)\\right)\\right) \\\\ &amp; \\operatorname{minimize}_{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}} \\frac{1}{N} \\sum_{i=1}^{N}-\\log \\left(\\mu_{Y_{i}}\\left(f_{A, b}\\left(X_{i}\\right)\\right)\\right) \\\\ &amp; \\operatorname{minimize}_{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}} \\frac{1}{N} \\sum_{i=1}^{N}-\\log \\left(\\frac{\\exp \\left(a_{Y_{i}}^{\\top} X_{i}+b_{Y_{i}}\\right)}{\\sum_{j=1}^{k} \\exp \\left(a_{j}^{\\top} X_{i}+b_{j}\\right)}\\right) \\\\ &amp; \\operatorname{minimize}_{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}} \\frac{1}{N} \\sum_{i=1}^{N}\\left(-\\left(a_{Y_{i}}^{\\top} X_{i}+b_{Y_{i}}\\right)+\\log \\left(\\sum_{j=1}^{k} \\exp \\left(a_{j}^{\\top} X_{i}+b_{j}\\right)\\right)\\right) \\end{aligned} \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#cross-entropy-loss","title":"Cross entropy loss","text":"<p>So</p> \\[ \\begin{array}{cc} \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} &amp; \\sum_{i=1}^{N} H\\left(\\mathcal{P}\\left(Y_{i}\\right), \\mu\\left(f_{A, b}\\left(X_{i}\\right)\\right)\\right) \\\\ \\operatorname{minimize}_{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}} &amp; \\frac{1}{N} \\sum_{i=1}^{N} \\ell^{\\mathrm{CE}}\\left(f_{A, b}\\left(X_{i}\\right), Y_{i}\\right) \\end{array} \\] <p>where</p> \\[ \\ell^{\\mathrm{CE}}(f, y)=-\\log \\left(\\frac{\\exp \\left(f_{y}\\right)}{\\sum_{j=1}^{k} \\exp \\left(f_{j}\\right)}\\right) \\] <p>is the cross entropy loss.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#classification-with-deep-networks","title":"Classification with deep networks","text":"<p>\\(\\mathrm{SR}=\\) linear model \\(f_{A, b}\\) with cross entropy loss:</p> \\[ \\operatorname{minimize}_{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell^{\\mathrm{CE}}\\left(f_{A, b}\\left(X_{i}\\right), Y_{i}\\right) \\Leftrightarrow \\operatorname{minimize}_{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}} \\sum_{i=1}^{N} D_{\\mathrm{KL}}\\left(\\mathcal{P}\\left(Y_{i}\\right) \\| \\mu\\left(f_{A, b}\\left(X_{i}\\right)\\right)\\right) \\] <p>(Note \\(e^{\\mathrm{CE}}(f, y)&gt;0\\). More on homework 3.)</p> <p>The natural extension of \\(S R\\) is to consider</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{P}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell^{\\mathrm{CE}}\\left(f_{\\theta}\\left(X_{i}\\right), Y_{i}\\right) \\Leftrightarrow \\underset{\\theta \\in \\mathbb{R}^{\\boldsymbol{P}}}{\\operatorname{minimize}} \\quad \\sum_{i=1}^{N} D_{\\mathrm{KL}}\\left(\\mathcal{P}\\left(Y_{i}\\right) \\| \\mu\\left(f_{\\theta}\\left(X_{i}\\right)\\right)\\right) \\] <p>where \\(f_{\\theta}\\) is a deep neural network.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#history-of-gpu-computing","title":"History of GPU Computing","text":"<p>Rendering graphics involves computing many small tasks in parallel. Graphics cards provide many small processors to render graphics. In 1999, Nvidia released GeForce 256 and introduced programmability in the form of vertex and pixel shaders. Marketed as the first 'Graphical Processing Unit (GPU)'. Researchers quickly learned how to implement linear algebra by mapping matrix data into textures and applying shaders.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#general-purpose-gpus-gpgpu","title":"General Purpose GPUs (GPGPU)","text":"<p>In 2007, Nvidia released 'Compute Unified Device Architecture (CUDA)', which enabled general purpose computing on a CUDA-enabled GPUs.</p> <p>Unlike CPUs which provide fast serial processing, GPUs provide massive parallel computing with its numerous slower processors. The 2008 financial crisis hit Nvidia very hard as GPUs were luxury items used for games. This encouraged Nvidia to invest further in GPGPUs and create a more stable consumer base.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#cpu-computing-model","title":"CPU computing model","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#gpu-computing-model","title":"GPU computing model","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#gpus-for-machine-learning","title":"GPUs for machine learning","text":"<p>Raina et al.'s 2009* paper demonstrated that GPUs can be used to train large neural networks. (This was not the first to use of GPUs in machine learning, but it was one of the most influential.) Modern deep learning is driven by big data and big compute, respectively provided by the internet and GPUs.</p> <p>Krizhevsky et al.'s 2012* landmark paper introduced AlexNet trained on GPUs and kickstarted the modern deep learning boom. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#example-power-iteration-with-gpus","title":"Example: Power iteration with GPUs","text":"<p>Computing \\(x^{100}=A^{100} x^{0}\\) with a GPU:</p> <pre><code>send A from host (CPU) to device (GPU)\nsend x=x0 from host (CPU) to device (GPU)\nfor _ in range(100):\n    tell GPU to compute x=A*x\nsend x from device (GPU) to host (CPU)\n</code></pre> <p>In this example and deep learning, GPU accelerates computation since: Amount of computation &gt; data communication. Large information resides in the GPU, and CPU issues commands to perform computation on the data. ( \\(A\\) in this example, neural network architecture in deep learning.)</p> <p>PyTorch demo</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#deep-learning-on-gpus","title":"Deep learning on GPUs","text":"<p>Steps for training neural network on GPU:</p> <ol> <li> <p>Create the neural network on CPU and send it to GPU. Neural network parameters stay on GPU.</p> </li> <li> <p>Sometimes you load parameters from CPU to GPU.</p> </li> <li> <p>Select data batch (image, label) and send it to GPU every iteration</p> </li> <li> <p>Data for real-world setups is large, so keeping all data on GPU is infeasible.</p> </li> <li> <p>On GPU, compute network output (forward pass)</p> </li> <li>On GPU, compute gradients (backward pass)</li> <li>On GPU, perform gradient update</li> <li> <p>Once trained, perform prediction on GPU.</p> </li> <li> <p>Send test data to GPU.</p> </li> <li>Compute network output.</li> <li>Retrieve output on CPU.</li> <li>Alternatively, neural network can be loaded on CPU and prediction can be done on CPU.</li> </ol> <p>PyTorch demo</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#chapter-3-convolutional-neural-networks","title":"Chapter 3: Convolutional Neural Networks","text":"<p>Mathematical Foundations of Deep Neural Networks Spring 2024 Department of Mathematical Sciences Ernest K. Ryu Seoul National University</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#fully-connected-layers","title":"Fully connected layers","text":"<p>Advantages of fully connected layers:</p> <ul> <li>Simple.</li> <li>Very general, in theory. (Sufficiently large MLPs can learn any function, in theory.)</li> </ul> <p>Disadvantage of fully connected layers:</p> <ul> <li>Too many trainable parameters.</li> <li>Does not encode shift equivariance/invariance and therefore has poor inductive bias. (More on this later.)</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#shift-equivarianceinvariance-in-vision","title":"Shift equivariance/invariance in vision","text":"<p>Many tasks in vision are equivariant/invariant with respect shifts/translations. </p> <p>Cat </p> <p>Still a Cat</p> <p>Roughly speaking, equivariance/invariance means shifting the object does not change the meaning (it only changes the position).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#shift-equivarianceinvariance-in-vision_1","title":"Shift equivariance/invariance in vision","text":"<p>Logistic regression (with a single fully connected layer) does not encode shift invariance. </p> <p>Since convolution is equivariant with respect to translations, constructing neural network layers with them is a natural choice.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#convolution","title":"Convolution","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#multiple-filters","title":"Multiple filters","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#2d-convolutional-layer-formal-definition","title":"2D convolutional layer: Formal definition","text":"<p>Input tensor: \\(X \\in \\mathbb{R}^{B \\times C_{\\text {in }} \\times m \\times n}, B\\) batch size, \\(C_{\\text {in }} \\#\\) of input channels, \\(m, n \\#\\) of vertical and horizontal indices. Output tensor: \\(Y \\in \\mathbb{R}^{B \\times C_{\\text {out }} \\times\\left(m-f_{1}+1\\right) \\times\\left(n-f_{2}+1\\right)}, B\\) batch size, \\(C_{\\text {out }} \\#\\) of output channels.  and \\(j=1, \\ldots, n-f_{2}+1\\) :</p> \\[ Y_{k, \\ell, i, j}=\\sum_{\\gamma=1}^{c_{\\text {in }}} \\sum_{\\alpha=1}^{f_{1}} \\sum_{\\beta=1}^{f_{2}} w_{\\ell, \\gamma, \\alpha, \\beta} X_{k, \\gamma, i+\\alpha-1, j+\\beta-1}+b_{\\ell} \\] <p>Operation is independent across elements of the batch. The vertical and horizontal indices are referred to as spatial dimensions. If bias=False, then \\(b=0\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#notes-on-convolution","title":"Notes on convolution","text":"<p>Mind the indexing. In math, indices start at 1. In Python, indices start at 0.</p> <p>1D conv is commonly used with 1D data, such as audio.</p> <p>3D conv is commonly used with 3D data, such as video.</p> <p>1D and 3D conv are defined analogously.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#zero-padding","title":"Zero padding","text":"<p>\\((C \\times 7 \\times 7\\) image \\() \\circledast(C \\times 5 \\times 5\\) filter \\()=(1 \\times 3 \\times 3\\) feature map \\()\\). Spatial dimension 7 reduced to 3 . </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#zero-padding_1","title":"Zero padding","text":"<p>\\((C \\times 7 \\times 7\\) image with zero padding \\(=2) \\circledast(C \\times 5 \\times 5\\) filter \\()=(1 \\times 7 \\times 7\\) feature map \\()\\). Spatial dimension is preserved. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#stride","title":"Stride","text":"<p>\\((7 x 7\\) image \\() \\circledast(3 x 3\\) filter with stride 2\\()=(\\) output \\(3 x 3)\\). (With stride 1 , output is \\(5 \\times 5\\).) </p> <p>If stride 3, dimensions don't fit. \\(7 \\times 7\\) image with zero padding of 1 becomes \\(9 \\times 9\\) image. \\((7 \\times 7\\) image, padding of 1\\() \\circledast(3 x 3\\) filter \\()\\) with stride 3 does fit.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#convolution-summary","title":"Convolution summary","text":"<p>Input \\(C_{\\text {in }} \\times W_{\\text {in }} \\times H_{\\text {in }}\\) Conv layer parameters</p> <ul> <li>\\(C_{\\text {out }}\\) filters</li> <li>\\(F\\) spatial extent ( \\(C_{\\mathrm{in}} \\times F \\times F\\) filters)</li> <li>\\(S\\) stride</li> <li>\\(\\quad P\\) padding</li> </ul> <p>Output \\(C_{\\text {out }} \\times W_{\\text {out }} \\times H_{\\text {out }}\\)</p> \\[ \\begin{aligned} &amp; W_{\\mathrm{out}}=\\left\\lfloor\\frac{W_{\\mathrm{in}}-F+2 P}{S}+1\\right\\rfloor \\\\ &amp; H_{\\mathrm{out}}=\\left\\lfloor\\frac{H_{\\mathrm{in}}-F+2 P}{S}+1\\right\\rfloor \\end{aligned} \\] <p>\\(\\lfloor\\cdot\\rfloor\\) denotes the floor (rounding down) operation. To avoid the complication of this floor operation, it is best to ensure the formula inside evaluates to an integer.</p> <p>Number of trainable parameters: </p> <p>Make sure you are able to derive these formulae yourself.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#aside-geometric-deep-learning","title":"Aside: Geometric deep learning","text":"<p>More generally, given a group \\(\\mathcal{G}\\) encoding a symmetry or invariance, one can define operations \"equivariant\" with respect \\(\\mathcal{G}\\) and construct equivariant neural networks.</p> <p>This is the subject of geometric deep learning, and its formulation utilizes graph theory and group theory.</p> <p>Geometric deep learning is particularly useful for non-Euclidean data. Examples include as protein molecule data and social network service connections.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#pooling","title":"Pooling","text":"<p>Primarily used to reduce spatial dimension. Similar to conv. Operates over each channel independently. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#pooling_1","title":"Pooling","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#lenet5","title":"LeNet5","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#modern-instances-of-lenet5-use","title":"Modern instances of LeNet5 use","text":"<ul> <li>\\(\\sigma=\\) ReLu</li> <li>MaxPool instead of AvgPool</li> <li>No \\(\\sigma\\) after S2, S4 (Why?)</li> <li>Full connection instead of Gaussian connections \\(1 \\times 28 \\times 28\\) MNIST image</li> <li>Complete C3 connections with \\(p=2 \\Rightarrow 1 \\times 32 \\times 32\\) </li> </ul> \\[ \\begin{aligned} &amp; \\text { Something like } \\\\ &amp; \\text { average pool } \\\\ &amp; f=2, s=2 \\end{aligned} \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#lenet5_1","title":"LeNet5","text":"<p>PyTorch demo</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#architectural-contribution-lenet","title":"Architectural contribution: LeNet","text":"<p>One of the earliest demonstration of using a deep CNN to learn a nontrivial task.</p> <p>Laid the foundation of the modern CNN architecture.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#weight-sharing","title":"Weight sharing","text":"<p>In neural networks, weight sharing is a way to reduce the number of parameters by reusing the same parameter in multiple operations. Convolutional layers are the primary example.</p> \\[ A_{w}=\\left[\\begin{array}{cccccccc} w_{1} &amp; \\cdots &amp; w_{r} &amp; 0 &amp; \\cdots &amp; &amp; &amp; 0 \\\\ 0 &amp; w_{1} &amp; \\cdots &amp; w_{r} &amp; 0 &amp; \\cdots &amp; &amp; 0 \\\\ 0 &amp; 0 &amp; w_{1} &amp; \\cdots &amp; w_{r} &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; &amp; &amp; \\ddots &amp; &amp; \\ddots &amp; &amp; \\vdots \\\\ 0 &amp; &amp; \\cdots &amp; 0 &amp; w_{1} &amp; \\cdots &amp; w_{r} &amp; 0 \\\\ 0 &amp; &amp; \\cdots &amp; 0 &amp; 0 &amp; w_{1} &amp; \\cdots &amp; w_{r} \\end{array}\\right] \\] <p>If we consider convolution with filter \\(w\\) as a linear operator, the components of \\(w\\) appear may times in the matrix representation. This is because the same \\(w\\) is reused for every patch in the convolution. Weight sharing in convolution may now seem obvious, but it was a key contribution back when the LeNet architecture was presented. Some models (not studied in this course) use weight sharing more explicitly in other ways.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#data-augmentation","title":"Data augmentation","text":"<p>Invariances</p> <ul> <li>Translation</li> <li>Horizontal flip</li> <li>Vertical flip</li> <li>Color change (?) </li> </ul> <p>Invariances</p> <ul> <li>Translation</li> <li>Horizontal flip</li> <li>Vertical flip</li> <li>Color change</li> </ul> <p>Translation invariance encoded in convolution, but other invariances are harder to encode (unless one uses geometric deep learning). Therefore encode invariances in data and have neural networks learn the invariance.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#data-augmentation_1","title":"Data augmentation","text":"<p>Data augmentation (DA) applies transforms to the data while preserving meaning and label.</p> <p>Option 1: Enlarge dataset itself.</p> <ul> <li>Usually cumbersome and unnecessary.</li> </ul> <p>Option 2: Use randomly transformed data in training loop.</p> <ul> <li>In PyTorch, we use Torchvision.transforms.</li> </ul> <p>PyTorch demo</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#spurious-correlation","title":"Spurious correlation","text":"<p>Hypothetical: A photographer prefers to take pictures with cats looking to the left and dogs looking to the right. Neural network learns to distinguish cats from dogs by which direction it is facing. This learned correlation will not be useful for pictures taken by another photographer.</p> <p>This is a spurious correlation, a correlation between the data and labels that does not capture the \"true\" meaning. Spurious correlations are not robust in the sense that the spurious correlation will not be a useful predictor when the data changes slightly.</p> <p>Removing spurious correlations is another purpose of DA.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#data-augmentation_2","title":"Data augmentation","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#we-use-da-to","title":"We use DA to:","text":"<ul> <li>Inject our prior knowledge of the structure of the data and force the neural network to learn it.</li> <li>Remove spurious correlations.</li> <li>Increase the effective data size. In particular, we ensure neural network never encounters the exact same data again and thereby prevent the neural network from performing exact memorization. (Neural network can memorize quite well.)</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#effects-of-da","title":"Effects of DA:","text":"<ul> <li>DA usually worsens the training error (but we don't care about training error).</li> <li>DA often, but not always, improves the test error.</li> <li>If DA removes a spurious correlation, then the test error can be worsened.</li> <li>DA usually improves robustness.</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#data-augmentation-on-test-data","title":"Data augmentation on test data?","text":"<p>DA is usually applied only on training data.</p> <p>DA is usually not applied on test data, because we want to ensure test scores are comparable. (There are many different DAs, and applying different DAs on test data will make the metric different.)</p> <p>However, one can perform \"test-time data augmentation\" to improve predictions without changing the test. More on this later.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#imagenet-dataset","title":"ImageNet dataset","text":"<p>ImageNet contains more 14 million hand-annotated images in more than 20,000 categories. </p> <p>Many classes, higher resolution, non-uniform image size, multiple objects per image.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#history","title":"History","text":"<ul> <li>Fei-Fei Li started the ImageNet project in 2006 with the goal of expanding and improving the data available for training Al algorithms.</li> <li>Images were annotated with Amazon Mechanical Turk.</li> <li>The ImageNet team first presented their dataset in the 2009 Conference on Computer Vision and Pattern Recognition (CVPR).</li> <li>From 2010 to 2017, the ImageNet project ran the ImageNet Large Scale Visual Recognition Challenge (ILSVRC).</li> <li>In the 2012 ILSVRC challenge, 150,000 images of 1000 classes were used.</li> <li>In 2017, 29 teams achieved above 95\\% accuracy. The organizers deemed task complete and ended the ILSVRC competition.</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#imagenet-1k","title":"ImageNet-1k","text":"<p>Commonly referred to as \"the ImageNet dataset\". Also called ImageNet2012</p> <p>However, ImageNet-1k is really a subset of full ImageNet dataset.</p> <p>ImageNet-1k has 150,000 images of 1000 roughly balanced classes.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#list-of-categories","title":"List of categories:","text":"<p>https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#imagenet-1k_1","title":"ImageNet-1k","text":"<p>Data has been removed from the ImageNet website. Downloading peer-to-peer via torrent is now the most convenient way to access the data.</p> <p>Privacy concerns: Although dataset is about recognizing objects, rather than humans, many human faces are in the images. Troublingly, identifying personal information is possible.</p> <p>NSFW concerns: Sexual and non-consensual content.</p> <p>Creating datasets while protecting privacy and other social values is an important challenge going forward.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#top-1-vs-top-5-accuracy","title":"Top-1 vs. top-5 accuracy","text":"<p>Classifiers on ImageNet-1k are often assessed by their top-5 accuracy, which requires the 5 categories with the highest confidence to contain the label.</p> <p>In contrast, the top-1 accuracy simply measures whether the network's single prediction is the label.</p> <p>For example, AlexNet had a top-5 accuracy of \\(84.6 \\%\\) and a top- 1 accuracy of \\(63.3 \\%\\).</p> <p>Nowadays, accuracies of classifiers has improved, so the top 1 accuracy is becoming the more common metric.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#classical-statistics-over-vs-underfitting","title":"Classical statistics: Over vs. underfitting","text":"<p>Given separate train and test data</p> <ul> <li>When (training loss) &lt;&lt; (testing loss) you are overfitting. What you have learned from the training data does not carry over to test data.</li> <li>When (training loss) \\(\\approx\\) (testing loss) you are underfitting. You have the potential to learn more from the training data. </li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#classical-statistics-over-vs-underfitting_1","title":"Classical statistics: Over vs. underfitting","text":"<p>The goal of ML is to learn patterns that generalize to data you have not seen. From each datapoint, you want to learn enough (don't underfit) but if you learn too much you overcompensate for an observation specific to the single experience.</p> <p>In classical statistics, underfitting vs. overfitting (bias vs. variance tradeoff) is characterized rigorously. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#modern-deep-learning-double-descent","title":"Modern deep learning: Double descent","text":"<p>In modern deep learning, you can overfit, but the state-of-the art neural networks do not overfit (or \"benignly overfit\") despite having more model parameters than training data.</p> <p>We do not yet have clarity with this new phenomenon.</p> <p>When overfitting happens and when it does not is unclear. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#double-descent-on-2-layer-neural-network-on-mnist-belkin-et-al-experimentally-demonstrates-the-double-descent-phenomenon-with-an-mlp-trained-on-the-mnist-dataset","title":"Double descent on 2-layer neural network on MNIST  Belkin et al. experimentally demonstrates the double descent phenomenon with an MLP trained on the MNIST dataset.","text":"<p>Fig. 3. Double-descent risk curve for a fully connected neural network on MNIST. Shown are training and test risks of a network with a single layer of \\(H\\) hidden units, learned on a subset of MNIST ( \\(n=4 \\cdot 10^{3}, d=784\\), \\(K=10\\) classes). The number of parameters is \\((d+1) \\cdot H+(H+1) \\cdot K\\). The interpolation threshold (black dashed line) is observed at \\(n \\cdot K\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#double-descent-example-2-layer-relu-nn-with-fixed-hidden-layer-weights","title":"Double descent example: 2-layer ReLU NN with fixed hidden layer weights","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#how-to-avoid-overfitting","title":"How to avoid overfitting","text":"<p>Regularization is loosely defined as mechanisms to prevent overfitting.</p> <p>When you are overfitting, regularize with:</p> <ul> <li>Smaller NN (fewer parameters) or larger NN (more parameters).</li> <li>Improve data by:</li> <li>using data augmentation</li> <li>acquiring better, more diverse, data</li> <li>acquiring more of the same data</li> <li>Weight decay</li> <li>Dropout</li> <li>Early stopping on SGD or late stopping on SGD</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#how-to-avoid-underfitting","title":"How to avoid underfitting","text":"<p>When you are underfitting, use:</p> <ul> <li>Larger NN (if computationally feasible)</li> <li>Less weight decay</li> <li>Less dropout</li> <li>Run SGD longer (if computationally feasible)</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#weight-decay-cong-ell2-regularization","title":"Weight decay \\(\\cong \\ell^{2}\\)-regularization","text":"<p>\\(\\ell^{2}\\)-regularization augments the loss function with</p> \\[ \\operatorname{minimize}_{\\theta \\in \\mathbb{R}^{p}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(f_{\\theta}\\left(x_{i}\\right), y_{i}\\right)+\\frac{\\lambda}{2}\\|\\theta\\|^{2} \\] <p>SGD on the augmented loss is usually implemented by changing SGD update rather than explicitly changing the loss since</p> \\[ \\begin{gathered} \\theta^{k+1}=\\theta^{k}-\\alpha\\left(g^{k}+\\lambda \\theta^{k}\\right) \\\\ =(1-\\alpha \\lambda) \\theta^{k}-\\alpha g^{k} \\end{gathered} \\] <p>Where \\(g^{k}\\) is stochastic gradient of original (unaugmented) loss. In classical statistics, this is called ridge regression or maximum a posteriori (MAP) estimation with Gaussian prior.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#weight-decay-cong-ell2-regularization_1","title":"Weight decay \\(\\cong \\ell^{2}\\)-regularization","text":"<p>In Pytorch, you can use SGD + weight decay by: augmenting the loss function</p> <pre><code>for param in model.parameters():\n    loss += (lamda/2)*param.pow(2.0).sum()\ntorch.optim.SGD(model.parameters(), lr=... , weight_decay=0)\n</code></pre> <p>or by using weight_decay in the optimizer torch.optim.SGD(model.parameters(), lr=... , weight_decay=lamda)</p> <p>For plain SGD, weight decay and \\(\\ell^{2}\\)-regularization are equivalent. For other optimizers, the two are similar but not the same. More on this later.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#dropout","title":"Dropout","text":"<p>Dropout is a regularization technique that randomly disables neurons.</p> <p>Standard layer,</p> \\[ h_{2}=\\sigma\\left(W_{1} h_{1}+b_{1}\\right) \\] <p>Dropout with drop probability \\(p\\) defines</p> \\[ h_{2}=\\sigma\\left(W_{1} h_{1}^{\\prime}+b_{1}\\right) \\] <p>with</p> \\[ \\left(h_{1}^{\\prime}\\right)_{j}= \\begin{cases}0 &amp; \\text { with probability } p \\\\ \\frac{\\left(h_{1}\\right)_{j}}{1-p} &amp; \\text { otherwise }\\end{cases} \\] <p> (a) Standard Neural Net  (b) After applying dropout.</p> <p>Figure 1: Dropout Neural Net Model. Left: A standard neural net with 2 hidden layers. Right: An example of a thinned net produced by applying dropout to the network on the left. Crossed units have been dropped.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#why-is-dropout-helpful","title":"Why is dropout helpful?","text":"<p>\"A motivation for dropout comes from a theory of the role of sex in evolution (Livnat et al., 2010).\"</p> <p>Sexual reproduction, compared to asexual reproduction, creates the criterion for natural selection mix-ability of genes rather than individual fitness, since genes are mixed in a more haphazard manner. \"Since a gene cannot rely on a large set of partners to be present at all times, it must learn to do something useful on its own or in collaboration with a small number of other genes. ... Similarly, each hidden unit in a neural network trained with dropout must learn to work with a randomly chosen sample of other units. This should make each hidden unit more robust and drive it towards creating useful features on its own without relying on other hidden units to correct its mistakes.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#why-is-dropout-helpful_1","title":"Why is dropout helpful?","text":"<p>The analogy to evolution is very interesting, but it is ultimately a heuristic argument. It also shifts the burden to the question: \"why is sexual evolution more powerful than asexual evolution?\"</p> <p>However, dropout can be shown to be loosely equivalent to \\(\\ell^{2}\\)-regularization. However, we do not yet have a complete understanding of the mathematical reason behind dropout's performance.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#dropout-in-pytorch","title":"Dropout in PyTorch","text":"<p>Dropout simply multiplies the neurons with a random \\(0-\\frac{1}{1-p_{\\text {drop }}}\\) mask.</p> <p>A direct implementation in PyTorch:</p> <pre><code>def dropout_layer(X, p_drop):\n    mask = (torch.rand(X.shape) &gt; p_drop).float()\n    return mask * X / (1.0 - p_drop)\n</code></pre> <p>PyTorch provides an implementation of dropout through torch.nn. Dropout.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#dropout-in-training-vs-test","title":"Dropout in training vs. test","text":"<p>Typically, dropout is used during training and turned off during prediction/testing. (Dropout should be viewed as an additional onus imposed during training to make training more difficult and thereby effective, but it is something that should be turned off later.)</p> <p>In PyTorch, activate the training mode with model.train() and activate evaluation mode with model.eval() dropout (and batchnorm) will behave differently in these two modes.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#when-to-use-dropout","title":"When to use dropout","text":"<p>Dropout is usually used on linear layers but not on convolutional layers.</p> <ul> <li>Linear layers have many weights and each weight is used only once per forward pass. (If \\(y=\\operatorname{Linear}_{A, b}(x)\\), then \\(A_{i j}\\) only affect \\(y_{i}\\).) So regularization seems more necessary.</li> <li>A convolutional filter has fewer weights and each weight is used multiple times in each forward pass. (If \\(y=\\operatorname{Conv} 2 \\mathrm{D}_{w, b}(x)\\), then \\(w_{i j k t}\\) affects \\(\\left.y_{i, .,:}.\\right)\\) So regularization seems less necessary.</li> </ul> <p>Dropout seems to be going out of fashion:</p> <ul> <li>Dropout's effect is somehow subsumed by batchnorm. (This is poorly understood.)</li> <li>Linear layers are less common due to their large number of trainable parameters.</li> </ul> <p>There is no consensus on whether dropout should be applied before or after the activation function. However, Dropout- \\(\\sigma\\) and \\(\\sigma\\)-Dropout are equivalent when \\(\\sigma\\) is \\(\\operatorname{ReLU}\\) or leaky \\(\\operatorname{ReLU}\\), or, more generally, when \\(\\sigma\\) is nonnegative homogeneous.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#sgd-early-stopping","title":"SGD early stopping","text":"<p>Early stopping of SGD refers to stopping the training early even if you have time for more iterations.</p> <p>The rationale is that SGD fits data, so too many iterations lead to overfitting.</p> <p>A similar phenomenon (too many iterations hurt) is observed in classical algorithms for inverse problems.</p> <p>Typical training and testing loss Us. iterations </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#epochwise-double-descent","title":"Epochwise double descent","text":"<p>Recently, however, an epochwise double descent has been observed.</p> <p>So perhaps one should stop SGD early or very late.</p> <p>We do not yet have clarity with this new phenomenon. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#more-data-by-data-auagmentation","title":"More data (by data auagmentation)","text":"<p>With all else fixed, using more data usually* leads to less overfitting.</p> <p>However, collecting more data is often expansive.</p> <p>Think of data augmentation (DA) as a mechanism to create more data for free. You can view DA as a form of regularization.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#summary-of-over-vs-underfitting","title":"Summary of over vs. underfitting","text":"<p>In modern deep learning, the double descent phenomenon has brought a conceptual and theoretical crisis regarding over and underfitting. Much of the machine learning practice is informed by classical statistics and learning theory, which do not take the double descent phenomenon into account.</p> <p>Double descent will bring fundamental changes to statistics, and researchers need more time to figure things out. Most researchers, practitioners and theoreticians, agree that not all classical wisdom is invalid, but what part do we keep, and what part do we replace?</p> <p>In the meantime, we will have to keep in mind the two contradictory viewpoints and move forward in the absence of clarity.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#alexnet","title":"AlexNet","text":"<p>Won the 2012 ImageNet challenge by a large margin: top-5 error rate \\(15.3 \\%\\) vs. \\(26.2 \\%\\) second place.</p> <p>Started the era of deep neural networks and their training via GPU computing. AlexNet was split into 2 as GPU memory was limited. (A single modern GPU can easily hold AlexNet.) </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#alexnet-for-imagenet","title":"AlexNet for ImageNet","text":"<p>^0</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#alexnet-cifar10","title":"AlexNet CIFAR10","text":"<p>Conv.-ReLU Max pool \\(f=3, s=2\\) (overlapping max pool) Network not split into 2 No local response normalization</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#architectural-contribution-alexnet","title":"Architectural contribution: AlexNet","text":"<p>A scaled-up version of LeNet.</p> <p>Demonstrated that deep CNNs can learn significantly complex tasks. (Some thought CNNs could only learn simple, toy tasks like MNIST.)</p> <p>Demonstrated GPU computing to be an essential component of deep learning.</p> <p>Demonstrated effectiveness of ReLU over sigmoid or tanh in deep CNNs for classification.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#sgd-type-optimizers","title":"SGD-type optimizers","text":"<p>In modern NN training, SGD and variants of SGD are usually used. There are many variants of SGD.</p> <p>The variants are compared mostly on an experimental basis. There is some limited theoretical basis in their comparisons. (Cf. Adam story.)</p> <p>So far, all efforts to completely replace SGD have failed.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#sgd-with-momentum","title":"SGD with momentum","text":"<p>SGD:</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha g^{k} \\] <p>SGD with momentum:</p> \\[ \\begin{gathered} v^{k+1}=g^{k}+\\beta v^{k} \\\\ \\theta^{k+1}=\\theta^{k}-\\alpha v^{k+1} \\end{gathered} \\] <p>\\(\\beta=0.9\\) is a common choice.</p> <p>When different coordinates (parameters) have very different scalings (i.e., when the problem is ill-conditioned, momentum can help find a good direction of progress.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#rmsprop","title":"RMSProp","text":"<p>RMSProp:</p> \\[ \\begin{gathered} m_{2}^{k+1}=\\beta_{2} m_{2}^{k}+\\left(1-\\beta_{2}\\right)\\left(g^{k} \\circledast g^{k}\\right) \\\\ \\theta^{k+1}=\\theta^{k}-\\alpha g^{k} \\oslash \\sqrt{m_{2}^{k+1}+\\epsilon} \\end{gathered} \\] <p>\\(\\beta_{2}=0.99\\) and \\(\\epsilon=10^{-8}\\) are common values. \\(\\circledast\\) and \\(\\oslash\\) are elementwise mult. and div. \\(m_{2}^{k}\\) is a running estimate of the \\(2^{\\text {nd }}\\) moment of the stochastic gradients, i.e., \\(\\left(m_{2}^{k}\\right)_{i} \\approx \\mathbb{E}\\left(g^{k}\\right)_{i}^{2}\\). \\(\\alpha \\oslash \\sqrt{m_{2}^{k+1}+\\epsilon}\\) is the learning rate scaled elementwise. Progress along steep and noisy directions are dampened while progress along flat and non-noisy directions are accelerated.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#adam-adaptive-moment-estimation","title":"Adam (Adaptive moment estimation)","text":"<p>Adam:</p> \\[ \\begin{gathered} m_{1}^{k+1}=\\beta_{1} m_{1}^{k}+\\left(1-\\beta_{1}\\right) g^{k}, m_{2}^{k+1}=\\beta_{2} m_{2}^{k}+\\left(1-\\beta_{2}\\right)\\left(g^{k} \\circledast g^{k}\\right) \\\\ \\tilde{m}_{1}^{k+1}=\\frac{m_{1}^{k+1}}{1-\\beta_{1}^{k+1}}, \\quad \\widetilde{m}_{2}^{k+1}=\\frac{m_{2}^{k+1}}{1-\\beta_{2}^{k+1}} \\\\ \\theta^{k+1}=\\theta^{k}-\\alpha \\widetilde{m}_{1}^{k+1} \\oslash \\sqrt{\\widetilde{m}_{2}^{k+1}+\\epsilon} \\end{gathered} \\] <ul> <li>\\(\\quad \\beta_{1}^{k+1}\\) means \\(\\beta_{1}\\) to the \\((k+1)\\) th power.</li> <li>\\(\\beta_{1}=0.9, \\beta_{2}=0.999\\), and \\(\\epsilon=10^{-8}\\) are common values. Initialize with \\(m_{1}^{0}=m_{2}^{0}=0\\).</li> <li>\\(m_{1}^{k}\\) and \\(m_{2}^{k}\\) are running estimates of the \\(1^{\\text {st }}\\) and \\(2^{\\text {nd }}\\) moments of \\(g^{k}\\).</li> <li>\\(\\tilde{m}_{1}^{k}\\) and \\(\\tilde{m}_{2}^{k}\\) are bias-corrected estimates of \\(m_{1}^{k}\\) and \\(m_{2}^{k}\\).</li> <li>Using \\(\\widetilde{m}_{1}^{k}\\) instead of \\(g^{k}\\) adds the effect of momentum.</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#bias-correction-of-adam","title":"Bias correction of Adam","text":"<p>To understand the bias correction, consider the hypothetical \\(g^{k}=g\\) for \\(k=0,1, \\ldots\\). Then</p> \\[ m_{1}^{k}=\\left(1-\\beta_{1}^{k}\\right) g \\] <p>and</p> \\[ m_{2}^{k}=\\left(1-\\beta_{2}^{k}\\right)(g \\circledast g) \\] <p>while \\(m_{1}^{k} \\rightarrow g\\) and \\(m_{2}^{k} \\rightarrow(g \\circledast g)\\) as \\(k \\rightarrow \\infty\\), the estimators are not exact despite there being no variation in \\(g^{k}\\).</p> <p>On the other hand, there is bias-corrected estimators are exact:</p> \\[ \\widetilde{m}_{1}^{k}=g \\] <p>and</p> \\[ \\widetilde{m}_{2}^{k}=(g \\circledast g) \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#the-cautionary-tale-of-adam","title":"The cautionary tale of Adam","text":"<p>Adam's original 2015 paper justified the effectiveness of the algorithm through experiments training deep neural networks with Adam. After all, this non-convex optimization is what Adam was proposed to do.</p> <p>However, the paper also provided a convergence proof under the assumption of convexity. This was perhaps unnecessary in an applied paper focusing on non-convex optimization.</p> <p>The proof was later shown to be incorrect! Adam does not always converge in the convex setup, i.e., the algorithm, rather than the proof, is wrong.</p> <p>Reddi and Kale presented the AMSGrad optimizer, which does come with a correct convergence proof, but AMSGrad tends to perform worse than Adam, empirically.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#how-to-choose-the-optimizer","title":"How to choose the optimizer","text":"<p>Extensive research has gone into finding the \"best\" optimizer. Schmidt et al.\" reports that, roughly speaking, that Adam works well most of the time.</p> <p>So, Adam is a good default choice. Currently, it seems to be the best default choice.</p> <p>However, Adam does not always work. For example, it seems to be that the widely used EfficientNet model can only be trained \\({ }^{\\dagger}\\) with RMSProp.</p> <p>However, there are some setups where the LR of SGD is harder to tune, but SGD outperforms Adam when properly tuned.#</p> <p>[^1]</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#how-to-tune-parameters","title":"How to tune parameters","text":"<p>Everything should be chosen by trial and error. The weight parameters and \\(\\beta, \\beta_{1}, \\beta_{2}\\) and the weight decay parameter \\(\\lambda\\), and the optimizers should be chosen based on trial and error.</p> <p>The LR (the stepsize \\(\\alpha\\) ) of different optimizers are not really comparable between the different optimizers. When you change the optimizer, the LR should be tuned again.</p> <p>Roughly, large stepsize, large momentum, small weight decay is faster but less stable, while small stepsize, small momentum, and large weight decay is slower but more stable.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#using-different-optimizers-in-pytorch","title":"Using different optimizers in PyTorch","text":"<p>In PyTorch, the torch. optim module implements the commonly used optimizers.</p> <p>Using SGD: torch.optim.SGD(model.parameters(), lr=X) Using SGD with momentum: torch.optim.SGD(model.parameters(), momentum=0.9, lr=X) Using RMSprop: torch.optim.RMSprop(model.parameters(), lr=X) Using Adam:</p> <pre><code>torch.optim.Adam(model.parameters(), lr=X)\n</code></pre> <p>Exercise: Try Homework 3 problem 1 with Adam but without the custom weight initialization.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#learning-rate-scheduler","title":"Learning rate scheduler","text":"<p>Sometimes, it is helpful to change (usually reduce) the learning rate as the training progresses. PyTorch provides learning rate schedulers to do this.</p> <pre><code>optimizer = SGD(model.parameters(), lr=0.1)\nscheduler = ExponentialLR(optimizer, gamma=0.9) # lr = 0.9*lr\nfor _ in range(...):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler.step() # .step() call updates (changes) the learning rate\n</code></pre>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#diminishing-learning-rate","title":"Diminishing learning rate","text":"<p>One common choice is to specify a diminishing learning rate via a function (a lambda expression). Choices like C/epoch or \\(\\mathrm{C} /\\) sqrt(iteration), where C is an appropriately chosen constant, are common.</p> <pre><code># lr_lambda allows us to set lr with a function\nscheduler = LambdaLR(optimizer, lr_lambda = lambda ep: 1e-2/ep)\nfor epoch in range(...):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler.step() # lr=0.01/epoch\n</code></pre>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#cosine-learning-rate","title":"Cosine learning rate","text":"<p>The cosine learning rate scheduler, which sets the learning rate with the cosine function, is also commonly used.</p> <p>The \\(2^{\\text {nd }}\\) case in the specification means \\(k\\) and its purpose is to prevent the learning rate from becoming 0 . It is also common to use only a half-period of the cosine rather than having the learning rate oscillate.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#cosineannealinglr","title":"COSINEANNEALINGLR","text":"\\[ \\begin{aligned} &amp; \\text { CLASS torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, } \\\\ &amp; \\text { last_epoch=- 1, verbose=False) [SOURCE] } \\\\ &amp; \\text { Set the learning rate of each parameter group using a cosine annealing schedule, where } \\eta_{\\max } \\text { is set to the } \\\\ &amp; \\text { initial Ir and } T_{\\text {cur }} \\text { is the number of epochs since the last restart in SGDR: } \\\\ &amp; \\eta_{t}=\\eta_{\\min }+\\frac{1}{2}\\left(\\eta_{\\max }-\\eta_{\\min }\\right)\\left(1+\\cos \\left(\\frac{T_{\\text {cur }}}{T_{\\max }} \\pi\\right)\\right), \\quad T_{\\text {cur }} \\neq(2 k+1) T_{\\max } ; \\\\ &amp; \\eta_{t+1}=\\eta_{t}+\\frac{1}{2}\\left(\\eta_{\\max }-\\eta_{\\min }\\right)\\left(1-\\cos \\left(\\frac{1}{T_{\\max }} \\pi\\right)\\right), \\quad T_{\\text {cur }}=(2 k+1) T_{\\max } . \\\\ &amp; \\text { When last_epoch=-1, sets initial Ir as Ir. Notice that because the schedule is defined recursively, the learning } \\\\ &amp; \\text { rate can be simultaneously modified outside this scheduler by other operators. If the learning rate is set solely } \\\\ &amp; \\text { by this scheduler, the learning rate at each step becomes: } \\\\ &amp; \\eta_{t}=\\eta_{\\text {min }}+\\frac{1}{2}\\left(\\eta_{\\text {max }}-\\eta_{\\text {min }}\\right)\\left(1+\\cos \\left(\\frac{T_{\\text {cur }}}{T_{\\text {max }}} \\pi\\right)\\right) \\\\ &amp; \\text { CLASS torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, } \\\\ &amp; \\text { last_epoch=- 1, verbose=False) [SOURCE] } \\\\ &amp; \\text { by this scheduler, the learning rate at each step becomes: } \\\\ &amp; \\text { It has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts. Note that this only } \\\\ &amp; \\text { implements the cosine annealing part of SGDR, and not the restarts. } \\\\ &amp; \\text { nen } \\end{aligned} \\] <p>a</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#wide-vs-sharp-minima","title":"Wide vs. sharp minima","text":"<p>As alluded to in hw1:</p> <ul> <li>Large step makes large and rough progress towards regions with small loss.</li> <li>Small steps refines the model by finding sharper minima.</li> </ul> <p>Also small steps better suppress the effect of noise. Mathematically, one can show that SGD with small steps becomes very similar to GD with small steps.#</p> <p>However, using small steps to converge to sharp minima may not always be optimal. There is some empirical evidence that wide minima have better test error than sharp minima.*</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#weight-initialization_2","title":"Weight initialization","text":"<p>Remember, SGD is</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha g^{k} \\] <p>where \\(\\theta^{0} \\in \\mathbb{R}^{p}\\) is an initial point. Using a good initial point is important in NN training. Prescription by LeCun et al.: \"Weights should be chosen randomly but in such a way that the [tanh] is primarily activated in its linear region. If weights are all very large then the [tanh] will saturate resulting in small gradients that make learning slow. If weights are very small then gradients will also be very small.\" (Cf. Vanishing gradient homework problem.) \"Intermediate weights that range over the [tanh's] linear region have the advantage that (1) the gradients are large enough that learning can proceed and (2) the network will learn the linear part of the mapping before the more difficult nonlinear part.\"</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#quick-math-review","title":"Quick math review","text":"<p>Using the \\(1^{\\text {st }}\\) order Taylor approximation,</p> \\[ \\tanh (z) \\approx z \\] <p>Write \\(X \\sim \\mathcal{N}\\left(\\mu, \\sigma^{2}\\right)\\) to denote that \\(X\\) is a Gaussian (normal) random variable with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).</p> <p>If \\(X\\) and \\(Y\\) are independent mean-zero random variables, then</p> \\[ \\begin{gathered} \\mathbb{E}[X Y]=0 \\\\ \\operatorname{Var}(X Y)=\\operatorname{Var}(X) \\operatorname{Var}(Y) \\end{gathered} \\] <p>If \\(X\\) and \\(Y\\) are uncorrelated, i.e., if \\(\\mathbb{E}\\left[\\left(X-\\mu_{X}\\right)\\left(Y-\\mu_{Y}\\right)\\right]=0\\), then \\(\\operatorname{Var}(X+Y)=\\operatorname{Var}(X)+\\) \\(\\operatorname{Var}(Y)\\). (Uncorrelated R.V. need not be independent.)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#weight-initialization_3","title":"Weight initialization","text":"<p>Consider </p> <p>If \\(w_{i} \\sim \\mathcal{N}\\left(0, \\sigma^{2}\\right)\\) (zero-mean variance \\(\\sigma^{2}\\) Gaussian) then \\(\\operatorname{Var}(\\mathrm{z})=3 \\sigma^{2}\\). If \\(\\sigma=\\frac{1}{\\sqrt{3}}\\), then \\(\\operatorname{Var}(\\mathrm{z})=1\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#lecun-initialization","title":"LeCun initialization","text":"<p>Consider the layer</p> \\[ \\begin{gathered} y=\\tanh (\\tilde{y}) \\\\ \\tilde{y}=A x+b \\end{gathered} \\] <p>where \\(x \\in \\mathbb{R}^{n_{\\text {in }}}\\) and \\(y, \\tilde{y} \\in \\mathbb{R}^{n_{\\text {out }}}\\). Assume \\(x_{j}\\) have mean \\(=0\\) variance \\(=1\\) and are uncorrelated. If we initialize \\(A_{i j} \\sim \\mathcal{N}\\left(0, \\sigma_{A}^{2}\\right)\\) and \\(b_{i} \\sim \\mathcal{N}\\left(0, \\sigma_{b}^{2}\\right)\\), IID, then</p> \\[ \\begin{aligned} &amp; \\tilde{y}_{i}=\\sum_{j=1}^{n_{\\mathrm{in}}} A_{i j} x_{j}+b_{i} \\quad \\text { has mean }=0 \\text { variance }=n_{\\mathrm{in}} \\sigma_{A}^{2}+\\sigma_{b}^{2} \\\\ &amp; y_{i}=\\tanh \\left(\\tilde{y}_{i}\\right) \\approx \\tilde{y}_{i} \\quad \\text { has mean } \\approx 0 \\text { variance } \\approx n_{\\mathrm{in}} \\sigma_{A}^{2}+\\sigma_{b}^{2} \\end{aligned} \\] <p>If we choose</p> \\[ \\sigma_{A}^{2}=\\frac{1}{n_{\\text {in }}}, \\quad \\sigma_{b}^{2}=0, \\] <p>(so \\(b=0\\) ) then we have \\(y_{i}\\) mean \\(\\approx 0\\) variance \\(\\approx 1\\) and are uncorrelated.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#lecun-initialization_1","title":"LeCun initialization","text":"<p>By induction, with an \\(L\\)-layer MLP,</p> <ul> <li>if the input to has mean \\(=0\\) variance \\(=1\\) and uncorrelated elements,</li> <li>the weights and biases are initialized with \\(A_{i j} \\sim \\mathcal{N}\\left(0, \\frac{1}{n_{\\text {in }}}\\right)\\) and \\(b_{i}=0\\), and</li> <li>the linear approximations \\(\\tanh (z) \\approx z\\) are valid, then we can expect the output layer to have mean \\(\\approx 0\\) variance \\(\\approx 1\\).</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#xavier-initialization","title":"Xavier initialization","text":"<p>Consider the layer</p> \\[ \\begin{gathered} y=\\tanh (\\tilde{y}) \\\\ \\tilde{y}=A x+b \\end{gathered} \\] <p>where \\(x \\in \\mathbb{R}^{n_{\\text {in }}}\\) and \\(y, \\tilde{y} \\in \\mathbb{R}^{n_{\\text {out }}}\\). Consider the gradient with respect to some loss \\(\\ell(y)\\). Assume \\(\\left(\\frac{\\partial \\ell}{\\partial y}\\right)_{i}\\) have mean \\(=0\\) variance \\(=1\\) and are uncorrelated. Then</p> \\[ \\frac{\\partial y}{\\partial x}=\\operatorname{diag}\\left(\\tanh ^{\\prime}(A x+b)\\right) A \\approx A \\] <p>if \\(\\tanh (\\tilde{y}) \\approx \\tilde{y}\\) and</p> \\[ \\frac{\\partial \\ell}{\\partial x}=\\frac{\\partial \\ell}{\\partial y} A \\] <p>If we initialize \\(A_{i j} \\sim \\mathcal{N}\\left(0, \\sigma_{A}^{2}\\right)\\) and \\(b_{i} \\sim \\mathcal{N}\\left(0, \\sigma_{b}^{2}\\right)\\), IID, and assume that \\(\\frac{\\partial \\ell}{\\partial y}\\) and \\(A\\) are independent \\({ }^{*}\\), then</p> \\[ \\left(\\frac{\\partial \\ell}{\\partial x}\\right)_{j}=\\sum_{i=1}^{n_{\\text {out }}}\\left(\\frac{\\partial \\ell}{\\partial y}\\right)_{i} A_{i j} \\text { has mean } \\approx 0 \\text { and variance } \\approx n_{\\text {out }} \\sigma_{A}^{2} \\] <p>If we choose</p> \\[ \\sigma_{A}^{2}=\\frac{1}{n_{\\mathrm{out}}} \\] <p>then \\(\\left(\\frac{\\partial \\ell}{\\partial x}\\right)_{j}\\) have mean \\(\\approx 0\\) variance \\(\\approx 1\\) and are uncorrelated.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#xavier-initialization_1","title":"Xavier initialization","text":"<p>\\(\\frac{\\partial \\ell}{\\partial y}\\) and \\(A\\) are not independent; \\(\\frac{\\partial \\ell}{\\partial y}\\) depends on the forward evaluation, which in turn depends on \\(A\\). Nevertheless, the calculation is an informative exercise and its result seems to be representative of common behavior.</p> <p>If \\(y=\\tanh (A x+b)\\) is an early layer (close to input) in a deep neural network, then the randomness of \\(A\\) is diluted through the forward and backward propagation and \\(\\frac{\\partial \\ell}{\\partial y}\\) and \\(A\\) will be nearly independent.</p> <p>If \\(y=\\tanh (A x+b)\\) is an later layer (close to output) in a deep neural network, then \\(\\frac{\\partial \\ell}{\\partial y}\\) and \\(A\\) will have strong dependency.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#xavier-initialization_2","title":"Xavier initialization","text":"<p>Consideration of forward and backward passes result in different prescriptions. The Xavier initialization uses the harmonic mean of the two:</p> \\[ \\sigma_{A}^{2}=\\frac{2}{n_{\\mathrm{in}}+n_{\\mathrm{out}}}, \\quad \\sigma_{b}^{2}=0 \\] <p>In the literature, the alternate notation fan \\(_{\\text {in }}\\) and fan out \\(_{\\text {out }}\\) are often used instead of \\(n_{\\text {in }}\\) and \\(n_{\\text {out }}\\). The fan-in and fan-out terminology originally refers to the number of electric connections entering and exiting a circuit or an electronic device.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#kaiming-he-initialization","title":"(Kaiming) He initialization","text":"<p>Consider the layer</p> \\[ y=\\operatorname{ReLU}(A x+b) \\] <p>We cannot use the Taylor expansion with ReLU.</p> <p>However, a similar line of reasoning with the forward pass gives rise to</p> \\[ \\sigma_{A}^{2}=\\frac{2}{n_{\\mathrm{in}}} \\] <p>And a similar consideration with backprop gives rise to</p> \\[ \\sigma_{A}^{2}=\\frac{2}{n_{\\text {out }}} \\] <p>In PyTorch, use mode='fan_in' and mode='fan_out' to toggle between the two modes.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#discussions-on-initializations","title":"Discussions on initializations","text":"<p>In the original description of the Xavier and He initializations, the biases are all initialized to 0 . However, the default initialization of Linear* and Conv2d \\({ }^{\\#}\\) layers in PyTorch uses initialize the biases randomly. A documented reasoning behind this choice (in the form of papers or GitHub discussions) do not seem to exist.</p> <p>Initializing weights with the proper scaling is sometimes necessary to get the network to train, as you will see with the VGG network. However, so long as the network gets trained, the choice of initialization does not seem to affect the final performance.</p> <p>Since initializations rely on the assumption that the input to each layer has roughly unit variance, it is important that the data is scaled properly. This is why PyTorch dataloader scales pixel intensity values to be in \\([0,1]\\), rather than \\([0,255]\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#initialization-for-conv","title":"Initialization for conv","text":"<p>Consider the layer</p> \\[ \\begin{aligned} &amp; y=\\tanh (\\tilde{y}) \\\\ &amp; \\tilde{y}=\\operatorname{Conv} 2 \\mathrm{D}_{w, b}(x) \\end{aligned} \\] <p>where \\(w \\in \\mathbb{R}^{C_{\\text {out }} \\times C_{\\text {in }} \\times f_{1} \\times f_{2}}\\) and \\(b \\in \\mathbb{R}^{C_{\\text {out }}}\\). Assume \\(x_{j}\\) have mean \\(=0\\) variance \\(=1\\) and are uncorrelated*. If we initialize \\(w_{i j k \\ell} \\sim \\mathcal{N}\\left(0, \\sigma_{w}^{2}\\right)\\) and \\(b_{i} \\sim \\mathcal{N}\\left(0, \\sigma_{b}^{2}\\right)\\), IID, then</p> \\[ \\begin{aligned} &amp; \\tilde{y}_{i} \\quad \\text { has mean }=0 \\text { variance }=\\left(C_{\\text {in }} f_{1} f_{2}\\right) \\sigma_{w}^{2}+\\sigma_{b}^{2} \\\\ &amp; y_{i} \\approx \\tilde{y}_{i} \\text { has mean } \\approx 0 \\text { variance } \\approx\\left(C_{\\text {in }} f_{1} f_{2}\\right) \\sigma_{w}^{2}+\\sigma_{b}^{2} \\end{aligned} \\] <p>If we choose</p> \\[ \\sigma_{w}^{2}=\\frac{1}{c_{\\text {in }} f_{1} f_{2}}, \\quad \\sigma_{b}^{2}=0 \\] <p>(so \\(b=0\\) ) then we have \\(y_{i}\\) mean \\(\\approx 0\\) variance \\(\\approx 1\\) and are correlated.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#initialization-for-conv_1","title":"Initialization for conv","text":"<p>Outputs from a convolutional layer are correlated. The uncorrelated assumption is false. Nevertheless, the calculation is an informative exercise and its result seems to be representative of common behavior.</p> <p>Xavier and He initialization is usually used with</p> \\[ n_{\\mathrm{in}}=C_{\\mathrm{in}} f_{1} f_{2} \\] <p>and</p> \\[ n_{\\text {out }}=C_{\\text {out }} f_{1} f_{2} \\] <p>Justification of \\(n_{\\text {out }}=C_{\\text {out }} f_{1} f_{2}\\) requires working through the complex indexing or considering the \"transpose convolution\". We will return to it later.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#imagenet-after-alexnet","title":"ImageNet after AlexNet","text":"<p>AlexNet won the 2012 ImageNet challenge with 8 layers. ZFNet won the 2013 ImageNet challenge also with 8 layers but with better parameter tuning.</p> <p>Research since AlexNet indicated that depth is more important than width. VGGNet ranked 2nd in the 2014 ImagNet challenge with 19 layers. GoogLeNet ranked 1st in the 2014 ImageNet challenge with 22 layers.</p> <p>VGG16</p> <ul> <li>16 layers with trainable parameters</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#vggnet","title":"VGGNet","text":"<ul> <li>\\(3 \\times 3\\) conv. \\(p=1\\) (spatial dimension preserved)</li> <li>No local response normalization</li> <li>Weight decay \\(5 \\times 10^{-4}\\)</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#by-the-oxford-visual-geometry-group","title":"By the Oxford Visual Geometry Group","text":"<ul> <li>Dropout(0.5) used</li> <li>Max pool \\(f=2, s=2\\)</li> <li>ReLU activation function (except after pool and FC1000)</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#vggnet_1","title":"VGGNet","text":"<p>VGG19</p> <ul> <li>19 layers with trainable parameters</li> <li>Slightly better than VGG16 </li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#vggnet-cifar10","title":"VGGNet-CIFAR10","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#13-layer-modification-of-vggnet-for-cifar10","title":"13-layer modification of VGGNet for CIFAR10","text":"<ul> <li>\\(3 \\times 3\\) conv. \\(p=1\\)</li> <li>Max pool \\(f=2, s=2\\) </li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#vggnet-training","title":"VGGNet training","text":"<p>Training VGGNet was tricky. A shallower version was first trained and then additional layers were gradually added.</p> <p>Our VGGNet-CIFAR10 is much easier to train since there are fewer layers and the task is simpler. However, good weight initialization is still necessary</p> <p>Batchnorm (not available when VGGNet was published) makes training VGGNet much easier. With Batchnorm, the complicated initialization scheme of training a smaller version first becomes unnecessary.</p> <p>PyTorch demo</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#architectural-contribution-vggnet","title":"Architectural contribution: VGGNet","text":"<p>Demonstrated simple deep CNNs can significantly improve upon AlexNet.</p> <p>In a sense, VGGNet represents the upper limit of the simple CNN architecture. (It is the best simple model.) Future architectures make gains through more complex constructions.</p> <p>Demonstrated effectiveness of stacked \\(3 \\times 3\\) convolutions over larger \\(5 \\times 5\\) or 11x11 convolutions. Large convolutions (larger than \\(5 \\times 5\\) ) are now uncommon.</p> <p>Due to its simplicity, VGGNet is one of the most common test subjects for testing something on deep CNNs.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#backprop-subseteq-autodiff","title":"Backprop \\(\\subseteq\\) autodiff","text":"<p>Autodiff (automatic differentiation) is an algorithm that automates gradient computation. In deep learning libraries, you only need to specify how to evaluate the function. Backprop (back propagation) is an instance of autodiff.</p> <p>Gradient computation costs roughly \\(5 \\times\\) the computation cost \\(^{*}\\) of forward evaluation.</p> <p>To clarify, backprop and autodiff are not</p> <ul> <li>finite difference or</li> <li>symbolic differentiation.</li> </ul> <p>Autodiff \\(\\approx\\) chain rule of vector calculus</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#autodiff-example","title":"Autodiff example","text":"<p>This complicated gradient computation is simplified by autodiff.</p> <p>PyTorch demo \\(\\operatorname{In}[\\cdot]:=\\mathrm{fn}=\\frac{\\operatorname{Sin}\\left[\\operatorname{Cosh}\\left[y^{2}+\\frac{x}{z}\\right]+\\operatorname{Tanh}[x y z]\\right]}{\\log [1+\\operatorname{Exp}[x]]}\\); \\(D[f n, x]\\) \\(\\% / .\\{x \\rightarrow 3.3, y \\rightarrow 1.1, z \\rightarrow 2.3\\} / / N\\) D[fn, \\(y\\) ] \\(\\% / .\\{x \\rightarrow 3.3, y \\rightarrow 1.1, z \\rightarrow 2.3\\} / / N\\) D[fn, z] \\(\\% / .\\{x \\rightarrow 3.3, y \\rightarrow 1.1, z \\rightarrow 2.3\\} / / N\\)</p> \\[ \\text { Out }[\\cdot]=-\\frac{e^{x} \\operatorname{Sin}\\left[\\operatorname{Cosh}\\left[y^{2}+\\frac{x}{z}\\right]+\\operatorname{Tanh}[x y z]\\right]}{\\left(1+e^{x}\\right) \\log \\left[1+\\mathbb{e}^{x}\\right]^{2}}+\\frac{\\operatorname{Cos}\\left[\\operatorname{Cosh}\\left[y^{2}+\\frac{x}{z}\\right]+\\operatorname{Tanh}[x y z]\\right]\\left(y z \\operatorname{Sech}[x y z]^{2}+\\frac{\\sinh \\left[y^{2}+\\frac{x}{z}\\right]}{z}\\right)}{\\log \\left[1+\\mathbb{e}^{x}\\right]} \\] <p>Out \\([0]=-0.285274\\) \\(\\frac{\\operatorname{Cos}\\left[\\operatorname{Cosh}\\left[y^{2}+\\frac{x}{z}\\right]+\\operatorname{Tanh}[x y z]\\right]\\left(x z \\operatorname{Sech}[x y z]^{2}+2 y \\operatorname{Sinh}\\left[y^{2}+\\frac{x}{z}\\right]\\right)}{\\log \\left[1+\\mathbb{e}^{x}\\right]}\\) Out[ \\([0]=-1.01578\\) \\(\\frac{\\operatorname{Cos}\\left[\\operatorname{Cosh}\\left[y^{2}+\\frac{x}{z}\\right]+\\operatorname{Tanh}[x y z]\\right]\\left(x y \\operatorname{Sech}[x y z]^{2}-\\frac{x \\operatorname{Sinh}\\left[y^{2}+\\frac{x}{z}\\right]}{z^{2}}\\right)}{\\log \\left[1+\\mathbb{e}^{x}\\right]}\\) Out[ \\([=0.288027\\)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#the-power-of-autodiff","title":"The power of autodiff","text":"<p>Autodiff is an essential yet often an underappreciated feature of the deep learning libraries. It allows deep learning researchers to use complicated neural networks, while avoiding the burden of performing derivative calculations by hand.</p> <p>Most deep learning libraries support \\(2^{\\text {nd }}\\) and higher order derivative computation, but we will only use \\(1^{\\text {st }}\\) order derivatives (gradients) in this class.</p> <p>Autodiff includes forward-mode, reverse-mode (backprop), and other orders. In deep learning, reverse-mode is most commonly used.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#autodiff-by-jacobian-multiplication","title":"Autodiff by Jacobian multiplication","text":"<p>Consider \\(g=f_{L} \\circ f_{L-1} \\circ \\cdots \\circ f_{2} \\circ f_{1}\\), where \\(f_{\\ell}: \\mathbb{R}^{n_{\\ell-1}} \\rightarrow \\mathbb{R}^{n_{\\ell}}\\) for \\(\\ell=1, \\cdots, L\\).</p> <p>Chain rule: \\(D g=D f_{L} \\quad D f_{L-1} \\quad \\cdots \\quad D f_{2} \\quad D f_{1}\\)</p> \\[ n_{L} \\times n_{L-1} \\quad n_{L-1} \\times n_{L-2} \\quad n_{2} \\times n_{1} \\quad n_{1} \\times n_{0} \\] <p>Forward-mode: \\(D f_{L}\\left(D f_{L-1}\\left(\\cdots\\left(D f_{2} D f_{1}\\right) \\cdots\\right)\\right)\\)</p> <p>Reverse-mode: \\(\\left(\\left(\\left(D f_{L} D f_{L-1}\\right) D f_{L-2}\\right) \\cdots\\right) D f_{1}\\)</p> <p>Reverse mode is optimal \\({ }^{*}\\) when \\(n_{L} \\leq n_{L-1} \\leq \\cdots \\leq n_{1} \\leq n_{0}\\). The number of neurons in each layer tends to decrease in deep neural networks for classification. So reverse-mode is often close to the most efficient mode of autodiff in deep learning.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#general-backprop","title":"General backprop","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#backprop-in-pytorch","title":"Backprop in PyTorch:","text":"<ol> <li>When the loss function is evaluated, a computation graph is constructed.</li> <li>The computation graph is a directed acyclic graph (DAG) that encodes dependencies of the individual computational components.</li> <li>A topological sort is performed on the DAG and the backprop is performed on the reversed order of this topological sort. (The topological sort ensures that nodes ahead in the DAG are processed first.)</li> </ol> <p>The general form combines a graph theoretic formulation with the principles of backprop that you have seen in the homework assignments.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#computation-graph-example","title":"Computation graph example","text":"<p>Consider \\(f(x, y)=y \\log x+\\sqrt{y \\log x}\\). Evaluate \\(f\\) with the computation graph: </p> <p>The chain rule: \\(\\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial c} \\frac{\\partial c}{\\partial b}\\left(\\frac{\\partial b}{\\partial a} \\frac{\\partial a}{\\partial x} \\frac{\\partial x}{\\partial x}+\\frac{\\partial b}{\\partial y} \\frac{\\partial y}{\\partial x}\\right)+\\frac{\\partial f}{\\partial b}\\left(\\frac{\\partial b}{\\partial a} \\frac{\\partial a}{\\partial x} \\frac{\\partial x}{\\partial x}+\\frac{\\partial b}{\\partial y} \\frac{\\partial y}{\\partial x}\\right)\\)</p> \\[ \\frac{\\partial f}{\\partial y}=\\frac{\\partial f}{\\partial c} \\frac{\\partial c}{\\partial b}\\left(\\frac{\\partial b}{\\partial a} \\frac{\\partial a}{\\partial x} \\frac{\\partial x}{\\partial y}+\\frac{\\partial b}{\\partial y} \\frac{\\partial y}{\\partial y}\\right)+\\frac{\\partial f}{\\partial b}\\left(\\frac{\\partial b}{\\partial a} \\frac{\\partial a}{\\partial x} \\frac{\\partial x}{\\partial y}+\\frac{\\partial b}{\\partial y} \\frac{\\partial y}{\\partial y}\\right) \\] <p>But in what order do you evaluate the chain rule expression?</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#computation-graph","title":"Computation graph","text":"<p>Let \\(y_{1}, \\ldots, y_{L}\\) be the output values (neurons) of the computational nodes. Assume \\(y_{1}, \\ldots, y_{L}\\) follow a linear topological ordering, i.e., the computation of \\(y_{\\ell}\\) depends on \\(y_{1}, \\ldots, y_{\\ell-1}\\) and does not depend on \\(y_{\\ell+1}, \\ldots, y_{L}\\).</p> <p>Define the graph \\(G=(V, E)\\), where \\(V=\\{1, \\ldots, L\\}\\) and \\((i, \\ell) \\in E\\), i.e., \\(i \\rightarrow \\ell\\), if the computation of \\(y_{\\ell}\\) directly depends on \\(y_{i}\\). Write the computation of \\(y_{1}, \\ldots, y_{L}\\) as</p> \\[ y_{\\ell}=f_{\\ell}\\left(\\left[y_{i}: \\text { for } i \\rightarrow \\ell\\right]\\right) \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#forward-pass-on-computation-graph","title":"Forward pass on computation graph","text":"<p>In the forward pass, sequentially compute \\(y_{1}, \\ldots, y_{L}\\) via</p> \\[ y_{\\ell}=f_{\\ell}\\left(\\left[y_{i}: \\text { for } i \\rightarrow \\ell\\right]\\right) \\] <pre><code># Use 1-based indexing\n# y[1] given\nfor l = 2,...,L\n    inputs = [y[i] for j such that (i-&gt;l)]\n    y[l] = f[l].eval(inputs)\nend\n</code></pre>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#forward-mode-autodiff","title":"Forward-mode autodiff","text":"<p>\\(\\begin{array}{lllll}\\text { Step } 0 &amp; \\text { Step } 1 &amp; \\text { Step } 2 &amp; \\text { Step } 3 &amp; \\text { Step } 4\\end{array}\\)  0. \\(x=3, y=2, \\frac{\\partial x}{\\partial x}=1, \\frac{\\partial x}{\\partial y}=0, \\frac{\\partial y}{\\partial x}=0, \\frac{\\partial y}{\\partial y}=1\\)</p> <ol> <li>\\(a=\\log x=\\log 3, \\frac{\\partial a}{\\partial x}=\\frac{1}{x} \\cdot \\frac{\\partial x}{\\partial x}=\\frac{1}{3}, \\frac{\\partial a}{\\partial y}=0\\)</li> <li>\\(b=y a=2 \\log 3, \\frac{\\partial b}{\\partial x}=\\frac{\\partial y}{\\partial x} a+y \\frac{\\partial a}{\\partial x}=\\frac{2}{3}, \\frac{\\partial b}{\\partial y}=\\frac{\\partial y}{\\partial y} a+y \\frac{\\partial a}{\\partial y}=a=\\log 3\\)</li> <li>\\(c=\\sqrt{b}=\\sqrt{2 \\log 3}, \\frac{\\partial c}{\\partial x}=\\frac{1}{2 \\sqrt{b}} \\frac{\\partial b}{\\partial x}=\\frac{1}{3 \\sqrt{2 \\log 3}}, \\frac{\\partial c}{\\partial y}=\\frac{1}{\\sqrt{b}} \\frac{\\partial b}{\\partial y}=\\frac{1}{2} \\sqrt{\\frac{\\log 3}{2}} \\longleftarrow \\quad\\) Computation only depends on node b</li> <li>\\(f=c+b=\\sqrt{2 \\log 3}+2 \\log 3, \\frac{\\partial f}{\\partial x}=\\frac{\\partial c}{\\partial x}+\\frac{\\partial b}{\\partial x}=\\frac{1}{3}\\left(2+\\frac{1}{3 \\sqrt{2 \\log 3}}\\right), \\frac{\\partial f}{\\partial y}=\\frac{\\partial c}{\\partial y}+\\frac{\\partial b}{\\partial y}=\\frac{1}{2} \\sqrt{\\frac{\\log 3}{2}}+\\log 3\\)</li> </ol>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#backprop-on-computation-graph","title":"Backprop on computation graph","text":"<pre><code># Use 1-based indexing\n# y[1],...,y[L] already computed\ng[:] = 0 // .zero_grad()\ng[L] = 1 // dy[L]/dy[L]=1\nfor l = L,...,2\n    for i such that (i-&gt;l)\n        g[i] += g[l]*f[l].grad(i)\n    end\nend\n</code></pre> <p>To perform backprop#, use</p> \\[ \\frac{\\partial y_{L}}{\\partial y_{i}}=\\sum_{\\ell: i \\rightarrow \\ell} \\frac{\\partial y_{L}}{\\partial y_{\\ell}} \\frac{\\partial f_{\\ell}}{\\partial y_{i}} \\] <p>to sequentially compute \\(\\frac{\\partial y_{L}}{\\partial y_{L}}, \\frac{\\partial y_{L}}{\\partial y_{L-1}}, \\ldots, \\frac{\\partial y_{L}}{\\partial y_{1}}\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#reverse-mode-autodiff-backprop","title":"Reverse-mode autodiff (backprop)","text":"<p> 0. \\(x=3, y=2\\)</p> <ol> <li>\\(a=\\log 3\\)</li> <li>\\(b=2 \\log 3\\)</li> <li>\\(c=\\sqrt{2 \\log 3}\\)</li> <li>\\(f=\\sqrt{2 \\log 3}+2 \\log 3\\) \\(0^{\\prime} \\cdot \\frac{\\partial f}{\\partial f}=1\\) 1'. \\(\\frac{\\partial f}{\\partial c}=\\frac{\\partial f}{\\partial f} \\frac{\\partial f}{\\partial c}=\\frac{\\partial f}{\\partial f} 1=1\\) 2'. \\(\\frac{\\partial f}{\\partial b}=\\frac{\\partial f}{\\partial c} \\frac{\\partial c}{\\partial b}+\\frac{\\partial f}{\\partial f} \\frac{\\partial f}{\\partial c}=\\frac{1}{2 \\sqrt{b}} 1+1=\\frac{1}{2 \\sqrt{2 \\log 3}}+1\\) 3'. \\(\\frac{\\partial f}{\\partial a}=\\frac{\\partial f}{\\partial b} \\frac{\\partial b}{\\partial a}=\\frac{\\partial f}{\\partial b} y=2+\\frac{1}{\\sqrt{2 \\log 3}}\\) 4'. \\(\\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial a} \\frac{\\partial a}{\\partial x}=\\frac{\\partial f}{\\partial a} \\frac{1}{x}=\\frac{1}{3}\\left(2+\\frac{1}{\\sqrt{2 \\log 3}}\\right)\\) \\(\\frac{\\partial f}{\\partial y}=\\frac{\\partial f}{\\partial b} \\frac{\\partial b}{\\partial y}=\\frac{\\partial f}{\\partial b} a=\\frac{1}{2} \\sqrt{\\frac{\\log 3}{2}}+\\log 3\\)</li> </ol>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#backprop-in-pytorch_1","title":"Backprop in PyTorch","text":"<p>In NN training, parameters and fixed inputs are distinguished. In PyTorch, you (1) clear the existing gradient with .zero_grad() (2) forward-evaluate the loss function by providing the input and label and (3) perform backprop with . backward().</p> <p>The forward pass stores the intermediate neuron values so that they can later be used in backprop. In the test loop, however, we don't compute gradients so the intermediate neuron values are unnecessary. The torch. no_grad() context manager allows intermediate node values to discarded or not be stored. This saves memory and can accelerate the test loop.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#linear-layers-have-too-may-parameters","title":"Linear layers have too may parameters","text":"<p>AlexNet: Conv layer params: 2,469,696 (4\\%) Linear layer params: 58,631,144 (96\\%) Total params: 61,100,840 </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#linear-layers-have-too-may-parameters_1","title":"Linear layers have too may parameters","text":"<p>VGG19: Conv layer params: 20,024,384 (14\\%) Linear layer params: 123,642,856 (86\\%) Total params: 143,667,240  fc8</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#network-in-network-nin-network","title":"Network in Network (NiN) Network","text":"<p>NiN for CIFAR10.</p> <ul> <li>Remove linear layers to reduce parameters. Use global average pool instead.</li> <li>Weight decay \\(1 \\times 10^{-5}\\).</li> <li>Dropout(0.5). (dropout after pool is not consistent with modern practice.)</li> <li>Maxpool \\(f=3, s=2\\). Use ceil_mode=True so that \\(\\frac{32-3}{2}+1=15.5\\) is rounded up to 16 . Default behavior of PyTorch is to round down. </li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#1x1-convolution","title":"1x1 convolution","text":"<p>A \\(1 \\times 1\\) convolution is like a fully connected layer acting independently and identically on each spatial location. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#192x32x32-96x32x32","title":"192x32x32  96x32x32","text":"<ul> <li>96 filters act on 192 channels separately for each pixel</li> <li>\\(96 \\times 192+96\\) parameters for weights and biases</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#regular-conv-layer","title":"Regular conv. layer","text":"<p>Input: \\(X \\in \\mathbb{R}^{C_{0} \\times m \\times n}\\)</p> <ul> <li>Select an \\(f \\times f\\) patch \\(\\tilde{X}=X[:, i: i+f, j: j+f]\\).</li> <li>Inner product \\(\\tilde{X}\\) and \\(w_{1}, \\ldots, w_{C_{1}} \\in \\mathbb{R}^{C_{0} \\times f \\times f}\\) and add bias \\(b_{1} \\in \\mathbb{R}^{C_{1}}\\).</li> <li>Apply \\(\\sigma\\). (Output in \\(\\mathbb{R}^{C_{1}}\\).)</li> </ul> <p>Repeat this for all patches. Output in \\(X \\in \\mathbb{R}^{C_{1} \\times(m-f+1) \\times(n-f+1)}\\). Repeat this for all batch elements.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#network-in-network","title":"\"Network in Network\"","text":"<p>Input: \\(X \\in \\mathbb{R}^{c_{0} \\times m \\times n}\\)</p> <ul> <li>Select an \\(f \\times f\\) patch \\(\\tilde{X}=X[i: i+f, j: j+f]\\).</li> <li>Inner product \\(\\tilde{X}\\) and \\(w_{1}, \\ldots, w_{C_{1}} \\in \\mathbb{R}^{C_{0} \\times f \\times f}\\) and add bias \\(b_{1} \\in \\mathbb{R}^{C_{1}}\\).</li> <li>Apply \\(\\sigma\\). (Output in \\(\\mathbb{R}^{C_{1}}\\).)</li> <li>Apply Linear \\(A_{A_{2}, b_{2}}(x)\\) where \\(A_{2} \\in \\mathbb{R}^{C_{2} \\times C_{1}}\\) and \\(b_{2} \\in \\mathbb{R}^{C_{2}}\\).</li> <li>Apply \\(\\sigma\\). (Output in \\(\\mathbb{R}^{C_{2}}\\).)</li> <li>Apply Linear \\(A_{A_{3}, b_{3}}(x)\\) where \\(A_{3} \\in \\mathbb{R}^{C_{3} \\times C_{2}}\\) and \\(b_{3} \\in \\mathbb{R}^{C_{3}}\\).</li> <li>Apply \\(\\sigma\\). (Output in \\(\\mathbb{R}^{C_{3}}\\).)</li> </ul> <p>Repeat this for all patches. Output in \\(X \\in \\mathbb{R}^{C_{3} \\times(m-f+1) \\times(n-f+1)}\\). Repeat this for all batch elements. Why is this equivalent to ( \\(3 \\times 3\\) conv)-( \\(1 \\times 1\\) conv)-( \\(1 \\times 1\\) conv)?</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#global-average-pool","title":"Global average pool","text":"<p>When using CNNs for classification, position of object is not important.</p> <p>The global average pool has no trainable parameters (linear layers have many) and it is translation invariant. Global average pool removes the spatial dependency. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#architectural-contribution-nin-network","title":"Architectural contribution: NiN Network","text":"<p>Used \\(1 \\times 1\\) convolutions to increase the representation power of the convolutional modules.</p> <p>Replaced linear layer with average pool to reduce number of trainable parameters.</p> <p>First step in the trend of architectures becoming more abstract. Modern CNNs are built with smaller building blocks.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#googlenet-inception-v1","title":"GoogLeNet (Inception v1)","text":"<p>Utilizes the inception module. Structure inspired by NiN and name inspired by 2010 Inception movie meme.</p> <p>Used \\(1 \\times 1\\) convolutions.</p> <ul> <li>Increased depth adds representation power (improves ability to represent nonlinear functions).</li> <li>Reduce the number of channels before the expensive \\(3 \\times 3\\) and \\(5 \\times 5\\) convolutions, and thereby reduce number of trainable weights and computation time. (Cf. hw5)</li> </ul> <p>The name GoogLeNet is a reference to the authors' Google affiliation and is an homage to LeNet. </p> <p>Inception module  </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#googlenet","title":"GoogLeNet","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#cl","title":"Cl! ! ! ! !","text":"<p>Two auxiliary classifiers used to slightly improve training. No longer necessary with batch norm.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#googlenet-for-cifar10","title":"GoogLeNet for CIFAR10","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#1024x7x7-1024x1x1","title":"1024x7x7 1024x1x1","text":"\\(2 \\mathbf{x}\\) \\(k=256480\\) \\(k=64\\) \\(k=128\\) 96128 128256 1632 2464 32 64 512 512 \\(k=192\\) 512 528 832 \\(96=160\\) \\(k=128\\) \\(k=112\\) \\(k=256\\) 96 208112 224128 256144 288160 320 1648 2464 2464 3264 32 128 64 64 64 64 128 \\(\\mathbf{2 x}\\) 832 1024 \\(k=256\\) \\(k=384\\) 160320 192384 32128 48128 128 128"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#architectural-contribution-googlenet","title":"Architectural contribution: GoogLeNet","text":"<p>Demonstrated that more complex modular neural network designs can outperform VGGNet's straightforward design.</p> <p>Together with VGGNet, demonstrated the importance of depth.</p> <p>Kickstarted the research into deep neural network architecture design.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#batch-normalization","title":"Batch normalization","text":"<p>The first step of many data processing algorithms is often to normalize data to have zero mean and unit variance.</p> <ul> <li>Step 1. Compute \\(\\hat{\\mu}=\\frac{1}{N} \\sum_{i=1}^{N} X_{i}, \\widehat{\\sigma^{2}}=\\frac{1}{N} \\sum_{i=1}^{N}\\left(X_{i}-\\hat{\\mu}\\right)^{2}\\)</li> </ul> \\[ \\hat{X}_{i}=\\frac{X_{i}-\\widehat{\\mu}}{\\sqrt{\\sigma^{2}}+\\varepsilon} \\] <ul> <li>Step 2. Run method with data \\(\\hat{X}_{1}, \\ldots, \\hat{X}_{N}\\)</li> </ul> <p>Batch normalization (BN) (sort of) enforces this normalization layer-by-layer. BN is an indispensable tool for training very deep neural networks. Theoretical justification is weak.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#bn-for-linear-layers","title":"BN for linear layers","text":"<p>Underlying assumption: Each element of the batch is an IID sample. Input: \\(X\\) (batch size) \\(\\times(\\#\\) entries) output: \\(\\mathrm{BN}_{\\beta, \\gamma}(X)\\). shape \\(\\left(\\mathrm{BN}_{\\beta, \\gamma}(X)\\right)=\\operatorname{shape}(X)\\) \\(\\mathrm{BN}_{\\beta, \\gamma}\\) for linear layers acts independently over neurons.</p> \\[ \\begin{gathered} \\hat{\\mu}[:]=\\frac{1}{B} \\sum_{b=1}^{B} X[b,:] \\quad \\text { FC } \\\\ \\hat{\\sigma}^{2}[:]=\\frac{1}{B} \\sum_{b=1}^{B}(X[b,:]-\\hat{\\mu}[:])^{2} \\\\ \\mathrm{BN}_{\\gamma, \\beta}(X)[b,:]=\\gamma[:] \\frac{X[b,:]-\\hat{\\mu}[:]}{\\sqrt{\\hat{\\sigma}^{2}[:]+\\varepsilon}}+\\beta[:] \\quad b=1, \\ldots, B \\end{gathered} \\] <p></p> <p>Batch Norm \\(\\beta, \\gamma\\) where operations are elementwise. BN normalizes each output neuron. The mean and variance are explicitly controlled through learned parameters \\(\\beta\\) and \\(\\gamma\\). In Pytorch, nn.BatchNorm1d.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#bn-for-convolutional-layers","title":"BN for convolutional layers","text":"<p>Underlying assumption: Each element of the batch, horizontal pixel, and vertical pixel is an IID sample.* Input: \\(X\\) (batch size \\() \\times(\\) channels \\() \\times(\\) vertical dim \\() \\times(\\) horizontal dim \\()\\) output: \\(\\mathrm{BN}_{\\beta, \\gamma}(X)\\). shape \\(\\left(\\mathrm{BN}_{\\beta, \\gamma}(X)\\right)=\\operatorname{shape}(X)\\) \\(\\mathrm{BN}_{\\beta, \\gamma}\\) for conv. layers acts independently over channels.</p> \\[ \\begin{gathered} \\hat{\\mu}[:]=\\frac{1}{B P Q} \\sum_{b=1}^{B} \\sum_{i=1}^{P} \\sum_{j=1}^{Q} X[b,:, i, j] \\\\ \\hat{\\sigma}^{2}[:]=\\frac{1}{B P Q} \\sum_{b=1}^{B} \\sum_{i=1}^{P} \\sum_{j=1}^{Q}(X[b,:, i, j]-\\hat{\\mu}[:])^{2} \\\\ \\operatorname{BN}_{\\gamma, \\beta}(X)[b,:, i, j]=\\gamma[:] \\frac{X[b,:, i, j]-\\hat{\\mu}[:]}{\\sqrt{\\hat{\\sigma}^{2}[:]+\\varepsilon}}+\\beta[:] \\quad \\begin{array}{l} b=1, \\ldots, B \\\\ i=1, \\ldots, P \\\\ j=1, \\ldots, Q \\end{array} \\end{gathered} \\] <p> Batch Norm</p> \\[ \\beta, \\gamma \\] <p>BN normalizes over each convolutional filter. The mean and variance are explicitly controlled through learned parameters \\(\\beta\\) and \\(\\gamma\\). In Pytorch, nn. BatchNorm2d.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#bn-during-testing","title":"BN during testing","text":"<p>\\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}\\) are estimated from batches during training. During testing, we don't update the NN, and we may only have a single input (so no batch). There are 2 strategies for computing final values of \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}\\) :</p> <ol> <li>After training, fix all parameters and evaluate NN on full training set to compute \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}\\) layer-by-layer. Store this computed value. (Computation of \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}\\) must be done sequentially layer-by-layer. Why?)</li> <li>During training, compute running average of \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}\\). This is the default behavior of PyTorch. In PyTorch, use model.train() and model.eval() to switch BN behavior between training and testing.</li> </ol>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#discussion-of-bn","title":"Discussion of BN","text":"<p>BN does not change the representation power of NN ; since \\(\\beta\\) and \\(\\gamma\\) are trained, the output of each layer can have any mean and variance. However, controlling the mean and variance as explicit trainable parameters makes training easier.</p> <p>With BN, the choice of batch size becomes a more important hyperparameter to tune.</p> <p>BN is indispensable in practice. Training of VGGNet and GoogLeNet becomes much easier with BN. Training of ResNet requires BN.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#bn-and-internal-covariate-shift","title":"BN and internal covariate shift","text":"<p>BN has insufficient theoretical justification. The original paper by loffe and Szegedy hypothesized that BN mitigates internal covariate shift (ICS), the shift in the mean and variance of the intermediate layer neurons throughout the training, and that this mitigation leads to improved training.</p> \\[ \\mathrm{BN} \\Rightarrow(\\text { reduced ICS }) \\Rightarrow \\text { (improved training }) \\] <p>However, Santukar et al. demonstrated that when experimentally measured, BN does not mitigate ICS, but nevertheless improves the training.</p> \\[ \\mathrm{BN} \\nRightarrow \\text { (reduced ICS) } \\] <p>Nevertheless</p> \\[ \\mathrm{BN} \\Rightarrow \\text { (improved training performance) } \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#bn-and-internal-covariate-shift_1","title":"BN and internal covariate shift","text":"<p>Santukar et al. argues that</p> \\[ \\mathrm{BN} \\Rightarrow \\text { (smoother loss landscape) } \\Rightarrow \\text { (improved training performance) } \\] <p>While this claim is more evidence-based than that of loffe and Szegedy, it is still not conclusive. It is also unclear why BN makes the loss landscape smoother, and it is not clear whether the smoother loss landscape fully explains the improved training performance.</p> <p>This story is a cautionary tale: we should carefully distinguish between speculative hypotheses and evidence-based claims, even in a primarily empirical subject.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#bn-has-trainable-parameters","title":"BN has trainable parameters","text":"<p>BN is usually not considered a trainable layer, much like pooling or dropout, and they are usually excluded when counting the \"depth\" of a NN. However, BN does have trainable parameters. Interestingly, if one randomly initializes a CNN, freezes all other parameters, and only train BN parameters, the performance is surprisingly good. </p> <p>Figure 2: Accuracy of ResNets for CIFAR-10 (top left, deep; top right, wide) and ImageNet (bottom left, top-1 accuracy; bottom right, top-5 accuracy) with different sets of parameters trainable.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#discussion-of-bn_1","title":"Discussion of BN","text":"<p>BN seems to also act as a regularizer, and for some reason subsumes effect Dropout. (Using dropout together with BN seems to worsen performance.) Since BN has been popularized, Dropout is used less often.*</p> <p>After training, functionality of BN can be absorbed into the previous layer when the previous layer is a linear layer or a conv layer. (Cf. homework 6.)</p> <p>The use of batch norm makes the scaling of weight initialization less important irrelevant.</p> <p>Use bias=false on layers preceding BN , since \\(\\beta\\) subsumes the bias.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#residual-network-resnet","title":"Residual Network (ResNet)","text":"<p>Winner of 2015 ImageNet Challenge Observation: Excluding the issue of computation cost, more layers it not always better   aeneric function classes  nested function classes</p> <p>Hypothesis 1: Deeper networks are harder to train. Is there a way to train a shallow network and embed it in a deeper network? Hypothesis 2: The deeper networks may be worse approximations of the true unknown function. Find an architecture representing a strictly increasing function class as a function of depth.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#residual-blocks","title":"Residual blocks","text":"<p>Use a residual connection so that [all weights=0] correspond to [block=identity] \\({ }^{*}\\) regular residual block  downsampling residual block </p> <p>Regular block must preserve spatial dimension and number of channels. Downsampling block halves the spatial dimension and changes the number of channels.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#resnet18","title":"ResNet18","text":"<p>Layer count excludes BN even though BN has trainable parameters.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#resnet34","title":"ResNet34","text":"<p>A trained ResNet18 architecture can be exactly fitted into a ResNet34: copy over the parameters and set parameters of the additional blocks to be 0 . The additional blocks with only serve to apply an additional ReLU, but this makes no difference as ReLU is idempotent.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#resnet-blocks-for-deeper-resnets","title":"ResNet blocks for deeper ResNets","text":"<p>ResNet in fact goes deeper. For the deeper variants, computation cost becomes more significant. To remedy this cost, use \\(1 \\times 1\\) conv to reduce number of channels, perform costly \\(3 \\times 3\\) convolution, and use \\(1 \\times 1\\) conv to restore the number of channels. This bottleneck\" structure is adapted from GoogLeNet. regular residual block with bottleneck  downsampling residual block with bottleneck </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#resnet50-101-152","title":"ResNet50, 101, 152","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#resnet18-for-cifar10","title":"ResNet18 for cifar10","text":"<p>ResNet{34,50,101,152} for CIFAR10. The intermediate layers are the same as before.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#resnet-v15","title":"ResNet v1.5","text":"<p>In the bottleneck blocks performing downsampling, the use of \\(1 \\times 1\\) conv with stride 2 is suboptimal as the operation simply ignores \\(75 \\%\\) of the neurons. ResNet v 1.5 replaces them with \\(3 \\times 3\\) conv with stride 2 . downsampling residual block v1  downsampling residual block v1.5 </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#resnet-v15_1","title":"ResNet v1.5","text":"<p>The fix is more important for the deeper downsampling residual blocks. downsampling residual block with bottleneck v1  downsampling residual block with bottleneck v1.5 </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#resnet-v2","title":"ResNet v2","text":"<p>Permutations of the ordering of conv, BN, and ReLU were tested. BN-ReLUconv had the best performance.</p> <p>Perform all operations before the residual connection so that the identity mapping can be learned. modifies residual block BN-ReLU-conv </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#architectural-contribution-resnet","title":"Architectural contribution: ResNet","text":"<p>Introduced residual connections as a key architectural component.</p> <p>Demonstrated that extremely deep neural networks can be trained with residual connections and BN. ResNet152 concluded the progression of depth. ImageNet challenge winners:</p> <ul> <li> <ol> <li>AlexNet with 8 layers.</li> </ol> </li> <li> <ol> <li>ZFNet with 8 layers.</li> </ol> </li> <li> <ol> <li>GoogLeNet with 22 layers.</li> </ol> </li> <li> <ol> <li>ResNet152 with 152 layers.</li> </ol> </li> <li> <ol> <li>Shao et al.. with 152 layers.</li> </ol> </li> <li> <ol> <li>SENet with 152 layers.</li> </ol> </li> </ul> <p>Residual connections and BN are very common throughout all of deep learning.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#resnext","title":"ResNext","text":"<p>2016 ImageNet challenge \\(2^{\\text {nd }}\\) place. Introduced cardinality as another network parameter, in addition to width (number of channels) and depth. Cardinality is the number of independent paths in the split-transform-merge structure.  S. Xie, R. Girshick, P. Doll\u00e1r, Z. Tu, and K. He, Aggregated residual transformations for deep neural networks, CVPR, 2017.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#resnext_1","title":"ResNext","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#equivalent","title":"equivalent","text":"<p> (a)  (b)</p> <p>Blocks (a) and (b) almost equivalent due to the by the following observation.</p> <p>Difference: Block (a) has 32 bias terms which are added to serve the role of the single bias term of block (b). </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#ensemble-learning","title":"Ensemble learning","text":"<p>Let \\((X, Y)\\) be a data-label pair. Let \\(m_{1}, \\ldots, m_{K}\\) be models estimating the \\(Y\\) given \\(X\\).</p> <p>An ensemble is a model</p> \\[ M=\\theta_{1} m_{1}+\\cdots+\\theta_{K} m_{K} \\] <p>where \\(\\theta_{1}, \\ldots, \\theta_{K} \\in \\mathbb{R}\\). Often \\(\\theta_{1}+\\cdots+\\theta_{K}=1\\) and \\(\\theta_{i} \\geq 0\\) for \\(i=1, \\ldots, K\\). (So \\(M\\) is often a nonnegative weighted average \\(m_{1}, \\ldots, m_{K}\\).)</p> <p>If \\(\\theta_{1}, \\ldots, \\theta_{K}\\) is chosen well, then</p> \\[ \\mathbb{E}_{(X, Y)}\\left[\\|M(X)-Y\\|^{2}\\right] \\leq \\min _{i=1, \\ldots, K} \\mathbb{E}_{(X, Y)}\\left[\\left\\|m_{i}(X)-Y\\right\\|^{2}\\right] \\] <p>(The ensemble can be worse if \\(\\theta_{1}, \\ldots, \\theta_{K}\\) is chosen poorly.)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#2016-imagenet-challenge-ensemble","title":"2016 ImageNet Challenge ensemble","text":"<p>Trimps-Soushen* won the 2016 ImageNet Challenge with an ensemble of</p> <ul> <li>Inception-v3[1]</li> <li>Inception-v4 \\({ }^{[2]}\\)</li> <li>Inception-Resnet-v2 \\({ }^{[2]}\\)</li> <li>ResNet-200[3]</li> <li>WRN-68-3 \\({ }^{[4]}\\) *J. Shao, X. Zhang, Z. Ding, Y. Zhao, Y. Chen, J. Zhou, W. Wang, L. Mei, and C. Hu, Trimps-Soushen, 2016. \\({ }^{[1]}\\) C. Szegedy, V. Vanhoucke, S. loffe, J. Shlens, and Z. Wojna, Rethinking the inception architecture for computer vision, CVPR, 2016. \\({ }^{[2]} \\mathrm{C}\\). Szegedy, S. loffe, V. Vanhoucke, and A. Alemi, Inception-v4, Inception-ResNet and the impact of residual connections on learning, AAAI, 2017. \\({ }^{[3]} \\mathrm{K} . \\mathrm{He}, \\mathrm{X}\\). Zhang, S. Ren, and J. Sun, Identity mappings in deep residual networks, ECCV, 2016. \\({ }^{[4]}\\) S. Zagoruyko and N. Komodakis, Wide residual networks, BMVC, 2016.</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#dropout-ensemble-interpretation","title":"Dropout ensemble interpretation","text":"<p>Let \\(m\\) be a model with dropout applied to \\(K\\) neurons. The there are \\(2^{K}\\) possible configurations, which we label \\(m_{1}, \\ldots, m_{2^{K}}\\). These models share weights.</p> <p>Dropout can be viewed as randomly selecting one of these models and updating it with an iteration of SGD.</p> <p>Turning off dropout at test time can be interpreted and making predictions with an ensemble of these \\(2^{K}\\), since each neuron is scaled so that the neuron value has the same expectation as when dropout is applied.</p> <p>However, this is not a very precise connection, and I am unsure as to how much to trust it.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#test-time-data-augmentation","title":"Test-time data augmentation","text":"<p>Test-time data augmentation is an ensemble technique to improve the prediction. (This is not a regularization or data augmentation technique)</p> <p>Given a single model \\(M\\) and input \\(X\\), make predictions with</p> \\[ \\frac{1}{K} \\sum_{i=1}^{K} M\\left(T_{i}(X)\\right) \\] <p>where \\(T_{1}, \\ldots, T_{K}\\) are random data augmentations.</p> <p>The original AlexNet paper uses test-time data augmentation with random crops and horizontal reflections: \"At test time, the network makes a prediction by extracting five ... patches ... as well as their horizontal reflections ..., and averaging the predictions made by the network's softmax layer on the ten patches.\" Most ImageNet classifiers use similar tricks.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#senet","title":"SENet","text":"<p>2017 ImageNet challenge \\(1^{\\text {st }}\\) place. Introduced the squeeze-and-excitation mechanism, which is referred to attention in more modern papers.</p> <p>Attention multiplicatively reweighs channels. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#squeeze-and-excitation","title":"Squeeze-and-excitation","text":"<p>Squeeze is a global average pool. Excitation is a bottleneck structure with \\(1 \\times 1\\) convolutions and outputs weights in \\((0,1)\\) by passing through sigmoid. Finally, scale each channel. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#conclusion","title":"Conclusion","text":"<p>We followed the ImageNet challenge from 2012 to 2017 and learned the foundations of the design and training of deep neural networks.</p> <p>With the advent of deep learning, research in computer vision shifted from \"feature engineering\" to \"network engineering\". Loosely speaking, the transition was from what to learn to learn to how to learn.</p> <p>A natural progression may be to continue studying the more recent neural network architectures, beyond the 2017 SENet. However, we will stop here to move on to learning about other machine learning tasks.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#chapter-4-cnns-for-other-supervised-learning-tasks","title":"Chapter 4:  CNNs for Other Supervised Learning Tasks","text":"<p>Mathematical Foundations of Deep Neural Networks Spring 2024 Department of Mathematical Sciences Ernest K. Ryu Seoul National University</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#inverse-problem-model","title":"Inverse problem model","text":"<p>In inverse problems, we wish to recover a signal \\(X_{\\text {true }}\\) given measurements \\(Y\\). The unknown and the measurements are related through</p> \\[ \\mathcal{A}\\left[X_{\\text {true }}\\right]+\\varepsilon=Y, \\] <p>where \\(\\mathcal{A}\\) is often, but not always, linear, and \\(\\varepsilon\\) represents small error.</p> <p>The forward model \\(\\mathcal{A}\\) may or may not be known. In other words, the goal of an inverse problem is to find an approximation of \\(\\mathcal{A}^{-1}\\).</p> <p>In many cases, \\(\\mathcal{A}\\) is not even be invertible. In such cases, we can still hope to find an mapping that serves as an approximate inverse in practice.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#gaussian-denoising","title":"Gaussian denoising","text":"<p>Given \\(X_{\\text {true }} \\in \\mathbb{R}^{w \\times h}\\), we measure</p> \\[ Y=X_{\\text {true }}+\\varepsilon \\] <p>where \\(\\varepsilon_{i j} \\sim \\mathcal{N}\\left(0, \\sigma^{2}\\right)\\) is IID Gaussian noise. For the sake of simplicity, assume we know \\(\\sigma\\). Goal is to recover \\(X_{\\text {true }}\\) from \\(Y\\).</p> <p>Guassian denoising is the simplest setup in which the goal is to remove noise from the image. In more realistic setups, the noise model will be more complicated and the noise level \\(\\sigma\\) will be unknown.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#dncnn","title":"DnCNN","text":"<p>In 2017, Zhang et al. presented the denoising convolutional neural networks (DnCNNs). They trained a 17-layer CNN \\(f_{\\theta}\\) to learn the noise with the loss</p> \\[ \\mathcal{L}(\\theta)=\\sum_{i=1}^{N}\\left\\|f_{\\theta}\\left(Y_{i}\\right)-\\left(Y_{i}-X_{i}\\right)\\right\\|^{2} \\] <p>so that the clean recovery can be obtained with \\(Y_{i}-f_{\\theta}\\left(Y_{i}\\right)\\). (This is equivalent to using a residual connection from beginning to end.) </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#dncnn_1","title":"DnCNN","text":"<p>Image denoising is was an area with a large body of prior work. DnCNN dominated all prior approaches that were not based on deep learning.</p> <p>Nowadays, all state-of-the-art denoising algorithms are based on deep learning. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#inverse-problems-via-deep-learning","title":"Inverse problems via deep learning","text":"<p>In deep learning, we use a neural network to approximate the inverse mapping</p> \\[ f_{\\theta} \\approx \\mathcal{A}^{-1} \\] <p>i.e., we want \\(f_{\\theta}(Y) \\approx X_{\\text {true }}\\) for the measurements \\(X\\) that we care about.</p> <p>If we have \\(X_{1}, \\ldots, X_{N}\\) and \\(Y_{1}, \\ldots, Y_{N}\\) (but no direct knowledge of \\(\\mathcal{A}\\) ), we can solve</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{\\boldsymbol{P}}}{\\operatorname{minimize}} \\sum_{i=1}^{N}\\left\\|f_{\\theta}\\left(Y_{i}\\right)-X_{i}\\right\\| \\] <p>If we have \\(X_{1}, \\ldots, X_{N}\\) and knowledge of \\(\\mathcal{A}\\), we can solve</p> \\[ \\operatorname{minimize}_{\\theta \\in \\mathbb{R}^{\\mathfrak{P}}} \\sum_{i=1}^{N}\\left\\|f_{\\theta}\\left[\\mathcal{A}\\left(X_{i}\\right)\\right]-X_{i}\\right\\| \\] <p>If we have \\(Y_{1}, \\ldots, Y_{N}\\) and knowledge of \\(\\mathcal{A}\\), we can solve</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{\\mathcal{P}}}{\\operatorname{minimize}} \\sum_{i=1}^{N}\\left\\|\\mathcal{A}\\left[f_{\\theta}\\left(Y_{i}\\right)\\right]-Y_{i}\\right\\| \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#image-super-resolution","title":"Image super-resolution","text":"<p>Given \\(X_{\\text {true }} \\in \\mathbb{R}^{w \\times h}\\), we measure</p> \\[ Y=\\mathcal{A}\\left(X_{\\text {true }}\\right) \\] <p>where \\(\\mathcal{A}\\) is a \"downsampling\" operator. So \\(Y \\in \\mathbb{R}^{w_{2} \\times h_{2}}\\) with \\(w_{2}&lt;w\\) and \\(h_{2}&lt;h\\). Goal is to recover \\(X_{\\text {true }}\\) from \\(Y\\).</p> <p>In the simplest setup, \\(\\mathcal{A}\\) is an average pool operator with \\(r \\times r\\) kernel and a stride \\(r\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#srcnn","title":"SRCNN","text":"<p>In 2015, Dong et al. presented super-resolution convolutional neural network (SRCNN). They trained a 3-layer \\(\\operatorname{CNN} f_{\\theta}\\) to learn the high-resolution reconstruction with the loss</p> \\[ \\mathcal{L}(\\theta)=\\sum_{i=1}^{N}\\left\\|f_{\\theta}\\left(\\tilde{Y}_{i}\\right)-X_{i}\\right\\|^{2} \\] <p>where \\(\\tilde{Y}_{i} \\in \\mathbb{R}^{w \\times h}\\) is an upsampled version of \\(Y_{i} \\in \\mathbb{R}^{(w / r) \\times(h / r)}\\), i.e., \\(\\tilde{Y}_{i}\\) has the same number of pixels as \\(X_{i}\\), but the image is pixelated or blurry. The goal is to have \\(f_{\\theta}\\left(\\tilde{Y}_{i}\\right)\\) be a sharp reconstruction. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#srcnn_1","title":"SRCNN","text":"<p>SRCNN showed that simple learning based approaches can match the state-of theart performances of superresolution task. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#vdsr","title":"VDSR","text":"<p>In 2016, Kim et al. presented VDSR. They trained a 20-layer CNN with a residual connection \\(f_{\\theta}\\) to learn the high-resolution reconstruction with the loss</p> \\[ \\mathcal{L}(\\theta)=\\sum_{i=1}^{N}\\left\\|f_{\\theta}\\left(\\tilde{Y}_{i}\\right)-X_{i}\\right\\|^{2} \\] <p>The residual connection was the key insight that enabled the training of much deeper CNNs. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#vdsr_1","title":"VDSR","text":"<p>VDSR dominated all prior approaches not based on deep learning. showed that simple learning based approaches can batch the state-of theart performances of super-resolution task. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#other-inverse-problem-tasks-and-results","title":"Other inverse problem tasks and results","text":"<p>There are many other inverse problems. Almost all of them now require deep neural networks to achieve state-of-the-art results.</p> <p>We won't spend more time on inverse problems in this course, but let's have fun and see a few other tasks and results. (These results are based on much more complex architectures and loss functions.)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#srgan","title":"SRGAN","text":"<p>SRGAN  C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi, Photo-realistic single image super-resolution using a generative adversarial network, CVPR, 2017.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#srgan_1","title":"SRGAN","text":"<p> bicubic interpolation </p> <p>SRGAN  ground truth</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#srgan_2","title":"SRGAN","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#image-colorization","title":"Image colorization","text":"<p> R. Zhang, P. Isola, and A. A. Efros, Colorful image colorization, ECCV, 2016.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#image-inpainting","title":"Image inpainting","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#image-inpainting_1","title":"Image inpainting","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#linear-operator-cong-matrix","title":"Linear operator \\(\\cong\\) matrix","text":"<p>Core tenet of linear algebra: matrices are linear operators and linear operators are matrices.</p> <p>Let \\(f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}\\) be linear, i.e.,</p> \\[ f(x+y)=f(x)+f(y) \\text { and } f(\\alpha x)=\\alpha f(x) \\] <p>for all \\(x, y \\in \\mathbb{R}^{n}\\) and \\(\\alpha \\in \\mathbb{R}\\).</p> <p>There exists a matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) that represents \\(f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}\\), i.e.,</p> \\[ f(x)=A x \\] <p>for all \\(x \\in \\mathbb{R}^{n}\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#linear-operator-cong-matrix_1","title":"Linear operator \\(\\cong\\) matrix","text":"<p>Let \\(e_{i}\\) be the \\(i\\)-th unit vector, i.e., \\(e_{i}\\) has all zeros elements except entry 1 in the \\(i\\)-th coordinate.</p> <p>Given a linear \\(f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}\\), we can find the matrix</p> \\[ A=\\left[\\begin{array}{llll} A_{;, 1} &amp; A_{;, 2} &amp; \\cdots &amp; A_{;, n} \\end{array}\\right] \\in \\mathbb{R}^{m \\times n} \\] <p>representing \\(f\\) with</p> \\[ f\\left(e_{j}\\right)=A e_{j}=A_{;, j} \\] <p>for all \\(j=1, \\ldots, n\\), or with</p> \\[ e_{i}^{\\top} f\\left(e_{j}\\right)=e_{i}^{\\top} A e_{j}=A_{i, j} \\] <p>for all \\(i=1, \\ldots, m\\) and \\(j=1, \\ldots, n\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#linear-operator-not-neq-matrix","title":"Linear operator \\(\\not \\neq\\) matrix","text":"<p>In applied mathematics and machine learning, there are many setups where explicitly forming the matrix representation \\(A \\in \\mathbb{R}^{m \\times n}\\) is costly, even though the matrix-vector products \\(A x\\) and \\(A^{\\top} y\\) are efficient to evaluate.</p> <p>In machine learning, convolutions are the primary example. Other areas, linear operators based on FFTs are the primary example.</p> <p>In such setups, the matrix representation is still a useful conceptual tool, even if we never intend to form the matrix.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#transpose-adjoint-of-a-linear-operator","title":"Transpose (adjoint) of a linear operator","text":"<p>Given a matrix \\(A\\), the transpose \\(A^{\\top}\\) is obtained by flipping the row and column dimensions, i.e., \\(\\left(A^{\\top}\\right)_{i j}=(A)_{j i}\\). However, using this definition is not always the most effective when understanding the action of \\(A^{\\top}\\).</p> <p>Another approach is to use the adjoint view. Since</p> \\[ y^{\\top}(A x)=\\left(A^{\\top} y\\right)^{\\top} x \\] <p>for any \\(x \\in \\mathbb{R}^{n}\\) and \\(y \\in \\mathbb{R}^{m}\\), understand the action of \\(A^{\\top}\\) by finding an expression of the form</p> \\[ y^{\\top} A x=\\sum_{j=1}^{n}(\\text { something })_{j} x_{j}=\\left(A^{\\top} y\\right)^{\\top} x \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#example-1d-transpose-convolution","title":"Example: 1D transpose convolution","text":"<p>Consider the 1D convolution represented by \\(A \\in \\mathbb{R}^{(n-f+1) \\times n}\\) defined with a given \\(w \\in \\mathbb{R}^{f}\\) and</p> \\[ A=\\left[\\begin{array}{cccccccc} w_{1} &amp; \\cdots &amp; w_{f} &amp; 0 &amp; \\cdots &amp; &amp; &amp; 0 \\\\ 0 &amp; w_{1} &amp; \\cdots &amp; w_{f} &amp; 0 &amp; \\cdots &amp; &amp; 0 \\\\ 0 &amp; 0 &amp; w_{1} &amp; \\cdots &amp; w_{f} &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; &amp; &amp; \\ddots &amp; &amp; \\ddots &amp; &amp; \\vdots \\\\ 0 &amp; &amp; \\cdots &amp; 0 &amp; w_{1} &amp; \\cdots &amp; w_{f} &amp; 0 \\\\ 0 &amp; &amp; \\cdots &amp; 0 &amp; 0 &amp; w_{1} &amp; \\cdots &amp; w_{f} \\end{array}\\right] \\] <p>Then we have</p> \\[ (A x)_{j}=\\sum_{i=1}^{f} w_{i} x_{j+i-1} \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#example-1d-transpose-convolution_1","title":"Example: 1D transpose convolution","text":"<p>and we have the following formula which coincides with transposing the matrix \\(A\\).</p> \\[ \\begin{aligned} y^{\\top} A x &amp; =\\sum_{j=1}^{n-f+1} y_{j} \\sum_{i=1}^{f} w_{i} x_{j+i-1} \\\\ &amp; =\\sum_{j=1}^{n-f+1} \\sum_{i=1}^{f} y_{j} w_{i} x_{j+i-1} \\sum_{k=1}^{n} \\mathbf{1}_{\\{k=j+i-1\\}} \\end{aligned} \\] \\[ =\\sum_{k=1}^{n} \\sum_{j=1}^{n-f+1} \\sum_{i=1}^{f} y_{j} w_{i} x_{k} \\mathbf{1}_{\\{k-j+1=i\\}} \\] \\[ =\\sum_{k=1}^{n} x_{k} \\sum_{j=1}^{n-f+1} \\sum_{i=1}^{f} w_{k-j+1} y_{j} \\mathbf{1}_{\\{k-j+1=i\\}} \\] \\[ =\\sum_{k=1}^{n} x_{k} \\sum_{j=1}^{n-f+1} w_{k-j+1} y_{j} \\sum_{i=1}^{f} \\mathbf{1}_{\\{k-j+1=i\\}} \\] \\[ =\\sum_{k=1}^{n} x_{k} \\sum_{j=1}^{n-f+1} w_{k-j+1} y_{j} \\mathbf{1}_{\\{1 \\leq k-j+1 \\leq f\\}} \\] \\[ =\\sum_{k=1}^{n} x_{k} \\sum_{j=1}^{n-f+1} w_{k-j+1} y_{j} \\mathbf{1}_{\\{j \\leq k\\}} \\mathbf{1}_{\\{k-f+1 \\leq j\\}} \\] \\[ =\\sum_{k=1}^{n} x_{k} \\sum_{j=\\max (k-f+1,1)}^{\\min (n-f+1, k)} w_{k-j+1} y_{j}=\\left(A^{\\top} y\\right)^{\\top} x \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#operations-increasing-spatial-dimensions","title":"Operations increasing spatial dimensions","text":"<p>In image classification tasks, the spatial dimensions of neural networks often decrease as the depth progresses.</p> <p>This is because we are trying to forget location information. (In classification, we care about what is in the image, but we do not where it is in the image.)</p> <p>However, there are many networks for which we want to increase the spatial dimension:</p> <ul> <li>Linear layers</li> <li>Upsampling</li> <li>Transposed convolution</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#upsampling-nearest-neighbor","title":"Upsampling: Nearest neighbor","text":"<p>torch.nn.Upsample with mode='nearest' </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#upsampling-bilinear-interpolation","title":"Upsampling: Bilinear interpolation","text":"<p>Torch.nn.Upsample with mode='bilinear' (We won't pay attention to the interpolation formula.)  'linear' interpolation is available for 1D data 'trilinear' interpolation is available for 3D data</p> 6.0000 6.5000 7.5000 8.0000 5.2500 5.6875 6.5625 7.0000 3.7500 4.0625 4.6875 5.0000 3.0000 3.2500 3.7500 4.0000"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#transposed-convolution","title":"Transposed convolution","text":"<p>In transposed convolution, input neurons additively distribute values to the output via the kernel.</p> <p>Before people noticed that this is the transpose</p> <p>Input </p> <p>Kernel </p> <p>Output </p> 0 0 1 0 4 6 4 12 9"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#transposed-convolution_1","title":"Transposed convolution","text":"<p>Input Kernel For each input neuron, multiply the kernel and add (accumulate) the value in the output.</p> <p>Can accommodate strides, padding, and multiple channels.  \\(+\\) \\(+\\) \\(+\\) \\(=\\)</p> 0 0 0 1 0 0 2 3 0 2 0 3 4 6 6 9"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#convolution-visualized","title":"Convolution visualized","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#transpose-convolution-visualized","title":"Transpose convolution visualized","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#2d-trans-conv-iayer-formal-definition","title":"2D trans. Conv. Iayer: Formal definition","text":"<p>Input tensor: \\(Y \\in \\mathbb{R}^{B \\times C_{\\mathrm{in}} \\times m \\times n}, B\\) batch size, \\(C_{\\mathrm{in}} \\#\\) of input channels. Output tensor: \\(X \\in \\mathbb{R}^{B \\times C_{\\text {out }} \\times\\left(m+f_{1}-1\\right) \\times\\left(n+f_{2}-1\\right)}, B\\) batch size, \\(C_{\\text {out }} \\#\\) of output channels, \\(m, n \\#\\) of vertical and horizontal indices. Filter \\(w \\in \\mathbb{R}^{C_{\\text {in }} \\times C_{\\text {out }} \\times f_{1} \\times f_{2}}\\), bias \\(b \\in \\mathbb{R}^{C_{\\text {out }}}\\). (If bias=False, then \\(b=0\\).)</p> <pre><code>def trans_conv(Y, w, b):\n    c_in, c_out, f1, f2 = w.shape\n    batch, c_in, m, n = Y.shape\n    X = torch.zeros(batch, c_out, m + f1 - 1, n + f2 - 1)\n    for k in range(c_in):\n        for i in range(Y.shape[2]):\n            for j in range(Y.shape[3]):\n                X[:, :, i:i+f1, j:j+f2] += Y[:, k, i, j].view(-1,1,1,1)*w[k, :, :, :].unsqueeze(0)\n    return X + b.view(1,-1,1,1)\n</code></pre>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#dependency-by-sparsity-pattern","title":"Dependency by sparsity pattern","text":"<p>In a matrix representation \\(A\\) of convolution. The dependencies of the inputs and outputs are represented by the non-zeros of \\(A\\), i.e., the sparsity pattern of \\(A\\). If \\(A_{i j}=0\\), then input neuron \\(j\\) does not affect the output neuron \\(i\\). If \\(A_{i j} \\neq 0\\), then \\(\\left(A^{\\top}\\right)_{j i} \\neq 0\\). So if input neuron \\(j\\) affects output neuron \\(i\\) in convolution, then input neuron \\(i\\) affects output neuron \\(j\\) in transposed convolution.</p> <p>We can combine this reasoning with our visual understanding of convolution. The diagram simultaneously illustrates the dependencies for both convolution and transposed convolution.</p> <p>Input for conv Output for trans.conv </p> <p>Output for conv Input for trans.conv.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#semantic-segmentation","title":"Semantic segmentation","text":"<p>In semantic segmentation, the goal is to segment the image into semantically meaningful regions by classifying each pixel.  </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#other-related-tasks","title":"Other related tasks","text":"<p>Object localization localizes a single object usually via a bounding box.</p> <p>Object detection detects many objects, with the same class often repeated, usually via bounding boxes. </p> <p>CAT </p> <p>CAT </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#other-related-tasks_1","title":"Other related tasks","text":"<p>Instance segmentation distinguishes multiple instances of the same object type. </p> <p>Image Recognition </p> <p>Object Detection </p> <p>Semantic Segmentation </p> <p>Instance Segmentation</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#pascal-voc","title":"Pascal VOC","text":"<p>We will use PASCAL Visual Object Classes (VOC) dataset for semantic segmentation. (Dataset also contains labels for object detection.)</p> <p>There are 21 classes: 20 main classes and 1 \"unlabeled\" class.</p> <p>Data \\(X_{1}, \\ldots, X_{N} \\in \\mathbb{R}^{3 \\times m \\times n}\\) and labels \\(Y_{1}, \\ldots, Y_{N} \\in\\{0,1, \\ldots, 20\\}^{m \\times n}\\), i.e., \\(Y_{i}\\) provides a class label for every pixel of \\(X_{i}\\).  image</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#loss-for-semantic-segmentation","title":"Loss for semantic segmentation","text":"<p>Consider the neural network</p> \\[ f_{\\theta}: \\mathbb{R}^{3 \\times m \\times n} \\rightarrow \\mathbb{R}^{k \\times m \\times n} \\] <p>such that \\(\\mu\\left(f_{\\theta}(X)\\right)_{i j} \\in \\Delta^{k}\\) is the probabilities for the \\(k\\) classes for pixel \\((i, j)\\).</p> <p>We minimize the sum of pixel-wise cross-entropy losses</p> \\[ \\mathcal{L}(\\theta)=\\sum_{l=1}^{N} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\ell^{\\mathrm{CE}}\\left(f_{\\theta}\\left(X_{l}\\right)_{i j},\\left(Y_{l}\\right)_{i j}\\right) \\] <p>where \\(\\ell^{C E}\\) is the cross entropy loss.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#u-net","title":"U-Net","text":"<p>The U-Net architecture:</p> <ul> <li>Reduce the spatial dimension to obtain high-level (coarse scale) features</li> <li>Upsample or transpose convolution to restore spatial dimension.</li> <li>Use residual connections across each dimension reduction stage. </li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#magnetic-resonance-imaging","title":"Magnetic resonance imaging","text":"<p>Magnetic resonance imaging (MRI) is an inverse problem in which we partially* measure the Fourier transform of the patient and the goal is to reconstruct the patient's image.</p> <p>So \\(X_{\\text {true }} \\in \\mathbb{R}^{n}\\) is the true original image (reshaped into a vector) with \\(n\\) pixels or voxels and \\(\\mathcal{A}\\left[X_{\\text {true }}\\right] \\in \\mathbb{C}^{k}\\) with \\(k \\ll n\\). (If \\(k=n\\), MRI scan can take hours.)</p> <p>Classical reconstruction algorithms rely on Fourier analysis, total variation regularization, compressed sensing, and optimization.</p> <p>Recent state-of-the-art use deep neural networks.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#fastmri-dataset","title":"fastMRI dataset","text":"<p>A team of researchers from Facebook AI Research and NYU released a large MRI dataset to stimulate datadriven deep learning research for MRI reconstruction. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#u-net-for-inverse-problems","title":"U-Net for inverse problems","text":"<p>Although U-Net was originally proposed as an architecture for semantic segmentation, it is also being used widely as one of the default architectures in inverse problems, including MRI reconstruction. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#computational-tomography","title":"Computational tomography","text":"<p>Computational tomography (CT) is an inverse problem in which we partially* measure the Radon transform of the patient and the goal is to reconstruct the patient's image.</p> <p>So \\(X_{\\text {true }} \\in \\mathbb{R}^{n}\\) is the true original image (reshaped into a vector) with \\(n\\) pixels or voxels and \\(\\mathcal{A}\\left[X_{\\text {true }}\\right] \\in \\mathbb{R}^{k}\\) with \\(k \\ll n\\). (If \\(k=n\\), the X -ray exposure to perform the CT scan can be harmful.)</p> <p>Recent state-of-the-art use deep neural networks.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#u-net-for-ct-reconstruction","title":"U-Net for CT reconstruction","text":"<p>U-Net is also used as one of the default architectures in CT reconstruction </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#chapter-5-unsupervised-learning","title":"Chapter 5: Unsupervised Learning","text":"<p>Mathematical Foundations of Deep Neural Networks Fall 2022 Department of Mathematical Sciences Ernest K. Ryu Seoul National University</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#unsupervised-learning","title":"Unsupervised learning","text":"<p>Unsupervised learning utilizes data \\(X_{1}, \\ldots, X_{N}\\) to learn the \"structure\" of the data. No labels are utilized.</p> <p>There are a wide range of unsupervised learning tasks. In this class, we discuss just a few.</p> <p>Generally, unsupervised learning tasks tend to have more mathematical complexity.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#low-dimensional-latent-representation","title":"Low-dimensional latent representation","text":"<p>Many high-dimensional data has some underlying low-dimensional structure.* </p> <p>If you randomly generate the pixels of a color image \\(X \\in \\mathbb{R}^{3 \\times m \\times n}\\), it will likely make no sense. Only a very small subset of pixel values correspond to meaningful images.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#finding-latent-representations","title":"Finding latent representations","text":"<p>In machine learning, especially in unsupervised learning, finding a \"meaningful\" lowdimensional latent representation is of interest.</p> <p>A good lower-dimensional representation of the data implies you have a good understanding of the data. </p> <p>An autoencoder (AE) has encoder \\(E_{\\theta}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{r}\\) and decoder \\(D_{\\varphi}: \\mathbb{R}^{r} \\rightarrow \\mathbb{R}^{n}\\) networks, where \\(r \\ll n\\). (If \\(r \\geq n\\), AE learns identity mapping, so pointless.) The two networks are trained through the loss</p> \\[ \\mathcal{L}(\\theta, \\varphi)=\\sum_{i=1}^{N}\\left\\|X_{i}-D_{\\varphi}\\left(E_{\\theta}\\left(X_{i}\\right)\\right)\\right\\|^{2} \\] <p>The low-dimensional output \\(E_{\\theta}(X)\\) is the latent vector. The encoder performs dimensionality reduction.</p> <p>The autoencoder can be thought of as a deep non-linear generalization of the principle component analysis (PCA).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#autoencoder-with-mnist","title":"Autoencoder with MNIST","text":"<p>PyTorch demo</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#applications-of-ae-denoising","title":"Applications of AE: Denoising","text":"<p>Autoencoders can be used to denoise or reconstruct corrupted images.  P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol, Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion, \\(J M L R, 2010\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#applications-of-ae-compression","title":"Applications of AE: Compression","text":"<p>Once an AE has been trained, storing the latent variable representation, rather than the original image can be used as a compression mechanism.</p> <p>More generally, latent variable representations can be used for video compression. https://youtu.be/NqmMnjJ6GEg</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#applications-of-ae-clustering","title":"Applications of AE: Clustering","text":"<p>Train an AE and then perform clustering on the latent variables. For the clustering algorithm, one can use things like k-means, which groups together </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#applications-of-ae-clustering_1","title":"Applications of AE: Clustering","text":"<p>Clustering is also referred to as unsupervised classification. Without labels, we want the group \"similar\" data. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#anomalyoutlier-detection","title":"Anomaly/outlier detection","text":"<p>Problem: detecting data that is significantly different from the data seen during training.</p> <p>Insight: AE should not be able to faithfully reconstruct novel data.</p> <p>Solution: Train an AE and define the score function to be the reconstruction loss:</p> \\[ s(X)=\\left\\|X-D_{\\varphi}\\left(E_{\\theta}(X)\\right)\\right\\|^{2} \\] <p>If score is high, determine the datapoint to be an outliner. (Cf. hw7.)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#probabilistic-generative-models","title":"Probabilistic generative models","text":"<p>A probabilistic generative model learns a distribution \\(p_{\\theta}\\) from \\(X_{1}, \\ldots, X_{N} \\sim p_{\\text {true }}\\) such that \\(p_{\\theta} \\approx p_{\\text {true }}\\) and such that we can generate new samples \\(X \\sim p_{\\theta}\\).</p> <p>The ability to generate new synthetic data is interesting, but by itself not very useful.*</p> <p>The structure of the data learned through the unsupervised learning is of higher value. However, we won't talk about the downstream applications in this course.</p> <p>In this class, we will talk about flow models, VAEs, and GANs.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#flow-model-change-of-variable-formula-combined-with-deep-neural-networks","title":"Flow model: Change of variable formula combined with deep neural networks","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#flow-models","title":"Flow models","text":"<p>Fit a probability density function \\(p_{\\theta}(x)\\) with continuous data \\(X_{1}, \\ldots, X_{N} \\sim p_{\\text {true }}(x)\\).</p> <ul> <li>We want to fit the data \\(X_{1}, \\ldots, X_{N}\\) (or really the underlying distribution \\(p_{\\text {true }}\\) ) well.</li> <li>We want to be able to sample from \\(p_{\\theta}\\).</li> <li>(We want to get a good latent representation.)</li> </ul> <p>We first develop the mathematical discussion with 1D flows, and then generalize the discussion to high dimensions.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#example-density-model-gaussian-mixture-model","title":"Example density model: Gaussian mixture model","text":"\\[ p_{\\theta}(x)=\\sum_{i=1}^{k} \\pi_{i} \\mathcal{N}\\left(x ; \\mu_{i}, \\sigma_{i}^{2}\\right) \\] <p>Parameters: means and variances of components, mixture weights</p> \\[ \\theta=\\left(\\pi_{1}, \\ldots, \\pi_{k}, \\mu_{1}, \\ldots, \\mu_{k}, \\sigma_{1}, \\ldots, \\sigma_{k}\\right) \\] <p>Problems with GMM:</p> <ul> <li>Highly non-convex optimization problem. Can easily get stuck in local minima.</li> <li>It is does not have the representation power to express high-dimensional data.</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#example-density-model-gaussian-mixture-model_1","title":"Example density model: Gaussian mixture model","text":"<p>GMM doesn't work with high-dimensional data. The sampling process is: 1.Pick a cluster center 2.Add Gaussian noise</p> <p>If this is done with natural images, a realistic image can be generated only if it is a cluster center, i.e., the clusters must already be realistic images. </p> <p>So then how do we fit a general (complex) density model?</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#math-review-1d-continuous-rv","title":"Math review: 1D continuous RV","text":"<p>A random variable \\(X\\) is continuous if there exists a probability density function \\(p_{X}(x) \\geq 0\\) such that</p> \\[ \\mathbb{P}(a \\leq X \\leq b)=\\int_{a}^{b} p_{X}(x) d x \\] <p>\\(p_{X}(x)\\) </p> <p>In this case, we write \\(X \\sim p_{X}\\).</p> <p>The cumulative distribution function (CDF) of \\(X\\) is defined as</p> \\[ F_{X}(t)=\\mathbb{P}(X \\leq t)=\\int_{-\\infty}^{t} p_{X}(x) d x \\] <p>\\(F_{X}(t)\\) is a nondecreasing function.  \\(F_{X}(t)\\) is a continuous function if \\(X\\) is a continuous random variable.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#naive-approach-prameterize-p_theta-as-dnn","title":"Na\u00efve approach: prameterize \\(p_{\\theta}\\) as DNN","text":"<p>Na\u00efve approach for fitting a density model. Represent \\(p_{\\theta}(x)\\) with DNN. </p> <p>There are some challenges:</p> <ol> <li>How to ensure proper distribution? \\(\\int_{-\\infty}^{+\\infty} p_{\\theta}(x) d x=1, \\quad p_{\\theta}(x) \\geq 0, \\quad x \\in \\mathbb{R}\\)</li> <li>How to sample?</li> </ol>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#normalization-of-p_theta","title":"Normalization of \\(p_{\\theta}\\)","text":"<p>For discrete random variables, one can use the soft-max function \\(\\mu: \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{k}\\) defined as</p> \\[ \\mu_{i}(z)_{i}=\\frac{e^{z_{i}}}{\\sum_{j=1}^{k} e^{z_{j}}} \\] <p>to normalize probabilities.</p> <p>For continuous random variables, we can ensure \\(p_{\\theta} \\geq 0\\) with \\(p_{\\theta}(x)=e^{f_{\\theta}(x)}\\), where \\(f_{\\theta}\\) is the output of the neural network. However, ensuring the normalization</p> \\[ \\int_{-\\infty}^{+\\infty} p_{\\theta}(x) d x=1 \\] <p>is not a simple matter. (Any Bayesian statistician can tell you how difficult this is.)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#what-happens-if-we-ignore-normalization","title":"What happens if we ignore normalization?","text":"<p>Do we really need this normalization thing? Yes, we do.</p> <p>Without normalization, one can just assign arbitrarily large probabilities everywhere when we perform maximum likelihood estimation:</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right) \\] <p>The solution is to set \\(p_{\\theta}(x)=M\\) with \\(M \\rightarrow \\infty\\).</p> <p>We want model to place large probability on data \\(X_{1}, \\ldots, X_{N}\\) while placing small probability elsewhere. Normalization forces model to place small probability where data doesn't reside.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#key-insight-parameterize-zf_thetax-with-dnn","title":"Key insight: Parameterize \\(Z=f_{\\theta}(X)\\) with DNN","text":"<p>Key insight of normalizing flow: DNN outputs random variable \\(Z\\), rather than \\(p_{\\theta}(X)\\) </p> <p>In normalizing flow, find \\(\\theta\\) such that the flow \\(f_{\\theta}\\) normalizes the random variable \\(X \\sim p_{X}\\) into \\(Z \\sim \\mathcal{N}(0,1)^{*}\\).</p> <p>Important questions to resolve:</p> <ol> <li>How to train? (How to evaluate \\(p_{\\theta}(x)\\) ? DNN outputs \\(f_{\\theta}\\), not \\(p_{\\theta}\\).)</li> <li>How to sample \\(X\\) ?</li> </ol>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#1d-change-of-variable-formula","title":"1D change of variable formula","text":"<p>Assume \\(f\\) is invertible, \\(f\\) is differentiable, and \\(f^{-1}\\) is differentiable. If \\(X \\sim p_{X}\\), then \\(Z=f(X)\\) has pdf</p> \\[ p_{Z}(z)=p_{X}\\left(f^{-1}(z)\\right)\\left|\\frac{d x}{d z}\\right| \\] <p>If \\(Z \\sim p_{Z}\\), then \\(X=f^{-1}(Z)\\) has pdf</p> \\[ p_{X}(x)=p_{Z}(f(x))\\left|\\frac{d f(x)}{d x}\\right| \\] <p>Since \\(Z=f(X)\\), one might think \\(p_{X}(x)=p_{Z}(z)=p_{Z}(f(x)) . \\leftarrow\\) This is wrong.</p> <p>Invertibility of \\(f\\) is essential; it is not a minor technical issue.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#training-flow-models","title":"Training flow models","text":"<p>Train model with MLE</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{Z}\\left(f_{\\theta}\\left(X_{i}\\right)\\right)+\\log \\left|\\frac{\\partial f_{\\theta}}{\\partial x}\\left(X_{i}\\right)\\right| \\] <p>where \\(f_{\\theta}\\) is invertible and differentiable, and \\(X=f_{\\theta}^{-1}(Z)\\) with \\(Z \\sim p_{Z}\\) so</p> \\[ p_{X}(x)=p_{Z}\\left(f_{\\theta}(x)\\right)\\left|\\frac{\\partial f_{\\theta}}{\\partial x}(x)\\right| \\] <p>Can optimize with SGD, if we know how to perform backprop on \\(\\left|\\frac{\\partial f_{\\theta}}{\\partial x}\\left(X_{i}\\right)\\right|\\). More on this later.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#sampling-from-flow-models","title":"Sampling from flow models","text":"<p>Step 1: Sample \\(Z \\sim p_{Z}\\) Step 2: Compute \\(X=f_{\\theta}^{-1}(Z)\\)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#requirements-of-flow-f_theta","title":"Requirements of flow \\(f_{\\theta}\\)","text":"<p>Theoretical requirement:</p> <ul> <li>\\(f_{\\theta}(x)\\) invertible and differentiable.</li> </ul> <p>Computational requirements:</p> <ul> <li>\\(f_{\\theta}(x)\\) and \\(\\nabla_{\\theta} f_{\\theta}(x)\\) efficient to evaluate (for training)</li> <li>\\(\\left|\\frac{\\partial f_{\\theta}}{\\partial x}(x)\\right|\\) and \\(\\nabla_{\\theta}\\left|\\frac{\\partial f_{\\theta}}{\\partial x}(x)\\right|\\) efficient to evaluate (for training)</li> <li>\\(f_{\\theta}^{-1}\\) efficient to evaluate (for sampling)</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#example-flow-to-z-uniform01","title":"Example: Flow to \\(Z\\) ~ Uniform([0,1])","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#before-training","title":"Before training","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#example-flow-to-z-sim-operatornamebeta55","title":"Example: Flow to \\(Z \\sim \\operatorname{Beta}(5,5)\\)","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#example-flow-to-z-sim-mathcaln01","title":"Example: Flow to \\(Z \\sim \\mathcal{N}(0,1)\\)","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#1d-flow-demonstration","title":"1D flow demonstration","text":"<p>PyTorch demo</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#universality-of-flows","title":"Universality of flows","text":"<p>Are flows universal, i.e., can \\(f_{\\theta}^{-1}(Z) \\sim p_{X}\\) for any \\(X\\) provided that \\(f_{\\theta}\\) can represent any invertible function?</p> <p>Yes, 1D flows are universal due to the inverse CDF sampling technique.*</p> <p>Higher dimensional flows are also universal as shown by Huang et al.# or earlier by the general theory of optimal transport. \\({ }^{\\%}\\)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#math-review-sampling-via-inverse-cdf","title":"Math review: Sampling via inverse CDF","text":"<p>Inverse CDF sampling is a technique for sampling \\(X \\sim p_{X}\\). If \\(F_{X}(t)\\) is furthermore a strictly increasing function, then \\(F_{X}\\) is invertible, i.e., \\(F_{X}^{-1}\\) exists.</p> <p>Generate a random number \\(U \\sim \\operatorname{Uniform}([0,1])\\) and compute \\(F_{X}^{-1}(U)\\). Then</p> \\[ F_{X}^{-1}(U) \\sim p_{X} \\] <p>since</p> \\[ \\mathbb{P}\\left(F_{X}^{-1}(U) \\leq t\\right)=\\mathbb{P}\\left(U \\leq F_{X}(t)\\right)=F_{X}(t) \\] <p>Technique can be generalized to when \\(F_{X}\\) is not invertible.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#universality-of-1d-flows","title":"Universality of 1D flows","text":"<p>Composition of flows is a flow, and inverse of a flow is a flow</p> <p>Universality of 1D flows:</p> <ul> <li>Use inverse CDF as flow to transform \\(X \\sim p_{X}\\) into \\(U \\sim \\operatorname{Uniform}([0,1])\\) and \\(Z \\sim \\mathcal{N}(0,1)\\) into \\(U \\sim\\) Uniform ( \\([0,1]\\) ).</li> <li>Compose flow \\(X \\rightarrow U\\) and inverse flow \\(U \\rightarrow Z\\) </li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#jacobian-notation","title":"Jacobian notation","text":"<p>Let \\(f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}\\), such that</p> \\[ f(x)=\\left[\\begin{array}{c} f_{1}(x) \\\\ f_{2}(x) \\\\ \\vdots \\\\ f_{n}(x) \\end{array}\\right] \\] <p>The Jacobian matrix is</p> \\[ \\frac{\\partial f}{\\partial x}(x)=\\left[\\begin{array}{cccc} \\frac{\\partial f_{1}}{\\partial x_{1}}(x) &amp; \\frac{\\partial f_{1}}{\\partial x_{2}}(x) &amp; \\cdots &amp; \\frac{\\partial f_{1}}{\\partial x_{n}}(x) \\\\ \\frac{\\partial f_{2}}{\\partial x_{1}}(x) &amp; \\frac{\\partial f_{2}}{\\partial x_{2}}(x) &amp; \\cdots &amp; \\frac{\\partial f_{2}}{\\partial x_{n}}(x) \\\\ \\vdots &amp; &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial f_{n}}{\\partial x_{1}}(x) &amp; \\frac{\\partial f_{n}}{\\partial x_{2}}(x) &amp; \\cdots &amp; \\frac{\\partial f_{n}}{\\partial x_{n}}(x) \\end{array}\\right]=\\left[\\begin{array}{c} \\left(\\nabla f_{1}(x)\\right)^{\\top} \\\\ \\left(\\nabla f_{2}(x)\\right)^{\\top} \\\\ \\vdots \\\\ \\left(\\nabla f_{n}(x)\\right)^{\\top} \\end{array}\\right] \\] <p>The Jacobian determinant is \\(\\operatorname{det}\\left(\\frac{\\partial f}{\\partial x}\\right)\\). We use the notation</p> \\[ \\left|\\frac{\\partial f}{\\partial x}(x)\\right|=\\left|\\operatorname{det}\\left(\\frac{\\partial f}{\\partial x}(x)\\right)\\right| \\] <p>where the second \\(|\\cdot|\\) is the absolute value of the determinant. (This notation is not completely standard.)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#math-review-multivariate-change-of-variables","title":"Math review: Multivariate change of variables","text":"<p>Let \\(f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}\\) be an invertible function such that both \\(f\\) and \\(f^{-1}\\) are differentiable. Let \\(U \\subseteq \\mathbb{R}^{n}\\). Then</p> \\[ \\int_{f(U)} h(v) d v=\\int_{U} h(f(u))\\left|\\frac{\\partial f}{\\partial u}(u)\\right| d u \\] <p>for any \\(h: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}\\). (Change of variable from \\(v=f(u)\\) to \\(u=f^{-1}(v)\\).)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#math-review-multivariate-continuous-rv","title":"Math review: Multivariate continuous RV","text":"<p>A multivariate random variable \\(X \\in \\mathbb{R}^{n}\\) is continuous if there exists a probability density function \\(p_{X}(x)\\) such that</p> \\[ \\mathbb{P}(X \\in A)=\\int_{A} p_{X}(x) d x \\] <p>where the integral is over the volume \\(A \\subseteq \\mathbb{R}^{n}\\). In this case, we write \\(X \\sim p_{X}\\).</p> <p>The joint cumulative distribution function (the copula) does not seem to be useful in the context of high-dimensional flow models.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#math-review-mult-change-of-variables-for-rv","title":"Math review: Mult. change of variables for RV","text":"<p>Let \\(f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}\\) be an invertible function such that both \\(f\\) and \\(f^{-1}\\) are differentiable. Let \\(X\\) be a continuous random variable with probability density function \\(p_{X}\\) and let \\(Y=f(X)\\) have density \\(p_{Y}\\). Then</p> \\[ p_{X}(x)=p_{Y}(f(x))\\left|\\frac{\\partial f}{\\partial x}(x)\\right| \\] <p>Proof)</p> \\[ \\mathbb{P}\\left(f^{-1}(Y) \\in A\\right)=\\mathbb{P}(Y \\in f(A))=\\int_{f(A)} p_{Y}(y) d y=\\int_{A} p_{Y}(f(x))\\left|\\frac{\\partial f}{\\partial x}(x)\\right| d x=\\mathbb{P}(X \\in A) \\] <p>Invertibility of \\(f\\) is essential; it is not a minor technical issue.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#math-review-determinant-formulae","title":"Math review: Determinant formulae","text":"<p>Fact: Determinant definitions in undergraduate linear algebra textbooks require exponentially many operations to compute:</p> \\[ \\operatorname{det}(A)=\\sum_{\\sigma \\in S_{n}}\\left(\\operatorname{sgn}(\\sigma) \\prod_{i=1}^{n} a_{i, \\sigma_{i}}\\right) \\] <p>Efficient computation of determinant for general matrices and performing backprop through the computation is difficult. Therefore, high-dimensional flow model are designed to compute determinants only on simple matrices.</p> <p>Product formula: if \\(A\\) and \\(B\\) are square, then</p> \\[ \\operatorname{det}(A B)=\\operatorname{det}(A) \\operatorname{det}(B) \\] <p>Block lower triangular formula: if \\(A \\in \\mathbb{R}^{n \\times n}\\) and \\(C \\in \\mathbb{R}^{m \\times m}\\), then</p> \\[ \\operatorname{det}\\left(\\begin{array}{ll} A &amp; 0 \\\\ B &amp; C \\end{array}\\right)=\\operatorname{det}(A) \\operatorname{det}(C) \\] <p>Lower triangular formula: if \\(a_{1}, \\ldots, a_{n} \\in \\mathbb{R}\\) and \\(*\\) represents arbitrary values, then</p> \\[ \\operatorname{det}\\left(\\begin{array}{cccc} a_{1} &amp; 0 &amp; \\cdots &amp; 0 \\\\ * &amp; a_{2} &amp; &amp; \\vdots \\\\ * &amp; * &amp; \\ddots &amp; 0 \\\\ * &amp; * &amp; * &amp; a_{n} \\end{array}\\right)=\\prod_{i=1}^{n} a_{i} \\] <p>Upper triangular formula: same as for lower triangular matrices.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#training-high-dim-flow-models","title":"Training high-dim flow models","text":"<p>Train model with MLE</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{Z}\\left(f_{\\theta}\\left(X_{i}\\right)\\right)+\\log \\left|\\frac{\\partial f_{\\theta}}{\\partial x}\\left(X_{i}\\right)\\right| \\] <p>where \\(f_{\\theta}(z)\\) is invertible and differentiable, and \\(X=f^{-1}(Z)\\) with \\(Z \\sim p_{Z}\\) so</p> \\[ p_{X}(x)=p_{Z}\\left(f_{\\theta}(x)\\right)\\left|\\frac{\\partial f_{\\theta}}{\\partial x}(x)\\right| \\] <p>(Exactly the same formula as with 1D flow.)</p> <p>Can optimize with SGD, if we know how to perform backprop on \\(\\left|\\frac{\\partial f_{\\theta}}{\\partial x}\\left(X_{i}\\right)\\right|\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#composing-flows","title":"Composing flows","text":"<p>Flows can be composed to increase expressiveness. (Deep NN more expressive.) Consider composition of \\(k\\) flows</p> \\[ \\begin{aligned} &amp; x \\rightarrow f_{1} \\rightarrow f_{2} \\rightarrow \\cdots \\rightarrow f_{k} \\rightarrow z \\\\ &amp; z=f_{k} \\circ \\cdots \\circ f_{1}(x) \\\\ &amp; x=f_{1}^{-1} \\circ \\cdots \\circ f_{k}^{-1}(z) \\end{aligned} \\] <p>Determinant computation splits nicely due to chain rule and product formula</p> \\[ \\begin{aligned} &amp; \\operatorname{det}\\left(\\frac{\\partial z}{\\partial x}\\right)=\\operatorname{det}\\left(\\frac{\\partial f_{k}}{\\partial f_{k-1}} \\cdots \\frac{\\partial f_{1}}{\\partial f_{0}}\\right)=\\operatorname{det}\\left(\\frac{\\partial f_{k}}{\\partial f_{k-1}}\\right) \\cdots \\operatorname{det}\\left(\\frac{\\partial f_{1}}{\\partial f_{0}}\\right) \\\\ &amp; \\log p_{\\theta}(x)=\\log p_{\\theta}(z)+\\sum_{i=1}^{k} \\log \\left|\\frac{\\partial f_{i}}{\\partial f_{i-1}}\\right| \\end{aligned} \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#basic-example-affine-flows","title":"Basic example: Affine flows","text":"<p>An affine (linear) transformation</p> \\[ f_{A, b}(x)=A^{-1}(x-b) \\] <p>is a flow if matrix \\(A\\) is invertible. Then</p> \\[ \\frac{\\partial f_{A, b}}{\\partial x}=A^{-1} \\] <p>and</p> \\[ \\left|\\frac{\\partial f_{A, b}}{\\partial x}\\right|=\\left|\\operatorname{det}\\left(A^{-1}\\right)\\right|=\\frac{1}{|\\operatorname{det}(A)|} \\] <p>Sampling: \\(X=A Z+b\\), where \\(Z \\sim \\mathcal{N}(0, I)\\). Problem with affine flows:</p> <ul> <li>Computing \\(|\\operatorname{det}(A)|\\) is expensive and performing backprop over it is difficult. We want \\(\\frac{\\partial f_{A, b}}{\\partial x}\\) to be further structured so that determinant is easy to compute.</li> <li>One affine flow is insufficient to generate complex data. However, composing multiple affine flows yields an affine flow and therefore is pointless. We need to introduce nonlinearities.</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#coupling-flows","title":"Coupling flows","text":"<p>A coupling flow is a general and practical approach for constructing non-linear flows.</p> <p>Partition input into two disjoint subsets \\(x=\\left(x^{A}, x^{B}\\right)\\). Then</p> \\[ f(x)=\\left(x^{A}, \\hat{f}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right)\\right) \\] <p>where \\(\\psi_{\\theta}\\) is a neural network and \\(\\hat{f}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right)\\) is another flow whose parameters depend on \\(x^{A}\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#coupling-flow-forward-evaluation","title":"Coupling flow: forward evaluation","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#coupling-flow-inverse-evaluation","title":"Coupling flow: inverse evaluation","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#jacobian-of-coupling-flows","title":"Jacobian of coupling flows","text":"<p>The Jacobian of a coupling flow has a nice block structure</p> \\[ \\frac{\\partial f_{\\theta}}{\\partial x}(x)=\\left[\\begin{array}{cc} I &amp; 0 \\\\ \\frac{\\partial \\hat{f}}{\\partial x^{A}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right) &amp; \\frac{\\partial \\hat{f}}{\\partial x^{B}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right) \\end{array}\\right] \\] <p>which leads to the simplified determinant formula</p> \\[ \\operatorname{det}\\left(\\frac{\\partial f_{\\theta}}{\\partial x}(x)\\right)=\\operatorname{det}\\left(\\frac{\\partial \\hat{f}}{\\partial x^{B}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right)\\right) \\] <p>Note \\(\\frac{\\partial \\hat{f}}{\\partial x^{A}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right)\\), which will be very complicated, does not appear in the determinant.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#coupling-transformation-hatfx-mid-psi","title":"Coupling transformation \\(\\hat{f}(x \\mid \\psi)\\)","text":"<p>Additive transformations (NICE)*</p> \\[ \\hat{f}(x \\mid \\psi)=x+t \\] <p>where \\(\\psi=t\\).</p> <p>Affine transformations (Real NVP) \\({ }^{\\#}\\)</p> \\[ \\hat{f}(x \\mid \\psi)=e^{s} \\odot x+t \\] <p>where \\(\\psi=(s, t)\\).</p> <p>Other transformations studied throughout the literature.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#nice-non-linear-independent-components-estimation","title":"NICE (Non-linear Independent Components Estimation)","text":"<p>NICE uses additive coupling layers: Split variables in half: \\(x_{1: n / 2}, x_{n / 2: n}\\)</p> \\[ \\begin{aligned} &amp; z_{1: n / 2}=x_{1: n / 2} \\\\ &amp; z_{n / 2: n}=x_{n / 2: n}+t_{\\theta}\\left(x_{1: n / 2}\\right) \\end{aligned} \\] <p>Easily invertible:</p> \\[ \\begin{aligned} &amp; x_{1: n / 2}=z_{1: n / 2} \\\\ &amp; x_{n / 2: n}=z_{n / 2: n}-t_{\\theta}\\left(x_{1: n / 2}\\right) \\end{aligned} \\] <p>Jacobian determinant is easy to compute: \\(\\operatorname{det} \\frac{\\partial f_{\\theta}}{\\partial x}(x)=\\operatorname{det}\\left[\\begin{array}{cc}I &amp; 0 \\\\ \\frac{\\partial \\hat{f}}{\\partial x^{A}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right) &amp; \\frac{\\partial \\hat{f}}{\\partial x^{B}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right)\\end{array}\\right]=\\operatorname{det}\\left[\\begin{array}{cc}I &amp; 0 \\\\ \\frac{\\partial \\hat{f}}{\\partial x^{A}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right) &amp; I\\end{array}\\right]=1\\)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#real-nvp-real-valued-non-volume-preserving","title":"Real NVP (Real-valued Non-Volume Preserving)","text":"<p>Real NVP uses affine coupling layers:</p> \\[ \\begin{aligned} &amp; z_{1: n / 2}=x_{1: n / 2} \\\\ &amp; z_{n / 2: n}=e^{s_{\\theta}\\left(x_{1: n / 2}\\right)} \\odot x_{n / 2: n}+t_{\\theta}\\left(x_{1: n / 2}\\right) \\end{aligned} \\] <p>Easily invertible:</p> \\[ \\begin{aligned} &amp; x_{1: n / 2}=z_{1: n / 2} \\\\ &amp; x_{n / 2: n}=\\left(z_{n / 2: n}-t_{\\theta}\\left(x_{1: n / 2}\\right)\\right) \\odot e^{-s_{\\theta}\\left(x_{1: n / 2}\\right)} \\end{aligned} \\] <p>Jacobian determinant is easy to compute:</p> \\[ \\begin{aligned} \\operatorname{det} \\frac{\\partial f_{\\theta}}{\\partial x}(x) &amp; =\\operatorname{det}\\left[\\begin{array}{cc} I &amp; 0 \\\\ \\frac{\\partial \\hat{f}}{\\partial x^{A}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right) &amp; \\frac{\\partial \\hat{f}}{\\partial x^{B}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right) \\end{array}\\right] \\\\ &amp; =\\operatorname{det}\\left[\\begin{array}{cc} I &amp; 0 \\\\ \\frac{\\partial \\hat{f}}{\\partial x^{A}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right) &amp; \\operatorname{diag}\\left(e^{s_{\\theta}\\left(x_{1: n / 2}\\right)}\\right) \\end{array}\\right]=\\exp \\left(\\mathbf{1}_{n / 2}^{\\top} s_{\\theta}\\left(x_{1: n / 2}\\right)\\right) \\end{aligned} \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#real-nvp-results","title":"Real NVP - Results","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#how-to-partition-variables","title":"How to partition variables?","text":"<p>Note that the additive and affine coupling layers of NICE and Real NVP are nonlinear mappings from \\(x_{1: n}\\) to \\(z_{1: n}\\), since \\(s_{\\theta}\\left(x_{1: n / 2}\\right)\\) and \\(t_{\\theta}\\left(x_{1: n / 2}\\right)\\) are nonlinear.</p> <p>Flow models compose multiple nonlinear flows. But if \\(x_{1: n / 2}\\) is always unchanged, then the full composition will leave it unchanged. Therefore, we change the partitioning for every coupling layer.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#nice-architecture","title":"NICE architecture","text":"<p>PyTorch demo</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#real-nvp-variable-partitioning","title":"Real NVP variable partitioning","text":"<p>Two partition strategies:</p> <ol> <li>Partition with checkerboard pattern.</li> <li>Reshape tensor and then partition channelwise. </li> </ol>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#real-nvp-architecture","title":"Real NVP Architecture","text":"<p>Input \\(X\\) : \\(c \\times 32 \\times 32\\) image with \\(c=3\\) </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#layer-1-input-x-c-times-32-times-32","title":"Layer 1: Input \\(X: c \\times 32 \\times 32\\)","text":"<ul> <li>Checkerboard \\(\\times 3\\), channel reshape into \\(4 c \\times 16 \\times 16\\), channel \\(\\times 3\\)</li> <li>Output: Split result to get \\(X_{1}: 2 c \\times 16 \\times 16\\) and \\(Z_{1}: 2 c \\times 16 \\times 16\\) (fine-grained latents) Layer 2: Input \\(X_{1}: 2 c \\times 16 \\times 16\\) from layer 1</li> <li>Checkerboard \\(\\times 3\\), channel reshape into \\(8 c \\times 8 \\times 8\\), channel \\(\\times 3\\)</li> <li>Split result to get \\(X_{2}: 4 c \\times 8 \\times 8\\) and \\(Z_{2}: 4 c \\times 8 \\times 8\\) (coarser latents)</li> </ul> <p>Layer 3: Input \\(X_{2}: 4 c \\times 8 \\times 8\\) from layer 2</p> <ul> <li>Checkerboard \\(\\times 3\\), channel reshape into \\(16 c \\times 4 \\times 4\\), channel \\(\\times 3\\)</li> <li>Get \\(Z_{3}: 16 c \\times 4 \\times 4\\) (latents for highest-level details)</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#batch-normalization_1","title":"Batch normalization","text":"<p>To train deep flows, BN is helpful. However, the large model size forces the use of small batch sizes, and BN is not robust with small batch sizes. RealNVP uses a modified form of BN</p> \\[ x \\mapsto \\frac{x-\\tilde{\\mu}}{\\sqrt{\\tilde{\\sigma}^{2}+\\varepsilon}} \\] <p>(No \\(\\beta\\) and \\(\\gamma\\) parameters.) This layer has the log Jacobian determinant</p> \\[ -\\frac{1}{2} \\sum_{i} \\log \\left(\\tilde{\\sigma}_{i}^{2}+\\varepsilon\\right) \\] <p>The mean and variance parameters are updated with</p> \\[ \\begin{aligned} \\tilde{\\mu}_{k+1} &amp; =\\rho \\tilde{\\mu}_{k}+(1-\\rho) \\hat{\\mu}_{k} \\\\ \\tilde{\\sigma}_{k+1}^{2} &amp; =\\rho \\tilde{\\sigma}_{k}^{2}+(1-\\rho) \\hat{\\sigma}_{k}^{2} \\end{aligned} \\] <p>where \\(\\rho\\) is the momentum. During gradient computation, only backprop through the current batch statistics \\(\\hat{\\mu}_{k}\\) and \\(\\hat{\\sigma}_{k}^{2}\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#s_theta-and-t_theta-networks","title":"\\(s_{\\theta}\\) and \\(t_{\\theta}\\) networks","text":"<p>The \\(s_{\\theta}\\) and \\(t_{\\theta}\\) do not need to be invertible. The original RealNVP paper does not describe its construction.</p> <p>We let \\(\\left(s_{\\theta}, t_{\\theta}\\right)\\) be a deep (20-layer) convolutional neural network using residual connections and standard batch normalization.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#real-nvp-architecture_1","title":"Real NVP architecture","text":"<p>PyTorch demo</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#glow-paper","title":"Glow paper","text":"<p>The authors of the Glow paper also released a blog post. https://openai.com/blog/glow/</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#ffjord","title":"FFJORD","text":"<p>Instead of a discrete composition of flows, what if we have a continuous-time flow?</p> \\[ \\begin{aligned} z_{0} &amp; =x \\\\ z_{t} &amp; =z_{0}+\\int_{0}^{t} h\\left(t, z_{t}\\right) d t \\\\ f(x) &amp; =z_{1} \\end{aligned} \\] <p>Inverse:</p> \\[ \\begin{aligned} z_{1} &amp; =z \\\\ z_{t} &amp; =z_{1}-\\int_{t}^{1} h\\left(t, z_{t}\\right) d t \\\\ f^{-1}(z) &amp; =z_{0} \\end{aligned} \\] <p>R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud, Neural ordinary differential equations, NeurIPS, 2018. W. GrathwohI, R. T. Q. Chen, J. Bettencourt, I. Sutskever, and D. Duvenaud, FFJORD: Free-form continuous dynamics for scalable reversible generative</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#math-review-conditional-probabilities","title":"Math review: Conditional probabilities","text":"<p>Let \\(A\\) and \\(B\\) be probabilistic events. Assume \\(A\\) has nonzero probability.</p> <p>Conditional probability satisfies</p> \\[ \\mathbb{P}(B \\mid A) \\mathbb{P}(A)=\\mathbb{P}(A \\cap B) \\] <p>Bayes' theorem is an application of conditional probability:</p> \\[ \\mathbb{P}(B \\mid A)=\\frac{\\mathbb{P}(A \\mid B) \\mathbb{P}(B)}{\\mathbb{P}(A)} \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#math-review-conditional-densities","title":"Math review: Conditional densities","text":"<p>Let \\(X \\in \\mathbb{R}^{m}\\) and \\(Z \\in \\mathbb{R}^{n}\\) be continuous random variables with joint density \\(p(x, z)\\).</p> <p>The marginal densities are defined by</p> \\[ p_{X}(x)=\\int_{\\mathbb{R}^{n}} p(x, z) d z, \\quad p_{Z}(z)=\\int_{\\mathbb{R}^{m}} p(x, z) d x \\] <p>The conditional density function \\(p(z \\mid x)\\) has the following properties</p> \\[ \\begin{gathered} \\mathbb{P}(Z \\in S \\mid X=x)=\\int_{S} p(z \\mid x) d z \\\\ p(z \\mid x) p_{X}(x)=p(x, z), \\quad p(z \\mid x)=\\frac{p(x \\mid z) p_{Z}(Z)}{p_{X}(X)} \\end{gathered} \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#variational-autoencoders-vae","title":"Variational autoencoders (VAE)","text":"<p>These are synthetic (fake) images. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#variational-autoencoders-vae_1","title":"Variational autoencoders (VAE)","text":"<p>Key idea of VAE:</p> <ul> <li>Latent variable model with conditional probability distribution represented by \\(p_{\\theta}(x \\mid z)\\).</li> <li>Efficiently estimate \\(p_{\\theta}(x)=\\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}(x \\mid Z)\\right]\\) by importance sampling with \\(Z \\sim q_{\\phi}(z \\mid x)\\).</li> </ul> <p>We can interpret \\(q_{\\phi}(z \\mid x)\\) as an encoder and \\(p_{\\theta}(x \\mid z)\\) as a decoder.</p> <p>VAEs differ from autoencoders as follows:</p> <ul> <li>Derivations (latent variable model vs. dimensionality reduction)</li> <li>VAE regularizes/controls latent distribution, while AE does not.</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#latent-variable-model","title":"Latent variable model","text":"<p>Assumption on data \\(X_{1}, \\ldots, X_{N}\\) : Assumes there is an underlying latent variable \\(Z\\) representing the \"essential structure\" of the data and an observable variable \\(X\\) which generation is conditioned on \\(Z\\). Implicitly assumes the conditional randomness of \\(X \\sim p_{X \\mid Z}\\) is significantly smaller than the overall randomness \\(X \\sim p_{X}\\).</p> <p>Example: \\(X\\) is a cat picture. \\(Z\\) encodes information about the body position, fur color, and facial expression of a cat. Latent variable \\(Z\\) encodes the overall content of the image, but \\(X\\) does contain details not specified in \\(Z\\).</p> <p>Specification VAE's model: VAEs implements a latent variable model with a NN that generates \\(X\\) given \\(Z\\). More precisely, NN is a deterministic function that outputs the conditional distribution \\(p_{\\theta}(x \\mid Z)\\), and \\(X\\) is randomly generated according to this distribution. This structure may effectively learn the latent structure from data if the assumption on data is accurate.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#latent-variable-model_1","title":"Latent variable model","text":"<p>Sampling process:</p> \\[ X \\sim p_{\\theta}(x \\mid Z), \\quad Z \\sim p_{Z}(z) \\] <p>Usually \\(p_{Z}\\) is a Gaussian (fixed) and \\(p_{\\theta}(x \\mid z)\\) is a NN parameterized by \\(\\theta\\).</p> <p>Evaluating density (likelihood):  \\(p_{\\theta}(x \\mid z)\\)</p> \\[ p_{\\theta}\\left(X_{i}\\right)=\\int_{z} p_{Z}(z) p_{\\theta}\\left(X_{i} \\mid z\\right) d z=\\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\] <p>Training via MLE: \\(\\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]\\)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#latent-variable-model_2","title":"Latent variable model","text":"<p>When \\(p_{Z}\\) is a discrete:</p> \\[ p_{\\theta}(x)=\\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}(x \\mid Z)\\right]=\\sum_{z} p_{Z}(z) p_{\\theta}(x \\mid Z) \\] <p>When \\(p_{Z}\\) is a continuous:</p> \\[ p_{\\theta}(x)=\\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}(x \\mid Z)\\right]=\\int_{z} p_{Z}(z) p_{\\theta}(x \\mid z) d z \\] <p>To clarify, specification of \\(p_{Z}(z)\\) and \\(p_{\\theta}(x \\mid z)\\) fully determines \\(p_{\\theta}(x)\\) (as above) and</p> \\[ p_{\\theta}(z \\mid x)=\\frac{p_{\\theta}(x \\mid z) p_{Z}(z)}{p_{\\theta}(x)} \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#latent-variable-model-training","title":"Latent variable model: Training","text":"<p>Training</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\] <p>requires evaluation \\(\\mathbb{E}_{Z}\\).</p> <p>Scenario 1: If \\(Z\\) is discrete and takes a few of values, then compute \\(\\sum_{z}\\) exactly.</p> <p>Scenario 2: If \\(Z\\) takes many values or if it is a continuous, then \\(\\sum_{z}\\) or \\(\\mathbb{E}_{Z}\\) is impractical to compute. In this case, approximate expectation with Monte Carlo and importance sampling.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#example-latent-variable-model-mixture-of-gaussians","title":"Example latent variable model: Mixture of Gaussians","text":"<p>Mixture of 3 Gaussians in \\(\\mathbb{R}^{2}\\), uniform prior over components. (We can make the mixture weights a trainable parameter.)</p> \\[ \\begin{gathered} p_{Z}(Z=A)=p_{Z}(Z=B)=p_{Z}(Z=C)=\\frac{1}{3} \\\\ p_{\\theta}(x \\mid Z=k)=\\frac{1}{2 \\pi\\left|\\Sigma_{k}\\right|^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2}\\left(x-\\mu_{k}\\right)^{\\top} \\Sigma_{k}^{-1}\\left(x-\\mu_{k}\\right)\\right) \\end{gathered} \\] <p>Training objective:</p> \\[ \\begin{aligned} \\underset{\\mu, \\Sigma}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\mu, \\Sigma}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log [ &amp; \\frac{1}{3} \\frac{1}{2 \\pi\\left|\\Sigma_{A}\\right|^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2}\\left(X_{i}-\\mu_{A}\\right)^{\\top} \\Sigma_{A}^{-1}\\left(X_{i}-\\mu_{A}\\right)\\right) \\\\ &amp; +\\frac{1}{3} \\frac{1}{2 \\pi\\left|\\Sigma_{B}\\right|^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2}\\left(X_{i}-\\mu_{B}\\right)^{\\top} \\Sigma_{B}^{-1}\\left(X_{i}-\\mu_{B}\\right)\\right) \\\\ &amp; \\left.+\\frac{1}{3} \\frac{1}{2 \\pi\\left|\\Sigma_{C}\\right|^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2}\\left(X_{i}-\\mu_{C}\\right)^{\\top} \\Sigma_{C}^{-1}\\left(X_{i}-\\mu_{C}\\right)\\right)\\right] \\end{aligned} \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#example-2d-mixture-of-gaussians","title":"Example: 2D mixture of Gaussians","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#vae-outline","title":"VAE outline","text":"<p>Train latent variable model with MLE</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\] <p>Outline of variational autoencoder (VAE):</p> <ol> <li>Approximate intractable objective with a single \\(Z\\) sample</li> </ol> \\[ \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\approx \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i} \\mid Z_{i}\\right), \\quad Z_{i} \\sim p_{Z} \\] <ol> <li>Improve accuracy of approximation by sampling \\(Z_{i}\\) with importance sampling</li> </ol> \\[ \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\approx \\sum_{i=1}^{N} \\log \\frac{p_{\\theta}\\left(X_{i} \\mid Z_{i}\\right) p_{Z}\\left(Z_{i}\\right)}{q_{i}\\left(Z_{i}\\right)}, \\quad Z_{i} \\sim q_{i} \\] <ol> <li>Optimize approximate objective with SGD.</li> </ol>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#iwae-outline","title":"IWAE outline","text":"<p>Importance weighted autoencoders (IWAE) approximates intractable with \\(K\\) samples of \\(Z\\) :</p> \\[ \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\approx \\sum_{i=1}^{N} \\log \\frac{1}{K} \\sum_{k=1}^{K} \\frac{p_{\\theta}\\left(X_{i} \\mid Z_{i, k}\\right) p_{Z}\\left(Z_{i, k}\\right)}{q_{i}\\left(Z_{i, k}\\right)}, \\quad Z_{i, 1}, \\ldots, Z_{i, K} \\sim q_{i} \\] <p>More on this in hw 9.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#why-does-vae-need-is","title":"Why does VAE need IS?","text":"<p>Sampling \\(Z_{i} \\sim p_{Z}\\) results in a high-variance estimator:</p> \\[ \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\approx p_{\\theta}\\left(X_{i} \\mid Z_{i}\\right), \\] <p>In the Gaussian mixture example, only \\(1 / 3\\) of the \\(Z\\) samples meaningfully contribute to the estimate. More specifically, if \\(X_{i}\\) is near \\(\\mu_{A}\\) but is far from \\(\\mu_{B}\\) and \\(\\mu_{C}\\), then \\(p_{\\theta}\\left(X_{i} \\mid Z=A\\right) \\gg 0\\) but \\(p_{\\theta}\\left(X_{i} \\mid Z=B\\right) \\approx 0\\) and \\(p_{\\theta}\\left(X_{i} \\mid Z=C\\right) \\approx 0\\).</p> <p>The issue worsens as the observable and latent variable dimension increases.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#naively-using-is-for-each-x_i","title":"Na\u00efvely using IS for each \\(X_{i}\\)","text":"<p>To improve estimation of \\(\\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]\\), consider importance sampling (IS) with sampling distribution \\(Z_{i} \\sim q_{i}(z)\\) :</p> \\[ \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\approx p_{\\theta}\\left(X_{i} \\mid Z_{i}\\right) \\frac{p_{Z}\\left(Z_{i}\\right)}{q_{i}\\left(Z_{i}\\right)} \\] <p>Optimal IS sampling distribution</p> \\[ q_{i}^{\\star}(z)=\\frac{p_{\\theta}\\left(X_{i} \\mid z\\right) p_{Z}(z)}{p_{\\theta}\\left(X_{i}\\right)}=p_{\\theta}\\left(z \\mid X_{i}\\right) \\] <p>To clarify, optimal sampling distribution depends on \\(X_{i}\\). To clarify, \\(p_{\\theta}\\left(X_{i}\\right)\\) is the unkown normalizing factor so \\(p_{\\theta}\\left(z \\mid X_{i}\\right)\\) is also unkown. We call \\(q_{i}^{\\star}(z)=p_{\\theta}\\left(z \\mid X_{i}\\right)\\) the true posterior distribution and we will soon consider the approximation \\(q_{\\phi}(z \\mid x) \\approx p_{\\theta}(z \\mid x)\\), which we call the approximate posterior.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#naively-using-is-for-each-x_i_1","title":"Na\u00efvely using IS for each \\(X_{i}\\)","text":"<p>For each \\(X_{i}\\), consider</p> \\[ \\begin{gathered} \\underset{q_{i}}{\\operatorname{minimize}} D_{\\mathrm{KL}}\\left(q_{i}(\\cdot) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right) \\\\ =\\underset{q_{i}}{\\operatorname{minimize}} \\mathbb{E}_{Z \\sim q_{i}} \\log \\left(\\frac{q_{i}(Z)}{p_{\\theta}\\left(Z \\mid X_{i}\\right)}\\right) \\\\ =\\underset{q_{i}}{\\operatorname{minimize}} \\mathbb{E}_{Z \\sim q_{i}} \\log \\left(\\frac{q_{i}(Z)}{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z) / p_{\\theta}\\left(X_{i}\\right)}\\right) \\\\ =\\underset{Z \\sim q_{i}}{\\operatorname{minimize}}\\left[\\log q_{i}(Z)-\\log p_{Z}(Z)-\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]+\\log p_{\\theta}\\left(X_{i}\\right) \\end{gathered} \\] <p>Note, \\(q_{i}(z), p_{Z}(z)\\), and \\(p_{\\theta}(x \\mid z)\\) are tractable/known while \\(p_{\\theta}\\left(X_{i}\\right)\\) and \\(p_{\\theta}\\left(z \\mid X_{i}\\right)\\) are intractable/unknown. Since \\(\\log p_{\\theta}\\left(X_{i}\\right)\\) does not depend on \\(q_{i}\\), all quantities needed in the optimization problems are tractable. However, solving this minimization problem to obtain each \\(q_{i}\\) for each data point \\(X_{i}\\) is computationally too expensive.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#non-amortized-inference","title":"Non-amortized inference","text":"<p>Individual inference (not amortized): For each \\(X_{1}, \\ldots, X_{N}\\), find corresponding optimal \\(q_{1}, \\ldots, q_{N}\\) by solving</p> \\[ \\underset{q_{i}}{\\operatorname{minimize}} \\quad D_{\\mathrm{KL}}\\left(q_{i}(\\cdot) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right) \\] <p>This is expensive as it requires solving \\(N\\) separate optimization problems.</p> <p>We need variational approach and amortized inference.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#variational-approach-and-amortized-inference","title":"Variational approach and amortized inference","text":"<p>General principle of variational approach: We can't directly use the \\(q\\) we want. So, instead, we propose a parameterized distribution \\(q_{\\phi}\\) that we can work with easily (in this case, sample from easily), and find a parameter setting that makes it as good as possible.</p> <p>Parametrization of VAE:</p> \\[ q_{\\phi}\\left(z \\mid X_{i}\\right) \\approx q_{i}^{\\star}(z)=p_{\\theta}\\left(z \\mid X_{i}\\right) \\quad \\text { for all } i=1, \\ldots, N \\] <p>Amortized inference: Train a neural network \\(q_{\\phi}(\\cdot \\mid x)\\) such that \\(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right)\\) approximates the optimal \\(q_{i}(\\cdot)\\).</p> \\[ \\underset{\\phi \\in \\Phi}{\\operatorname{minimize}} \\sum_{i=1}^{N} D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right) \\] <p>Approximation \\(q_{\\phi}\\left(z \\mid X_{i}\\right) \\approx p_{\\theta}\\left(z \\mid X_{i}\\right)\\) is often less precise than that of individual inference \\(q_{i}(z) \\approx\\) \\(p_{\\theta}\\left(z \\mid X_{i}\\right)\\), but amortized inference is often significantly faster.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#encoder-q_phi-optimization","title":"Encoder \\(q_{\\phi}\\) optimization","text":"<p>In analogy with autoencoders, we call \\(q_{\\phi}\\) the encoder.</p> <p>Optimization problem for encoder</p> \\[ \\begin{aligned} &amp; \\underset{\\phi \\in \\Phi}{\\operatorname{minimize}} \\sum_{i=1}^{N} D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right) \\\\ &amp; \\quad=\\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right)\\right]+\\text { constant independent of } \\phi \\\\ &amp; \\quad=\\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]-D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right) \\end{aligned} \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#decoder-p_theta-optimization","title":"Decoder \\(p_{\\theta}\\) optimization","text":"<p>In analogy with autoencoders, we call \\(p_{\\theta}\\) the decoder. Perform approximate MLE with</p> \\[ \\begin{aligned} &amp; \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\\\ &amp; \\stackrel{(a)}{\\approx} \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z_{i}\\right) p_{Z}\\left(Z_{i}\\right)}{q_{\\phi}\\left(Z_{i} \\mid X_{i}\\right)}\\right), \\quad Z_{i} \\sim q_{\\phi}\\left(z \\mid X_{i}\\right) \\\\ &amp; \\stackrel{(b)}{\\approx} \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\\\ &amp; \\sum_{i=1}^{N} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right)\\right] \\\\ &amp;=\\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]-D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right) \\end{aligned} \\] <p>The \\(\\stackrel{(a)}{\\approx}\\) step replaces expectation inside the log with an estimate with \\(Z_{i}\\). The \\(\\stackrel{(b)}{\\approx}\\) step replaces the random variable with the expectation. These steps take \\(\\mathbb{E}_{Z}\\) outside of the log. More on this later.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#vae-optimization","title":"VAE optimization","text":"<p>The optimization objectives for the encoder and decoder are the same.</p> <p>Simultaneously train \\(p_{\\theta}\\) and \\(q_{\\phi}\\) by solving</p> \\[ \\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\underbrace{\\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]-D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right)}_{\\stackrel{\\text { def }}{=} \\mathrm{VLB}_{\\theta, \\phi}\\left(X_{i}\\right)} \\] <p>We refer to the optimization objective as the variational lower bound (VLB) or evidence lower bound (ELBO) for reasons that will be explained soon.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#vae-standard-instance","title":"VAE standard instance","text":"<p>A standard VAE setup: Remember from hw6 that</p> \\[ p_{Z}=\\mathcal{N}(0, I) \\quad D_{\\mathrm{KL}}\\left(\\mathcal{N}\\left(\\mu_{\\phi}(X), \\Sigma_{\\phi}(X)\\right) \\| \\mathcal{N}(0, I)\\right) \\] \\[ \\begin{aligned} &amp; q_{\\phi}(z \\mid x)=\\mathcal{N}\\left(\\mu_{\\phi}(x), \\Sigma_{\\phi}(x)\\right) \\text { with diagonal } \\Sigma_{\\phi}=\\frac{1}{2}\\left(\\operatorname{tr}\\left(\\Sigma_{\\phi}(X)\\right)+\\left\\|\\mu_{\\phi}(X)\\right\\|^{2}-d-\\log \\operatorname{det}\\left(\\Sigma_{\\phi}(X)\\right)\\right) \\\\ &amp; p_{\\theta}(x \\mid z)=\\mathcal{N}\\left(f_{\\theta}(z), \\sigma^{2} I\\right) \\end{aligned} \\] <p>\\(\\mu_{\\phi}(x), \\Sigma_{\\phi}^{2}(x)\\), and \\(f_{\\theta}(z)\\) are deterministic NN . The training objective</p> \\[ \\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]-D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right) \\] <p>becomes \\(\\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{minimize}} \\sum_{i=1}^{N} \\frac{1}{\\sigma^{2}} \\mathbb{E}_{Z \\sim \\mathcal{N}\\left(\\mu_{\\phi}\\left(X_{i}\\right), \\Sigma_{\\phi}\\left(X_{i}\\right)\\right)}\\left\\|X_{i}-f_{\\theta}(Z)\\right\\|^{2}+\\operatorname{tr}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right)+\\left\\|\\mu_{\\phi}\\left(X_{i}\\right)\\right\\|^{2}-\\log \\operatorname{det}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right)\\)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#with-reparameterization-trick","title":"With reparameterization trick","text":"<p>The standard instance of VAE \\(\\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{minimize}} \\sum_{i=1}^{N} \\frac{1}{\\sigma^{2}} \\mathbb{E}_{Z \\sim \\mathcal{N}\\left(\\mu_{\\phi}\\left(X_{i}\\right), \\Sigma_{\\phi}\\left(X_{i}\\right)\\right)}\\left\\|X_{i}-f_{\\theta}(Z)\\right\\|^{2}+\\operatorname{tr}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right)+\\left\\|\\mu_{\\phi}\\left(X_{i}\\right)\\right\\|^{2}-\\log \\operatorname{det}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right)\\) can be equivalently written with the reparameterization trick</p> \\[ \\begin{gathered} \\operatorname{minimize}_{\\theta \\in \\Theta, \\phi \\in \\Phi} \\\\ \\sum_{i=1}^{N} \\frac{1}{\\sigma^{2}} \\mathbb{E}_{\\varepsilon \\sim \\mathcal{N}(0, I)}\\left\\|X_{i}-f_{\\theta}\\left(\\mu_{\\phi}\\left(X_{i}\\right)+\\Sigma_{\\phi}^{1 / 2}\\left(X_{i}\\right) \\varepsilon\\right)\\right\\|^{2}+\\operatorname{tr}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right)+\\left\\|\\mu_{\\phi}\\left(X_{i}\\right)\\right\\|^{2}-\\log \\operatorname{det}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right) \\\\ \\text { where } \\Sigma_{\\phi}^{1 / 2} \\text { is diagonal with } \\sqrt{ } \\text { of the diagonal elements of } \\Sigma_{\\phi} \\text {. (Remember, } \\Sigma_{\\phi} \\text { is diagonal.) } \\end{gathered} \\] <p>To clarify \\(Z \\stackrel{\\mathcal{D}}{=} \\mu_{\\phi}\\left(X_{i}\\right)+\\Sigma_{\\phi}^{1 / 2}\\left(X_{i}\\right) \\varepsilon\\), where \\(\\stackrel{\\mathcal{D}}{\\underline{\\mathcal{D}}}\\) denotes equality in distribution.</p> <p>We now have an objective amenable to stochastic optimization.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#vae-standard-instance-architecture-training","title":"VAE standard instance architecture: Training","text":"<p>With reparameterization trick </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#vae-standard-instance-architecture-sampling","title":"VAE standard instance architecture: Sampling","text":"<p>During sampling, only the decoder network is used. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#discussions","title":"Discussions","text":"<p>Review of terminology</p> <ul> <li>Likelihood \\(p_{\\theta}(x)\\) (exact evaluation intractable)</li> <li>Prior \\(p_{Z}(z)\\)</li> <li>Conditional distribution \\(p_{\\theta}(x \\mid z)\\) </li> <li>True posterior \\(p_{\\theta}(z \\mid x)\\) (exact evaluation intractable)</li> <li>Approximate posterior \\(q_{\\phi}(z \\mid x)\\)</li> </ul> <p>Conditional distribution \\(p_{\\theta}(x \\mid z)\\) and prior \\(p_{Z}(z)\\) determines the posterior \\(p_{\\theta}(z \\mid x)\\).</p> <p>There is no easy way to evaluate \\(p_{\\theta}(x)\\), but we can sample \\(X \\sim p_{\\theta}(x)\\) easily: \\(Z \\sim p_{Z}(z)\\) then \\(X \\sim p_{\\theta}(x \\mid Z)\\).</p> <p>NN in VAE do not directly generate random output. NN outputs parameters for random sampling.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#training-vae-with-rt","title":"Training VAE with RT","text":"<p>To obtain stochastic gradients of the VAE objective \\(\\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{minimize}} \\quad \\sum_{i=1}^{N} \\underbrace{\\frac{1}{\\sigma^{2}} \\mathbb{E}_{\\varepsilon \\sim \\mathcal{N}(0, I)}\\left\\|X_{i}-f_{\\theta}\\left(\\mu_{\\phi}\\left(X_{i}\\right)+\\Sigma_{\\phi}^{1 / 2}\\left(X_{i}\\right) \\varepsilon\\right)\\right\\|^{2}+\\operatorname{tr}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right)+\\left\\|\\mu_{\\phi}\\left(X_{i}\\right)\\right\\|^{2}-\\log \\operatorname{det}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right)}\\) select a data \\(X_{i}\\), sample \\(\\varepsilon_{i} \\sim \\mathcal{N}(0, I)\\), evaluate \\(\\xlongequal{\\text { def }-\\mathrm{VLB}_{\\theta, \\phi}\\left(X_{i}\\right)}\\) \\(-\\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}, \\varepsilon_{i}\\right) \\xlongequal{\\text { def }} \\frac{1}{\\sigma^{2}}\\left\\|X_{i}-f_{\\theta}\\left(\\mu_{\\phi}\\left(X_{i}\\right)+\\Sigma_{\\phi}^{1 / 2}\\left(X_{i}\\right) \\varepsilon_{i}\\right)\\right\\|^{2}+\\operatorname{tr}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right)+\\left\\|\\mu_{\\phi}\\left(X_{i}\\right)\\right\\|^{2}-\\log \\operatorname{det}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right)\\) and backprop on \\(\\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}, \\varepsilon_{i}\\right)\\).</p> <p>Usually, batch of \\(X_{i}\\) is selected. One can sample multiple \\(Z_{i, 1}, \\ldots, Z_{i, K}\\) (equivalently \\(\\varepsilon_{i, 1}, \\ldots, \\varepsilon_{i, K}\\) ) for each \\(X_{i}\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#training-vae-with-log-derivative-trick","title":"Training VAE with log-derivative trick","text":"<p>Computing stochastic gradients without the reparameterization trick.</p> \\[ \\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\underbrace{\\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right)\\right]}_{\\stackrel{\\text { def }}{=} \\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right)} \\] <p>To obtain unbiased estimates of \\(\\nabla_{\\theta}\\), compute</p> \\[ \\frac{1}{K} \\sum_{k=1}^{K} \\log p_{\\theta}\\left(X_{i} \\mid Z_{i, k}\\right), \\quad Z_{i, 1}, \\ldots, Z_{i, K} \\sim q_{\\phi}\\left(z \\mid X_{i}\\right) \\] <p>and backprop with respect to \\(\\theta\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#training-vae-with-log-derivative-trick_1","title":"Training VAE with log-derivative trick","text":"<p>We differentiate the VLB objectives (cf. hw 8 problem 8)</p> \\[ \\begin{aligned} \\nabla_{\\phi} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right)\\right] &amp; =\\nabla_{\\phi} \\int \\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid z\\right) p_{Z}(z)}{q_{\\phi}\\left(z \\mid X_{i}\\right)}\\right) q_{\\phi}\\left(z \\mid X_{i}\\right) d z \\\\ &amp; =\\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\left(\\nabla_{\\phi} \\log q_{\\phi}\\left(Z \\mid X_{i}\\right)\\right) \\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right)\\right] \\end{aligned} \\] <p>To obtain unbiased estimates of \\(\\nabla_{\\phi}\\), compute</p> \\[ \\frac{1}{K} \\sum_{k=1}^{K}\\left(\\nabla_{\\phi} \\log q_{\\phi}\\left(Z_{i, k} \\mid X_{i}\\right)\\right) \\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z_{i, k}\\right) p_{Z}\\left(Z_{i, k}\\right)}{q_{\\phi}\\left(Z_{i, k} \\mid X_{i}\\right)}\\right), \\quad Z_{i, 1}, \\ldots, Z_{i, K} \\sim q_{\\phi}\\left(z \\mid X_{i}\\right) \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#why-variational-autoencoder","title":"Why variational \"autoencoder\"?","text":"<p>VAE loss (VLB) contains a reconstruction loss resembling that of an autoencoder.</p> \\[ \\begin{aligned} \\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right) &amp; =\\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]-D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right) \\\\ &amp; =-\\frac{1}{2 \\sigma^{2}} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\left\\|X_{i}-f_{\\theta}(Z)\\right\\|^{2}\\right]-D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right) \\\\ &amp; =-\\underbrace{\\frac{1}{2 \\sigma^{2}} \\mathbb{E}_{\\varepsilon \\sim \\mathcal{N}(0, I)}\\left\\|X_{i}-f_{\\theta}\\left(\\mu_{\\phi}\\left(X_{i}\\right)+\\Sigma_{\\phi}^{1 / 2}\\left(X_{i}\\right) \\varepsilon\\right)\\right\\|^{2}}_{\\text {Reconstruction loss }}-\\underbrace{D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right)}_{\\text {Regularization }} \\end{aligned} \\] <p>VLB also contains a regularization term on the output of the encoder, which is not present in standard autoencoder losses.</p> <p>The choice of \\(\\sigma\\) determines the relative weight between the reconstruction loss and the regularization.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#how-tight-is-the-vlb","title":"How tight is the VLB?","text":"<p>How accurate is the approximation?</p> \\[ \\begin{aligned} &amp; \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\frac{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right] \\\\ &amp; \\stackrel{?}{\\approx} \\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right)\\right] \\\\ &amp;=\\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right) \\end{aligned} \\] <p>This turns out that \\(\\log p_{\\theta}\\left(X_{i}\\right) \\geq \\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right)\\). So we are maximizing a lower bound of the log likelihood. How large is the gap?</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#log-likelihood-geq-vlb-derivation-1","title":"Log-likelihood \\(\\geq\\) VLB: Derivation 1","text":"<p>Derivation via Jensen:</p> \\[ \\begin{aligned} \\log p_{\\theta}\\left(X_{i}\\right) &amp; =\\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\\\ &amp; =\\log \\left(\\mathbb{E}_{Z \\sim q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right) \\frac{p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right]\\right) \\\\ &amp; \\geq \\mathbb{E}_{Z \\sim q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\left[\\log \\left(p_{\\theta}\\left(X_{i} \\mid Z\\right) \\frac{p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right)\\right] \\\\ &amp; \\stackrel{\\text { def }}{=} \\mathrm{VLB}_{\\theta, \\phi}\\left(X_{i}\\right) \\end{aligned} \\] <p>Does not explicitly characterize gap.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#log-likelihood-geq-vlb-derivation-2","title":"Log-likelihood \\(\\geq\\) VLB: Derivation 2","text":"<p>Derivation via KL divergence:</p> \\[ \\begin{aligned} D_{\\mathrm{KL}}\\left[q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right] &amp; =\\mathbb{E}_{Z \\sim q_{\\theta}\\left(z \\mid X_{i}\\right)}\\left[\\log q_{\\theta}\\left(Z \\mid X_{i}\\right)-\\log p_{\\theta}\\left(Z \\mid X_{i}\\right)\\right] \\\\ &amp; =\\underbrace{\\mathbb{E}_{Z \\sim q_{\\theta}\\left(z \\mid X_{i}\\right)}\\left[\\log q_{\\theta}\\left(Z \\mid X_{i}\\right)-\\log p_{Z}(Z)-\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]}_{=-\\mathrm{VLB}_{\\theta, \\phi}\\left(X_{i}\\right)}+\\log p_{\\theta}\\left(X_{i}\\right) \\end{aligned} \\] <p>and</p> \\[ \\begin{aligned} \\log p_{\\theta}\\left(X_{i}\\right) &amp; =\\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right)+D_{\\mathrm{KL}}\\left[q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right] \\\\ &amp; \\geq \\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right) \\end{aligned} \\] <p>This derivation explicitly characterizes the gap as \\(D_{\\mathrm{KL}}\\left[q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right]\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#vlb-is-tight-if-encoder-infinitely-powerful","title":"VLB is tight if encoder infinitely powerful","text":"<p>If the encoder \\(q_{\\phi}\\) is powerful enough such that there is a \\(\\phi^{\\star}\\) achieving</p> \\[ q_{\\phi^{\\star}}\\left(\\cdot \\mid X_{i}\\right)=p_{\\theta}\\left(\\cdot \\mid X_{i}\\right) \\] <p>or equivalently</p> \\[ D_{\\mathrm{KL}}\\left[q_{\\phi^{\\star}}\\left(\\cdot \\mid X_{i}\\right) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right]=0 \\] <p>Then</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)={\\underset{\\theta}{\\theta \\in \\Theta, \\phi \\in \\Phi}}_{\\operatorname{maximize}} \\sum_{i=1}^{N} \\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right) \\] <p>This follows from</p> \\[ \\log p_{\\theta}\\left(X_{i}\\right)=\\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right)+\\underbrace{D_{\\mathrm{KL}}\\left[q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right]}_{\\geq 0} \\] <p>and hw 8 problem 2.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#vq-vae","title":"VQ-VAE","text":"<p>Figure 2: Left: ImageNet \\(128 \\times 128 \\times 3\\) images, right: reconstructions from a VQ-VAE with a \\(32 \\times 32 \\times 1\\) latent space, with \\(\\mathrm{K}=512\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#vq-vae_1","title":"VQ-VAE","text":"<p>Figure 3: Samples (128x128) from a VQ-VAE with a PixelCNN prior trained on ImageNet images.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#vq-vae-2","title":"VQ-VAE-2","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#vq-vae-2_1","title":"VQ-VAE-2","text":"\\[ \\ell_{\\theta, \\phi}\\left(X_{i}\\right)=\\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]-\\beta D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right) \\] <p>when \\(\\beta=1, \\ell_{\\theta, \\phi}\\left(X_{i}\\right)=\\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right)\\), i.e., \\(\\beta\\)-VAE coincides with VAE when \\(\\beta=1\\).</p> <p>With \\(\\beta&gt;1\\), authors observed better feature disentanglement.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#minimax-optimization","title":"Minimax optimization","text":"<p>In a minimax optimization problem we minimize with respect to one variable and maximize with respect to another:</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathcal{L}(\\theta, \\phi) \\] <p>We say \\(\\left(\\theta^{\\star}, \\phi^{\\star}\\right)\\) is a solution to the minimax problem if \\(\\theta^{\\star} \\in \\Theta, \\phi^{\\star} \\in \\Phi\\), and</p> \\[ \\mathcal{L}\\left(\\theta^{\\star}, \\phi\\right) \\leq \\mathcal{L}\\left(\\theta^{\\star}, \\phi^{\\star}\\right) \\leq \\mathcal{L}\\left(\\theta, \\phi^{\\star}\\right), \\quad \\forall \\theta \\in \\Theta, \\phi \\in \\Phi . \\] <p>In other words, unilaterally deviating from \\(\\theta^{\\star} \\in \\Theta\\) increases the value of \\(\\mathcal{L}(\\theta, \\phi)\\) while unilaterally deviating from \\(\\phi^{\\star} \\in \\Phi\\) decreases the value of \\(\\mathcal{L}(\\theta, \\phi)\\). In yet other words, the solution is defined as a Nash equilibrium in a 2-player zero-sum game.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#minimax-optimization_1","title":"Minimax optimization","text":"<p>So far, we trained NN by solving minimization problems.</p> <p>However, GANs are trained by solving minimax problems. Since the advent of GANs, minimax training has become more widely used in all areas of deep learning.</p> <p>Examples:</p> <ul> <li>Adversarial training to make NN robust against adversarial attacks.</li> <li>Domain adversarial networks to train NN to make fair decisions (e.g. not base its decision on a persons race or gender).</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#minimax-vs-maximin","title":"Minimax vs. maximin","text":"<p>When a solution (as we defined it) does not exist, then min-max is not the same as max-min:</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathcal{L}(\\theta, \\phi) \\neq \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\mathcal{L}(\\theta, \\phi) \\] <p>This is a technical distinction that we will not explore in this class.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#minimax-optimization-algorithm","title":"Minimax optimization algorithm","text":"<p>First, consider deterministic gradient setup. Let \\(\\alpha\\) and \\(\\beta\\) be the stepsizes (learning rates) for the descent and ascent steps respectively.</p> <p>Simultaneous gradient ascent-descent:</p> \\[ \\begin{aligned} \\phi^{k+1} &amp; =\\phi^{k}+\\beta \\nabla_{\\phi} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k}\\right) \\\\ \\theta^{k+1} &amp; =\\theta^{k}-\\alpha \\nabla_{\\theta} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k}\\right) \\end{aligned} \\] <p>Alternating gradient ascent-descent:</p> \\[ \\begin{aligned} \\phi^{k+1} &amp; =\\phi^{k}+\\beta \\nabla_{\\phi} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k}\\right) \\\\ \\theta^{k+1} &amp; =\\theta^{k}-\\alpha \\nabla_{\\theta} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k+1}\\right) \\end{aligned} \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#minimax-optimization-algorithm_1","title":"Minimax optimization algorithm","text":"<p>Gradient multi-ascent-single-descent:</p> \\[ \\begin{aligned} \\phi_{0}^{k+1} &amp; =\\phi_{n_{\\mathrm{dis}}}^{k} \\\\ \\phi_{i+1}^{k+1} &amp; =\\phi_{i}^{k+1}+\\beta \\nabla_{\\phi} \\mathcal{L}\\left(\\theta^{k}, \\phi_{i}^{k+1}\\right), \\quad \\text { for } i=0, \\ldots, n_{\\mathrm{dis}}-1 \\\\ \\theta^{k+1} &amp; =\\theta^{k}-\\alpha \\nabla_{\\theta} \\mathcal{L}\\left(\\theta^{k}, \\phi_{n_{\\mathrm{dis}}}^{k+1}\\right) \\end{aligned} \\] <p>( \\(n_{\\text {dis }}\\) stands for number of discriminator updates.) When \\(n_{\\text {dis }}=1\\), this algorithm reduces to alternating ascent-descent.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#stochastic-minimax-optimization","title":"Stochastic minimax optimization","text":"<p>In deep learning, however, we have access to stochastic gradients.</p> <p>Stochastic gradient simultaneous ascent-descent</p> \\[ \\begin{array}{rlrl} \\phi^{k+1} &amp; =\\phi^{k}+\\beta g_{\\phi}^{k}, &amp; \\mathbb{E}\\left[g_{\\phi}^{k}\\right]=\\nabla_{\\phi} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k}\\right) \\\\ \\theta^{k+1} &amp; =\\theta^{k}-\\alpha g_{\\theta}^{k}, &amp; &amp; \\mathbb{E}\\left[g_{\\theta}^{k}\\right]=\\nabla_{\\theta} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k}\\right) \\end{array} \\] <p>Stochastic gradient alternating ascent-descent</p> \\[ \\begin{array}{rlrl} \\phi^{k+1} &amp; =\\phi^{k}+\\beta g_{\\phi}^{k}, &amp; \\mathbb{E}\\left[g_{\\phi}^{k}\\right]=\\nabla_{\\phi} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k}\\right) \\\\ \\theta^{k+1}=\\theta^{k}-\\alpha g_{\\theta}^{k}, &amp; \\mathbb{E}\\left[g_{\\theta}^{k}\\right]=\\nabla_{\\theta} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k+1}\\right) \\end{array} \\] <p>Stochastic gradient multi-ascent-single-descent</p> \\[ \\begin{array}{rlr} \\phi_{0}^{k+1} &amp; =\\phi_{n_{\\text {dis }}}^{k} \\\\ \\phi_{i+1}^{k+1} &amp; =\\phi_{i}^{k+1}+\\beta \\nabla_{\\phi} g_{\\phi}^{k, i}, \\quad \\mathbb{E}\\left[g_{\\phi}^{k, i}\\right]=\\nabla_{\\phi} \\mathcal{L}\\left(\\theta^{k}, \\phi_{i}^{k+1}\\right), \\quad \\text { for } i=0, \\ldots, n_{\\text {dis }}-1 \\\\ \\theta^{k+1} &amp; =\\theta^{k}-\\alpha g_{\\theta}^{k}, \\quad \\mathbb{E}\\left[g_{\\theta}^{k}\\right]=\\nabla_{\\theta} \\mathcal{L}\\left(\\theta^{k}, \\phi_{n_{\\text {dis }}^{k}}^{k+1}\\right) \\end{array} \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#minimax-optimization-in-pytorch","title":"Minimax optimization in PyTorch","text":"<p>To perform minimax optimization in PyTorch, we maintain two separate optimizers, one for the ascent, one for the descent. The OPTIMIZER can be anything like SGD or Adam.</p> <pre><code>G = Generator(...).to(device)\nD = Discriminator(...).to(device)\nD_optimizer = optim.OPTIMIZER(D.parameters(), lr = beta)\nG_optimizer = optim.OPTIMIZER(G.parameters(), lr = alpha)\n</code></pre> <p>Simultaneous ascent-descent:</p> <pre><code>Evaluate D_loss\nD_loss.backward()\nEvaluate G_loss\nG_loss.backward()\nD_optimizer.step()\nG_optimizer.step()\n</code></pre>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#minimax-optimization-in-pytorch_1","title":"Minimax optimization in PyTorch","text":"<p>Alternating ascent-descent</p> <pre><code>Evaluate D_loss\nD_loss.backward()\nD_optimizer.step()\nEvaluate G_loss\nG_loss.backward()\nG_optimizer.step()\n</code></pre>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#minimax-optimization-in-pytorch_2","title":"Minimax optimization in PyTorch","text":"<p>Multi-ascent-single-descent</p> <pre><code>for _ in range(ndis) :\n    Evaluate D loss\n    D_loss.backward()\n    D_optimizer.step()\nEvaluate G_loss\nG_loss.backward()\nG_optimizer.step()\n</code></pre>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#generative-adversarial-networks-gan","title":"Generative adversarial networks (GAN)","text":"<p>These are synthetic (fake) images. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#gan","title":"GAN","text":"<p>In generative adversarial networks (GAN) a generator network and a discriminator network compete adversarially. </p> <p>Given data \\(X_{1}, \\ldots, X_{N} \\sim p_{\\text {true }}\\). GAN aims to learn \\(p_{\\theta} \\approx p_{\\text {true }}\\).</p> <p>Generator aims to generate fake data similar to training data.</p> <p>Discriminator aims to distinguish the training data from fake data.</p> <p>Analogy: Criminal creating fake money vs. police distinguishing fake money from real.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#generator-network","title":"Generator network","text":"<p>The generator \\(G_{\\theta}: \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{n}\\) is a neural network parameterized by \\(\\theta \\in \\Theta\\). The generator takes a random latent vector \\(Z \\sim p_{Z}\\) as input and outputs generated (fake) data \\(\\tilde{X}=G_{\\theta}(Z)\\). The latent distribution is usually \\(p_{Z}=\\mathcal{N}(0, I)\\).</p> <p>Write \\(p_{\\theta}\\) for the probability distribution of \\(\\tilde{X}=G_{\\theta}(Z)\\). Although we can't evaluate the density \\(p_{\\theta}(x)\\), neither exactly nor approximately, we can sample from \\(\\tilde{X} \\sim p_{\\theta}\\). </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#discriminator-network","title":"Discriminator network","text":"<p>The discriminator \\(D_{\\phi}: \\mathbb{R}^{n} \\rightarrow(0,1)\\) is a neural network parameterized by \\(\\phi \\in \\Phi\\). The discriminator takes an image \\(X\\) as input and outputs whether \\(X\\) is a real or fake.#</p> <ul> <li>\\(D_{\\phi}(X) \\approx 1\\) : discriminator confidently predicts \\(X\\) is real.</li> <li>\\(D_{\\phi}(X) \\approx 0\\) : discriminator confidently predicts \\(X\\) is fake.</li> <li>\\(D_{\\phi}(X) \\approx 0.5\\) : discriminator is unsure whether \\(X\\) is real or fake.  </li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#discriminator-loss","title":"Discriminator loss","text":"<p>Cost of incorrectly classifying real as fake (type I error):</p> \\[ \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[-\\log D_{\\phi}(X)\\right] \\] <p>Cost of incorrectly classifying fake as real (type II error):</p> \\[ \\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}\\left[-\\log \\left(1-D_{\\phi}(\\tilde{X})\\right)\\right]=\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[-\\log \\left(1-D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\] <p>Discriminator solves</p> \\[ \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\quad \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log D_{\\phi}(X)\\right]+\\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}\\left[\\log \\left(1-D_{\\phi}(\\tilde{X})\\right)\\right] \\] <p>which is equivalent to</p> \\[ \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\quad \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log D_{\\phi}(X)\\right]+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[\\log \\left(1-D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#discriminator-loss_1","title":"Discriminator loss","text":"<p>We can view</p> \\[ \\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}\\left[\\log \\left(1-D_{\\phi}(\\tilde{X})\\right)\\right]=\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[\\log \\left(1-D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\] <p>as an instance of the reparameterization technique.</p> <p>The loss</p> \\[ \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log D_{\\phi}(X)\\right]+\\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}\\left[\\log \\left(1-D_{\\phi}(\\tilde{X})\\right)\\right] \\] <p>puts equal weight on type I and type II errors. Alternatively, one can use the loss</p> \\[ \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log D_{\\phi}(X)\\right]+\\lambda \\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}\\left[\\log \\left(1-D_{\\phi}(\\tilde{X})\\right)\\right] \\] <p>where \\(\\lambda&gt;0\\) represents the relative significance of a type II error over a type I error.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#generator-loss","title":"Generator loss","text":"<p>Since the goal of the generator is to deceive the discriminator, the generator minimizes the same loss.</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log D_{\\phi}(X)\\right]+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[\\log \\left(1-D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\] <p>(The generator and discriminator operate under a zero-sum game.)</p> <p>Note, only the second term depend on \\(\\theta\\), while the both terms depend on \\(\\phi\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#empirical-risk-minimization","title":"Empirical risk minimization","text":"<p>In practice, we have finite samples \\(X_{1}, \\ldots, X_{N}\\), so we instead use the loss</p> \\[ \\frac{1}{N} \\sum_{i=1}^{N} \\log D_{\\phi}\\left(X_{i}\\right)+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[\\log \\left(1-D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\] <p>Since \\(\\tilde{X}=G_{\\theta}(Z)\\) is generated with \\(Z \\sim p_{Z}\\), we have unlimited \\(\\tilde{X}\\) samples. So we replace \\(\\mathbb{E}_{X} \\approx \\frac{1}{N} \\sum\\) while leaving \\(\\mathbb{E}_{Z}\\) as is.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#minimax-training-zero-sum-game","title":"Minimax training (zero-sum game)","text":"<p>Train generator and discriminator simultaneously by solving</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathcal{L}(\\theta, \\phi) \\] <p>where</p> \\[ \\mathcal{L}(\\theta, \\phi)=\\frac{1}{N} \\sum_{i=1}^{N} \\log D_{\\phi}\\left(X_{i}\\right)+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[\\log \\left(1-D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\] <p>It remains to specify the architectures for \\(G_{\\theta}\\) and \\(D_{\\phi}\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#gan-demo","title":"GAN demo","text":"<p>PyTorch demo</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#dcgan","title":"DCGAN","text":"<p>The original GAN was also deep and convolutional. However, Radford et al.'s Deep Convolutional Generative Adversarial Networks (DCGAN) paper proposed the following architectures, which crucially utilize batchnorm. </p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#math-review-f-divergence","title":"Math review: f-divergence","text":"<p>The f -divergence of \\(p\\) from \\(q\\), where \\(f\\) is a convex function such that \\(f(1)=0\\), is</p> \\[ D_{f}(p \\| q)=\\int f\\left(\\frac{p(x)}{q(x)}\\right) q(x) d x, \\] <p>This includes the KL divergence:</p> <ul> <li>If \\(f(u)=u \\log u\\), then \\(D_{f}(p \\| q)=D_{\\mathrm{KL}}(p \\| q)\\).</li> <li>If \\(f(u)=-\\log u\\), then \\(D_{f}(p \\| q)=D_{\\text {KL }}(q \\| p)\\).</li> </ul>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#math-review-js-divergence","title":"Math review: JS-divergence","text":"<p>Jensen-Shannon-divergence (JS-divergence) is</p> \\[ D_{\\mathrm{JS}}(p, q)=\\frac{1}{2} D_{\\mathrm{KL}}\\left(p \\| \\frac{1}{2}(p+q)\\right)+\\frac{1}{2} D_{\\mathrm{KL}}\\left(q \\| \\frac{1}{2}(p+q)\\right) \\] <p>With, \\(f(u)=\\left\\{\\begin{array}{ll}\\frac{1}{2}\\left(u \\log u-(u+1) \\log \\frac{u+1}{2}\\right) &amp; \\text { for } u \\geq 0 \\\\ \\infty &amp; \\text { otherwise }\\end{array}\\right.\\) we have \\(D_{f}=D_{\\mathrm{IS}}\\).</p> <p>With, \\(f(u)=\\left\\{\\begin{array}{ll}u \\log u-(u+1) \\log (u+1)+\\log 4 &amp; \\text { for } u \\geq 0 \\\\ \\infty &amp; \\text { otherwise }\\end{array}\\right.\\) we have \\(D_{f}=2 D_{\\mathrm{JS}}\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#gan-approx-jsd-minimization","title":"GAN \\(\\approx\\) JSD minimization","text":"<p>Let us understand the minimax problem</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathcal{L}(\\theta, \\phi) \\] <p>via the minimization problem</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\mathcal{J}(\\theta) \\] <p>where</p> \\[ \\mathcal{J}(\\theta)=\\sup _{\\phi \\in \\Phi} \\mathcal{L}(\\theta, \\phi) \\] <p>For simplicity, assume the discriminator is infinitely powerful, i.e., \\(D_{\\phi}(x)\\) can represent any arbitrary function.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#gan-approx-jsd-minimization_1","title":"GAN \\(\\approx\\) JSD minimization","text":"<p>Note</p> \\[ \\begin{aligned} \\mathcal{L}(\\theta, \\phi) &amp; =\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log D_{\\phi}(X)\\right]+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[\\log \\left(1-D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\\\ &amp; =\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log D_{\\phi}(X)\\right]+\\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}\\left[\\log \\left(1-D_{\\phi}(\\tilde{X})\\right)\\right] \\\\ &amp; =\\int p_{\\text {true }}(x) \\log D_{\\phi}(x)+p_{\\theta}(x) \\log \\left(1-D_{\\phi}(x)\\right) d x \\end{aligned} \\] <p>Since</p> \\[ \\frac{d}{d y}(a \\log y+b \\log (1-y))=0 \\quad \\Rightarrow \\quad y^{\\star}=\\frac{a}{a+b} \\] <p>The integral is maximized by</p> \\[ D_{\\phi^{\\star}}(x)=\\frac{p_{\\text {true }}(x)}{p_{\\text {true }}(x)+p_{\\theta}(x)} \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#g-a-n-approx-j-s-d-minimization","title":"\\(G A N \\approx J S D\\) minimization","text":"<p>If we plug in the optimal discriminator,</p> \\[ D_{\\phi^{\\star}}(x)=\\frac{p_{\\text {true }}(x)}{p_{\\text {true }}(x)+p_{\\theta}(x)} \\] <p>we get</p> \\[ \\begin{aligned} \\mathcal{L}\\left(\\theta, \\phi^{\\star}\\right) &amp; =\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log \\frac{p_{\\text {true }}(X)}{p_{\\text {true }}(X)+p_{\\theta}(X)}\\right]+\\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}\\left[\\log \\frac{p_{\\theta}(\\tilde{X})}{p_{\\text {true }}(\\tilde{X})+p_{\\theta}(\\tilde{X})}\\right] \\\\ &amp; =2 D_{\\mathrm{JS}}\\left(p_{\\text {true }}, p_{\\theta}\\right)-\\log (4) \\end{aligned} \\] <p>Therefore,</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathcal{L}(\\theta, \\phi) \\approx \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} D_{\\mathrm{JS}}\\left(p_{\\text {true }}, p_{\\theta}\\right) \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#f-gan","title":"f-GAN","text":"<p>With GANs, we started from a minimax formulation and later reinterpreted it as minimizing the JS-divergence.</p> <p>Let us instead the start from an f-divergence minimization</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad D_{f}\\left(p_{\\text {true }} \\| p_{\\theta}\\right) \\] <p>and then variationally approximate \\(D_{f}\\) to obtain a minimax formulation.</p> <p>Variational approach: Evaluating \\(D_{f}\\) directly is difficult, so we pose it as a maximization problem and parameterize the maximizing function as a \"discriminator\" neural network.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#f-gan_1","title":"f-GAN","text":"<p>For simplicity, however, we only consider the order</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad D_{f}\\left(p_{\\text {true }} \\| p_{\\theta}\\right) \\] <p>However, one can also consider</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad D_{f}\\left(p_{\\theta} \\| p_{\\text {true }}\\right) \\] <p>to obtain similar results. (During our coverage of f-GANs, we will have notational conflict between \\(D_{f}\\), the fdivergence, and \\(D_{\\phi}\\), the discriminator network. Hopefully there won't be any confusion.)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#convex-conjugate","title":"Convex conjugate","text":"<p>Let \\(f: \\mathbb{R} \\rightarrow \\mathbb{R} \\cup\\{\\infty\\}\\). Define the convex conjugate of \\(f\\) as</p> \\[ f^{*}(t)=\\sup _{u \\in \\mathbb{R}}\\{t u-f(u)\\} \\] <p>where \\(f^{*}: \\mathbb{R} \\rightarrow \\mathbb{R} \\cup\\{\\infty\\}\\). This is also referred to as the Legendre transform.</p> <p>If \\(f\\) is a nice \\({ }^{\\#}\\) convex function, then \\(f^{*}\\) is convex and \\(f^{* *}=f\\), i.e., the conjugate of the conjugate is the original function. \\({ }^{\\%}\\) So</p> \\[ f(u)=\\sup _{t \\in \\mathbb{R}}\\left\\{t u-f^{*}(t)\\right\\} \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#convex-conjugate-examples","title":"Convex conjugate: Examples","text":"<p>The following are some examples. Computation of \\(f^{*}\\) uses basic calculus.</p> \\[ \\begin{aligned} &amp; f_{\\mathrm{KL}}(u)= \\begin{cases}u \\log u &amp; \\text { for } u \\geq 0 \\\\ \\infty &amp; \\text { otherwise } \\\\ -\\log u &amp; \\text { for } u&gt;0 \\\\ \\infty &amp; \\text { otherwise }\\end{cases} \\\\ &amp; f_{\\mathrm{LK}}(u)=\\left\\{\\begin{array}{ll} \\mathrm{KL} \\end{array}\\right)=\\exp (t-1) \\\\ &amp; f_{\\mathrm{SH}}(u)=(\\sqrt{u}-1)^{2} \\\\ &amp; f_{\\mathrm{JS}}(u)= \\begin{cases}-1-\\log (-t) &amp; \\text { for } t&lt;0 \\\\ \\infty &amp; f_{\\mathrm{SH}}^{*}(t)= \\begin{cases}\\frac{1}{1 / t-1} &amp; \\text { for } t&lt;1 \\\\ \\infty \\log u-(u+1) \\log (u+1)+\\log 4 &amp; \\text { for } u \\geq 0 \\\\ \\infty &amp; \\text { otherwise }\\end{cases} \\\\ f_{\\mathrm{JS}}^{*}(t)= \\begin{cases}-\\log (1-\\exp (t))-\\log 4 &amp; \\text { for } t&lt;0 \\\\ \\infty &amp; \\text { otherwise }\\end{cases} \\end{cases} \\end{aligned} \\] <p>(Keeping track of the \\(\\infty\\) output is necessary.) \\(K L=K L\\), LK=reverse-KL, SH=squared Hellinger distance, JS=JS</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#convex-conjugate-examples_1","title":"Convex conjugate: Examples","text":"<p>We get the following f-divergences: \\(D_{f_{\\mathrm{KL}}}(p \\| q)=D_{\\mathrm{KL}}(p \\| q)\\)</p> \\[ \\begin{aligned} &amp; D_{f_{\\mathrm{LK}}}(p \\| q)=D_{\\mathrm{KL}}(q \\| p) \\\\ &amp; D_{f_{\\mathrm{SH}}}(p \\| q)=D_{\\mathrm{SH}}(q, p) \\\\ &amp; D_{f_{\\mathrm{JS}}}(p \\| q)=2 D_{\\mathrm{JS}}(q, p) \\end{aligned} \\] <p>We don't use the following property, but it's interesting so we mention it. If \\(f\\) and \\(f^{*}\\) are differentiable, then \\(\\left(f^{\\prime}\\right)^{-1}=\\left(f^{*}\\right)^{\\prime}\\) :</p> \\[ \\begin{array}{ll} \\frac{d}{d u} f_{\\mathrm{KL}}(u)=1+\\log u &amp; \\frac{d}{d t} f_{\\mathrm{KL}}^{*}(t)=\\exp (t-1) \\\\ \\frac{d}{d u} f_{\\mathrm{LK}}(u)=-\\frac{1}{u} &amp; \\frac{d}{d t} f_{\\mathrm{LK}}^{*}(t)=-\\frac{1}{t} \\\\ \\frac{d}{d u} f_{\\mathrm{SH}}(u)=1-\\frac{1}{\\sqrt{u}} &amp; \\frac{d}{d t} f_{\\mathrm{SH}}^{*}(t)=\\frac{1}{(1-t)^{2}} \\\\ \\frac{d}{d u} f_{\\mathrm{JS}}(u)=\\log \\frac{u}{1+u} &amp; \\frac{d}{d t} f_{\\mathrm{JS}}^{*}(t)=\\frac{1}{e^{-t}-1} \\end{array} \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#variational-formulation-of-f-divergence","title":"Variational formulation of f-divergence","text":"<p>Variational formulation of f-divergence:</p> \\[ \\begin{aligned} D_{f}(p \\| q) &amp; =\\int q(x) f\\left(\\frac{p(x)}{q(x)}\\right) d x \\\\ &amp; =\\int q(x) \\sup _{t}\\left\\{t \\frac{p(x)}{q(x)}-f^{*}(t)\\right\\} d x=\\int q(x) T^{\\star}(x) \\frac{p(x)}{q(x)}-q(x) f^{*}\\left(T^{\\star}(x)\\right) d x \\\\ &amp; =\\sup _{T \\in \\mathcal{T}}\\left(\\int p(x) T(x) d x-\\int q(x) f^{*}(T(x)) d x\\right)=\\sup _{T \\in \\mathcal{T}}\\left(\\mathbb{E}_{X \\sim p}[T(X)]-\\mathbb{E}_{\\tilde{X} \\sim q}\\left[f^{*}(T(\\tilde{X}))\\right]\\right) \\\\ &amp; \\geq \\sup _{\\phi \\in \\Phi}\\left(\\mathbb{E}_{X \\sim p}\\left[D_{\\phi}(X)\\right]-\\mathbb{E}_{\\tilde{X} \\sim q}\\left[f^{*}\\left(D_{\\phi}(\\tilde{X})\\right)\\right]\\right) \\end{aligned} \\] <p> \\(D_{\\phi}\\) is a neural network parameterized by \\(\\phi\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#f-gan-minimax-formulation","title":"f-GAN minimax formulation","text":"<p>Minimax formulation of f-GANs.</p> \\[ \\begin{gathered} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} D_{f}\\left(p_{\\text {true }} \\| p_{\\theta}\\right) \\\\ \\approx \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[D_{\\phi}(X)\\right]-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[f^{*}\\left(D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\end{gathered} \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#f-gan-with-kl-divergence","title":"f-GAN with KL-divergence","text":"<p>Instantiate f-GAN with KL-divergence: \\(f^{*}(t)=e^{t-1}\\).</p> \\[ \\begin{aligned} &amp; \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad D_{\\mathrm{KL}}\\left(p_{\\text {true }} \\| p_{\\theta}\\right) \\\\ &amp; \\approx \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[D_{\\phi}(X)\\right]-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[e^{D_{\\phi}\\left(G_{\\theta}(Z)\\right)-1}\\right] \\\\ &amp; \\stackrel{(*)}{=} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} 1+\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[D_{\\phi}(X)\\right]-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[e^{D_{\\phi}\\left(G_{\\theta}(Z)\\right)}\\right] \\\\ &amp; =\\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[D_{\\phi}(X)\\right]-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[e^{D_{\\phi}\\left(G_{\\theta}(Z)\\right)}\\right] \\end{aligned} \\] <p>Step (*) uses the substitution \\(D_{\\phi} \\mapsto D_{\\phi}+1\\), which is valid if the final layer of \\(D_{\\phi}\\) has a trainable bias term. ( \\(D_{\\phi}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}\\).)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#f-gan-with-squared-hellinger","title":"f-GAN with squared Hellinger","text":"<p>Instantiate f-GAN with squared Hellinger distance \\(\\#: f^{*}(t)= \\begin{cases}\\frac{1}{1 / t-1} &amp; \\text { if } t&lt;1 \\\\ \\infty &amp; \\text { otherwise }\\end{cases}\\)</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad D_{\\mathrm{SH}}\\left(p_{\\text {true }}, p_{\\theta}\\right) \\] \\[ \\approx \\begin{array}{lll} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} &amp; \\begin{array}{l} \\operatorname{maximize} \\\\ \\\\ \\\\ \\text { subject to } \\end{array} &amp; \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[D_{\\phi}(X)\\right]-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[\\frac{1}{1 /\\left(D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)-1}\\right] \\\\ \\left.D_{\\theta}(z)\\right)&lt;1 \\text { for all } z \\in \\mathbb{R}^{k} \\end{array} \\] <p>When the constraint is violated, the \\(f^{*}(t)=\\infty\\) case makes the maximization objective \\(-\\infty\\). However, directly enforcing the neural networks to satisfy \\(D_{\\phi}\\left(G_{\\theta}(z)\\right)&lt;1\\) is awkward.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#solution-output-activation-rho","title":"Solution: Output activation \\(\\rho\\)","text":"<p>When \\(D_{\\phi}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}\\) and \\(\\left\\{t \\mid f^{*}(t)&lt;\\infty\\right\\} \\neq \\mathbb{R}\\), then \\(f^{*}\\left(D_{\\phi}(\\tilde{X})\\right)=\\infty\\) is possible. To prevent this, substitute \\(T(x) \\mapsto \\rho(\\tilde{T}(x))\\), where \\(\\rho: \\mathbb{R} \\rightarrow\\left\\{t \\mid f^{*}(t)&lt;\\infty\\right\\}\\) is a one-to-one function:</p> \\[ \\begin{aligned} D_{f}(p \\| q) &amp; =\\sup _{T \\in \\mathcal{T}}\\left\\{\\mathbb{E}_{X \\sim p}[T(X)]-\\mathbb{E}_{\\tilde{X} \\sim q}\\left[f^{*}(T(\\tilde{X}))\\right]\\right\\} \\\\ &amp; \\stackrel{(*)}{=} \\sup _{\\substack{T \\in \\mathcal{T}}}\\left\\{\\mathbb{E}_{X \\sim p}[T(X)]-\\mathbb{E}_{\\tilde{X} \\sim q}\\left[f^{*}(T(\\tilde{X}))\\right]\\right\\} \\\\ &amp; \\stackrel{(* *)}{=} \\sup _{\\tilde{T} \\in \\mathcal{T}}\\left\\{\\mathbb{E}_{X \\sim p}[\\rho(\\tilde{T}(X))]-\\mathbb{E}_{\\tilde{X} \\sim q}\\left[f^{*}(\\rho(\\tilde{T}(\\tilde{X})))\\right]\\right\\} \\\\ &amp; \\geq \\sup _{\\phi \\in \\Phi}\\left\\{\\mathbb{E}_{X \\sim p}\\left[\\rho\\left(D_{\\phi}(X)\\right)\\right]-\\mathbb{E}_{\\tilde{X} \\sim q}\\left[f^{*}\\left(\\rho\\left(D_{\\phi}(\\tilde{X})\\right)\\right)\\right]\\right\\} \\end{aligned} \\] <p>() We can restrict the search over \\(T\\) since if \\(f^{*}(T(x))=\\infty\\), then the objective becomes \\(-\\infty\\).# (*) With \\(T=\\rho \\circ \\tilde{T}\\), have \\(\\left[T \\in \\mathcal{T}\\right.\\) and \\(\\left.f^{*}(T(x))&lt;\\infty\\right] \\Leftrightarrow[\\tilde{T} \\in \\mathcal{T}]\\) since \\(\\rho\\) is one-to-one.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#f-gan-with-output-activation","title":"f-GAN with output activation","text":"<p>Formulate f-GAN with output activation function \\(\\rho\\) :</p> \\[ \\begin{gathered} \\operatorname{minimize}_{\\theta \\in \\Theta}^{\\min } \\quad D_{f}\\left(p_{\\text {true }} \\| p_{\\theta}\\right) \\\\ \\approx \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\quad \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\rho\\left(D_{\\phi}(X)\\right)\\right]-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[f^{*}\\left(\\rho\\left(D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right)\\right] \\end{gathered} \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#f-gan-with-squared-hellinger_1","title":"f-GAN with squared Hellinger","text":"<p>Instantiate f-GAN with squared Hellinger distance using \\(\\rho(r)=1-e^{-r}\\) and</p> \\[ f^{*}(t)= \\begin{cases}\\frac{1}{1 / t-1} &amp; \\text { if } t&lt;1 \\\\ \\infty &amp; \\text { otherwise }\\end{cases} \\] <p>Note that \\(f^{*}(\\rho(r))=-1+e^{r}\\).</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad D_{\\mathrm{SH}}\\left(p_{\\text {true }}, p_{\\theta}\\right) \\] \\[ \\begin{array}{cc} \\approx \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} &amp; \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\\\ =\\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} &amp; 2-\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[e^{-D_{\\phi}(X)}\\right]-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[e^{D_{\\phi}\\left(G_{\\theta}(Z)\\right)}\\right] \\\\ \\underset{\\phi \\in \\Phi}{ } \\operatorname{maximize}-\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[e^{-D_{\\phi}(X)}\\right]-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[e^{D_{\\phi}\\left(G_{\\theta}(Z)\\right)}\\right] \\end{array} \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#f-gan-with-reverse-kl","title":"f-GAN with reverse KL","text":"<p>Instantiate f-GAN with reverse KL using \\(\\rho(r)=-e^{r}\\) and</p> \\[ f^{*}(t)= \\begin{cases}-1-\\log (-t) &amp; \\text { if } t&lt;0 \\\\ \\infty &amp; \\text { otherwise }\\end{cases} \\] <p>Note that \\(f^{*}(\\rho(r))=-1-r\\).</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad D_{\\mathrm{KL}}\\left(p_{\\theta} \\| p_{\\text {true }}\\right) \\] \\[ \\begin{aligned} &amp; \\approx \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} 1-\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[e^{D_{\\phi}(X)}\\right]+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right] \\\\ &amp; =\\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}}-\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[e^{D_{\\phi}(X)}\\right]+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right] \\end{aligned} \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#recovering-standard-gan","title":"Recovering standard GAN","text":"<p>We recover standard GAN with</p> \\[ \\rho(r)=\\log (\\sigma(r)), \\quad \\sigma(r)=\\frac{1}{1+e^{-r}}, \\quad f^{*}(t)= \\begin{cases}-\\log (1-\\exp (t))-\\log 4 &amp; \\text { for } t&lt;0 \\\\ \\infty &amp; \\text { otherwise }\\end{cases} \\] <p>Note that \\(\\sigma\\) is the familiar sigmoid and</p> \\[ f^{*}(\\rho(r))=-\\log (1-\\sigma(r))-\\log 4 \\] \\[ \\begin{gathered} \\operatorname{minimize}_{\\theta \\in \\Theta} D_{\\mathrm{JS}}\\left(p_{\\text {true }}, p_{\\theta}\\right) \\\\ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log \\sigma\\left(D_{\\phi}(X)\\right)\\right]+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[\\log \\left(1-\\sigma\\left(D_{\\phi}\\left(G_{\\theta}(X)\\right)\\right)\\right)\\right] \\end{gathered} \\] <p>where \\(D_{\\phi}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}\\). (Standard GAN has \\(D_{\\phi}: \\mathbb{R}^{n} \\rightarrow(0,1)\\). Here, \\(\\left(\\sigma \\circ D_{\\phi}\\right): \\mathbb{R}^{n} \\rightarrow(0,1)\\) serves the same purpose.)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#wgan","title":"WGAN","text":"<p>The Wasserstein GAN (WGAN) minimizes the Wasserstein distance:</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad W\\left(p_{\\text {true }}, p_{\\theta}\\right) \\] <p>The \\(W(p, q)\\) is a distance (metric) on probability distributions defined as</p> \\[ W(p, q)=\\inf _{f} \\mathbb{E}_{(X, Y) \\sim f(x, y)}\\|X-Y\\| \\] <p>where the infimum is taken over joint probability distributions \\(f\\) with marginals \\(p\\) and \\(q\\), i.e.,</p> \\[ p(x)=\\int f(x, y) d y, \\quad q(y)=\\int f(x, y) d x \\] <p>(The mathematics of \\(W(p, q)\\) exceeds the scope of this class, but I still want to give you a high-level exposure to WGANs.)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#wp-q-by-optimal-transport","title":"\\(W(p, q)\\) by optimal transport","text":"<p>Another equivalent formulation of the Wasserstein distance is by the theory of optimal transport. Given distributions \\(p\\) and \\(q\\) (initial and target)</p> \\[ W(p, q)=\\inf _{T} \\int\\|x-T(x)\\| p(x) d x \\] <p>where \\(T\\) is a transport plan that transports \\(p\\) to \\(q .{ }^{\\%}\\) Figuratively speaking, we are transporting grains of sand from one pile to another, and we wan to minimize the aggregate transport distance. </p> <p>Transport plan \\(T\\) from \\(p\\) to \\(q\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#minimax-via-kr-duality","title":"Minimax via KR duality","text":"<p>Kantorovich-Rubinstein duality# establishes:</p> \\[ W\\left(p_{\\text {true }}, p_{\\theta}\\right)=\\sup _{\\|T\\|_{L \\leq 1}} \\mathbb{E}_{X \\sim p_{\\text {true }}}[T(X)]-\\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}[T(\\tilde{X})] \\] <p>Minimax formulation of WGAN:</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} W\\left(p_{\\text {true }}, p_{\\theta}\\right) \\] \\[ \\approx \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\begin{array}{ll} \\operatorname{maximize} \\\\ \\text { subject to } &amp; \\mathbb{E}_{X \\sim p_{\\text {truu }}}\\left[D_{\\phi}(X)\\right]-\\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}} \\text { is 1-Lipschitz } \\end{array} \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#spectral-normalization","title":"Spectral normalization","text":"<p>How do we enforce the constraint that \\(D_{\\phi}\\) is 1-Lipschitz? Consider an MLP:</p> \\[ \\begin{aligned} y_{L}= &amp; A_{L} y_{L-1}+b_{L} \\\\ y_{L-1}= &amp; \\sigma\\left(A_{L-1} y_{L-2}+b_{L-1}\\right) \\\\ &amp; \\vdots \\\\ y_{2}= &amp; \\sigma\\left(A_{2} y_{1}+b_{2}\\right) \\\\ y_{1}= &amp; \\sigma\\left(A_{1} x+b_{1}\\right), \\end{aligned} \\] <p>where \\(\\sigma\\) is a 1-Lipschitz continuous activation function, such as ReLU and tanh. If</p> \\[ \\left\\|A_{i}\\right\\|_{\\mathrm{op}}=\\sigma_{\\max }\\left(A_{i}\\right) \\leq 1 \\] <p>for \\(i=1, \\ldots, L\\), where \\(\\sigma_{\\text {max }}\\) denotes the largest singular value, then each layer is 1-Lipschitz continuous and the entire mapping \\(x \\mapsto y_{L}\\) is 1 -Lipschitz. (A sufficient, but not a necessary, condition.)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#spectral-normalization_1","title":"Spectral normalization","text":"<p>Replace Lipschitz constraint with a singular-value constraint</p> \\[ \\begin{array}{rll} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} &amp; \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} &amp; \\frac{1}{N} \\sum_{i=1}^{N} D_{\\phi}\\left(X_{i}\\right)-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right] \\\\ &amp; \\text { subject to } &amp; \\sigma_{\\max }\\left(A_{i}\\right) \\leq 1, \\quad i=1, \\ldots, L \\end{array} \\] <p>Constraint is handled with a projected gradient method. (Note that \\(A_{1}, \\ldots, A_{L}\\) are part of the discriminator parameters \\(\\phi\\).) (Specifically, one performs an (approximate) projection after the ascent step in the stochastic gradient ascent-descent methods. The approximate projection involves computing the largest singular with the power iteration.)</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#conclusion_1","title":"Conclusion","text":"<p>We discussed the following unsupervised learning techniques:</p> <ul> <li>Autoencoders</li> <li>Flow models</li> <li>Variational autoencoders</li> <li>GANs</li> </ul> <p>Unsupervised learning techniques, particularly generative models, tend to utilize more math in their formulations. This chapter provided a brief and gentle introduction to the mathematical foundations of these formulations.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#appendix-a-basics-of-monte-carlo","title":"Appendix A:  Basics of Monte Carlo","text":"<p>Mathematical Foundations of Deep Neural Networks Spring 2024 Department of Mathematical Sciences Ernest K. Ryu Seoul National University</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#monte-carlo","title":"Monte Carlo","text":"<p>We quickly cover some basic notions of Monte Carlo simulations.</p> <p>These concepts will be used with VAEs.</p> <p>These ideas are also extensively used in reinforcement learning (although not a topic of this course).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#monte-carlo-estimation","title":"Monte Carlo estimation","text":"<p>Consider IID data \\(X_{1}, \\ldots, X_{N} \\sim f\\). Let \\(\\phi(X) \\geq 0\\) be some function*. Consider the problem of estimating</p> \\[ I=\\mathbb{E}_{X \\sim f}[\\phi(X)]=\\int \\phi(x) f(x) d x \\] <p>One commonly uses</p> \\[ \\hat{I}_{N}=\\frac{1}{N} \\sum_{i=1}^{N} \\phi\\left(X_{i}\\right) \\] <p>to estimate \\(I\\). After all, \\(\\mathbb{E}\\left[\\hat{I}_{N}\\right]=I\\) and \\(\\hat{I}_{N} \\rightarrow I\\) by the law of large numbers.#</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#monte-carlo-estimation_1","title":"Monte Carlo estimation","text":"<p>We can quantify convergence with variance:</p> \\[ \\operatorname{Var}_{X \\sim f}\\left(\\hat{I}_{N}\\right)=\\sum_{i=1}^{N} \\operatorname{Var}_{X_{i} \\sim f}\\left(\\frac{\\phi\\left(X_{i}\\right)}{N}\\right)=\\frac{1}{N} \\operatorname{Var}_{X \\sim f}(\\phi(X)) \\] <p>In other words</p> \\[ \\mathbb{E}\\left[\\left(\\hat{I}_{N}-I\\right)^{2}\\right]=\\frac{1}{N} \\operatorname{Var}_{X \\sim f}(\\phi(X)) \\] <p>and</p> \\[ \\mathbb{E}\\left[\\left(\\hat{I}_{N}-I\\right)^{2}\\right] \\rightarrow 0 \\] <p>as \\(N \\rightarrow \\infty\\).#</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#empirical-risk-minimization_1","title":"Empirical risk minimization","text":"<p>In machine learning and statistics, we often wish to solve</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad \\mathcal{L}(\\theta) \\] <p>where the objective function</p> \\[ \\mathcal{L}(\\theta)=\\mathbb{E}_{X \\sim p_{X}}\\left[\\ell\\left(f_{\\theta}(X), f_{\\star}(X)\\right)\\right] \\] <p>Is the (true) risk. However, the evaluation of \\(\\mathbb{E}_{X \\sim p_{X}}\\) is impossible (if \\(p_{X}\\) is unknown) or intractable (if \\(p_{X}\\) is known but the expectation has no closed-form solution). Therefore, we define the proxy loss function</p> \\[ \\mathcal{L}_{N}(\\theta)=\\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(f_{\\theta}\\left(X_{i}\\right), f_{\\star}\\left(X_{i}\\right)\\right) \\] <p>which we call the empirical risk, and solve</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad \\mathcal{L}_{N}(\\theta) \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#empirical-risk-minimization_2","title":"Empirical risk minimization","text":"<p>This is called empirical risk minimization (ERM). The idea is that</p> \\[ \\mathcal{L}_{N}(\\theta) \\approx \\mathcal{L}(\\theta) \\] <p>with high probability, so minimizing \\(\\mathcal{L}_{N}(\\theta)\\) should be similar to minimizing \\(\\mathcal{L}(\\theta)\\).</p> <p>Technical note) The law of large numbers tells us that</p> \\[ \\mathbb{P}\\left(\\left|\\mathcal{L}_{N}(\\theta)-\\mathcal{L}(\\theta)\\right|&gt;\\varepsilon\\right)=\\text { small } \\] <p>for any given \\(\\theta\\), but we need</p> \\[ \\mathbb{P}\\left(\\sup _{\\theta \\in \\Theta}\\left|\\mathcal{L}_{N}(\\theta)-\\mathcal{L}(\\theta)\\right|&gt;\\varepsilon\\right)=\\text { small } \\] <p>for all compact \\(\\Theta\\) in order to conclude that the argmins of the two losses to be similar. These types of results are established by a uniform law of large numbers.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#importance-sampling","title":"Importance sampling","text":"<p>Importance sampling (IS) is a technique for reducing the variance of a Monte Carlo estimator.</p> <p>Key insight of important sampling:</p> \\[ I=\\int \\phi(x) f(x) d x=\\int \\frac{\\phi(x) f(x)}{g(x)} g(x) d x=\\mathbb{E}_{X \\sim g}\\left[\\frac{\\phi(X) f(X)}{g(X)}\\right] \\] <p>(We do have to be mindful of division by 0.) Then</p> \\[ \\hat{I}_{N}=\\frac{1}{N} \\sum_{i=1}^{N} \\phi\\left(X_{i}\\right) \\frac{f\\left(X_{i}\\right)}{g\\left(X_{i}\\right)} \\] <p>with \\(X_{1}, \\ldots, X_{N} \\sim g\\) is also an estimator of \\(I\\). Indeed, \\(\\mathbb{E}\\left[\\hat{I}_{N}\\right]=I\\) and \\(\\hat{I}_{N} \\rightarrow I\\). The weight \\(\\frac{f(x)}{g(x)}\\) is called the likelihood ratio or the Radon-Nikodym derivative.</p> <p>So we can use samples from \\(g\\) to compute expectation with respect to \\(f\\).</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#is-example-low-probability-events","title":"IS example: Low probability events","text":"<p>Consider the setup of estimating the probability</p> \\[ \\mathbb{P}(X&gt;3)=0.00135 \\] <p>where \\(X \\sim \\mathcal{N}(0,1)\\). If we use the regular Monte Carlo estimator</p> \\[ \\hat{I}_{N}=\\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}_{\\left\\{X_{i}&gt;3\\right\\}} \\] <p>where \\(X_{i} \\sim \\mathcal{N}(0,1)\\), if \\(N\\) is not sufficiently large, we can have \\(\\hat{I}_{N}=0\\). Inaccurate estimate.</p> <p>If we use the IS estimator</p> \\[ \\hat{I}_{N}=\\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}_{\\left\\{Y_{i}&gt;3\\right\\}} \\exp \\left(\\frac{\\left(Y_{i}-3\\right)^{2}-Y_{i}^{2}}{2}\\right) \\] <p>where \\(Y_{i} \\sim \\mathcal{N}(3,1)\\), having \\(\\hat{I}_{N}=0\\) is much less likely. Estimate is much more accurate.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#importance-sampling_1","title":"Importance sampling","text":"<p>Benefit of IS quantified by with variance:</p> \\[ \\operatorname{Var}_{X \\sim g}\\left(\\hat{I}_{N}\\right)=\\sum_{i=1}^{N} \\operatorname{Var}_{X \\sim g}\\left(\\frac{\\phi\\left(X_{i}\\right) f\\left(X_{i}\\right)}{n g\\left(X_{i}\\right)}\\right)=\\frac{1}{N} \\operatorname{Var}_{X \\sim g}\\left(\\frac{\\phi(X) f(X)}{g(X)}\\right) \\] <p>If \\(\\operatorname{Var}_{X \\sim g}\\left(\\frac{\\phi(X) f(X)}{g(X)}\\right)&lt;\\operatorname{Var}_{X \\sim f}(\\phi(X))\\), then IS provides variance reduction.</p> <p>We call \\(g\\) the importance or sampling distribution. Choosing \\(g\\) poorly can increase the variance. What is the best choice of \\(g\\) ?</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#optimal-sampling-distribution","title":"Optimal sampling distribution","text":"<p>The sampling distribution</p> \\[ g(x)=\\frac{\\phi(x) f(x)}{I} \\] <p>makes \\(\\operatorname{Var}_{X \\sim g}\\left(\\frac{\\phi(X) f(X)}{g(X)}\\right)=\\operatorname{Var}_{X \\sim g}(I)=0\\) and therefore is optimal. (I serves as the normalizing factor that ensures the density \\(g\\) integrates to 1.)</p> <p>Problem: Since we do not know the normalizing factor \\(I\\), the answer we wish to estimate, sampling from \\(g\\) is usually difficult.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#optimizedtrained-sampling-distribution","title":"Optimized/trained sampling distribution","text":"<p>Instead, we consider the optimization problem</p> \\[ \\underset{g \\in \\mathcal{G}}{\\operatorname{minimize}} \\quad D_{\\mathrm{KL}}\\left(g \\| \\frac{\\phi f}{I}\\right) \\] <p>and compute a suboptimal, but good, sampling distribution within a class of sampling distributions \\(\\mathcal{G}\\). (In ML, \\(\\mathcal{G}=\\left\\{g_{\\theta} \\mid \\theta \\in \\Theta\\right\\}\\) is parameterized by neural networks.)</p> <p>Importantly, this optimization problem does not require knowledge of \\(I\\).</p> \\[ \\begin{aligned} D_{\\mathrm{KL}}\\left(g_{\\theta} \\| \\phi f / I\\right) &amp; =\\mathbb{E}_{X \\sim g_{\\theta}}\\left[\\log \\left(\\frac{I g_{\\theta}(X)}{\\phi(X) f(X)}\\right)\\right] \\\\ &amp; =\\mathbb{E}_{X \\sim g_{\\theta}}\\left[\\log \\left(\\frac{g_{\\theta}(X)}{\\phi(X) f(X)}\\right)\\right]+\\log I \\\\ &amp; =\\mathbb{E}_{X \\sim g_{\\theta}}\\left[\\log \\left(\\frac{g_{\\theta}(X)}{\\phi(X) f(X)}\\right)\\right]+\\text { constant independent of } \\theta \\end{aligned} \\] <p>How do we compute stochastic gradients?</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#log-derivative-trick","title":"Log-derivative trick","text":"<p>Generally, consider the setup where we wish to solve</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\mathbb{E}_{X \\sim f_{\\theta}}[\\phi(X)] \\] <p>with SGD. (Previous slide had \\(\\theta\\)-dependence both on and inside the expectation. For now, let's simplify the problem so that \\(\\phi\\) does not depend on \\(\\theta\\).)</p> <p>Incorrect gradient computation:</p> \\[ \\nabla_{\\theta} \\mathbb{E}_{X \\sim f_{\\theta}}[\\phi(X)] \\stackrel{?}{=} \\mathbb{E}_{X \\sim f_{\\theta}}\\left[\\nabla_{\\theta} \\phi(X)\\right]=\\mathbb{E}_{X \\sim f_{\\theta}}[0]=0 \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#log-derivative-trick_1","title":"Log-derivative trick","text":"<p>Correct gradient computation:</p> \\[ \\begin{aligned} \\nabla_{\\theta} \\mathbb{E}_{X \\sim f_{\\theta}}[\\phi(X)] &amp; =\\nabla_{\\theta} \\int \\phi(x) f_{\\theta}(x) d x=\\int \\phi(x) \\nabla_{\\theta} f_{\\theta}(x) d x \\\\ &amp; =\\int \\phi(x) \\frac{\\nabla_{\\theta} f_{\\theta}(x)}{f_{\\theta}(x)} f_{\\theta}(x) d x=\\mathbb{E}_{X \\sim f_{\\theta}}\\left[\\phi(X) \\frac{\\nabla_{\\theta} f_{\\theta}(X)}{f_{\\theta}(X)}\\right] \\\\ &amp; =\\mathbb{E}_{X \\sim f_{\\theta}}\\left[\\phi(X) \\nabla_{\\theta} \\log \\left(f_{\\theta}(X)\\right)\\right] \\end{aligned} \\] <p>Therefore, \\(\\phi(X) \\nabla_{\\theta} \\log \\left(f_{\\theta}(X)\\right)\\) with \\(X \\sim f_{\\theta}\\) is a stochastic gradient of the loss function. This technique is called the log-derivative trick, the likelihood ratio gradient#, or REINFORCE*.</p> <p>Formula with the log-derivative \\(\\left(\\nabla_{\\theta} \\log (\\cdot)\\right)\\) is convenient when dealing with Gaussians, or more generally exponential families, since the densities are of the form</p> \\[ f_{\\theta}(x)=h(x) \\exp (\\text { function of } \\theta) \\]"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#log-derivative-trick-example","title":"Log-derivative trick example","text":"<p>Learn \\(\\mu \\in \\mathbb{R}^{2}\\) to minimize the objective below.</p> \\[ \\underset{\\mu \\in \\mathbb{R}^{2}}{\\operatorname{minimize}} \\mathbb{E}_{X \\sim \\mathcal{N}(\\mu, I)}\\left\\|X-\\binom{5}{5}\\right\\|^{2} \\] <p></p> <p>Then the loss function is</p> \\[ \\mathcal{L}(\\mu)=\\mathbb{E}_{X \\sim \\mathcal{N}(\\mu, I)}\\left\\|X-\\binom{5}{5}\\right\\|^{2}=\\int\\left\\|x-\\binom{5}{5}\\right\\|^{2} \\frac{1}{2 \\pi} \\exp \\left(-\\frac{1}{2}\\|x-\\mu\\|^{2}\\right) d x \\] <p>And, using \\(X_{1}, \\ldots, X_{B} \\sim \\mathcal{N}(\\mu, I)\\), we have stochastic gradients</p> \\[ \\nabla_{\\mu} \\mathcal{L}(\\mu)=\\mathbb{E}_{X \\sim q_{\\mu}}\\left[\\left\\|x-\\binom{5}{5}\\right\\|^{2} \\nabla_{\\mu}\\left(-\\frac{1}{2}\\|x-\\mu\\|^{2}\\right)\\right] \\approx \\frac{1}{B} \\sum_{i=1}^{B}\\left\\|X_{i}-\\binom{5}{5}\\right\\|^{2}\\left(X_{i}-\\mu\\right) \\] <p>These stochastic gradients have large variance and thus SGD is slow.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#log-derivative-trick-example_1","title":"Log-derivative trick example","text":""},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#reparameterization-trick","title":"Reparameterization trick","text":"<p>The reparameterization trick (RT) or the pathwise derivative (PD) relies on the key insight.</p> \\[ \\mathbb{E}_{X \\sim \\mathcal{N}\\left(\\mu, \\sigma^{2}\\right)}[\\phi(X)]=\\mathbb{E}_{Y \\sim \\mathcal{N}(0,1)}[\\phi(\\mu+\\sigma Y)] \\] <p>Gradient computation:</p> \\[ \\begin{aligned} \\nabla_{\\mu, \\sigma} \\mathbb{E}_{X \\sim \\mathcal{N}\\left(\\mu, \\sigma^{2}\\right)}[\\phi(X)] &amp; =\\mathbb{E}_{Y \\sim \\mathcal{N}(0,1)}\\left[\\nabla_{\\mu, \\sigma} \\phi(\\mu+\\sigma Y)\\right]=\\mathbb{E}_{Y \\sim \\mathcal{N}(0,1)}\\left[\\phi^{\\prime}(\\mu+\\sigma Y)\\left[\\begin{array}{c} 1 \\\\ Y \\end{array}\\right]\\right] \\\\ &amp; \\approx \\frac{1}{B} \\sum_{i=1}^{B} \\phi^{\\prime}\\left(\\mu+\\sigma Y_{i}\\right)\\left[\\begin{array}{c} 1 \\\\ Y_{i} \\end{array}\\right], \\quad Y_{1}, \\ldots, Y_{B} \\sim \\mathcal{N}(0, I) \\end{aligned} \\] <p>RT is less general than log-derivative trick, but it usually produces stochastic gradients with lower variance.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#reparameterization-trick-example","title":"Reparameterization trick example","text":"<p>Consider the same example as before</p> \\[ \\mathcal{L}(\\mu)=\\mathbb{E}_{X \\sim \\mathcal{N}(\\mu, I)}\\left\\|X-\\binom{5}{5}\\right\\|^{2}=\\mathbb{E}_{Y \\sim \\mathcal{N}(0, I)}\\left\\|Y+\\mu-\\binom{5}{5}\\right\\|^{2} \\] <p>Gradient computation:</p> \\[ \\begin{aligned} \\nabla_{\\mu} \\mathcal{L}(\\mu) &amp; =\\mathbb{E}_{Y \\sim \\mathcal{N}(0, I)} \\nabla_{\\mu}\\left\\|Y+\\mu-\\binom{5}{5}\\right\\|^{2}=2 \\mathbb{E}_{Y \\sim \\mathcal{N}(0, I)}\\left(Y+\\mu-\\binom{5}{5}\\right) \\\\ &amp; \\approx \\frac{2}{B} \\sum_{i=1}^{B}\\left(Y_{i}+\\mu-\\binom{5}{5}\\right), \\quad Y_{1}, \\ldots, Y_{B} \\sim \\mathcal{N}(0, I) \\end{aligned} \\] <p>These stochastic gradients have smaller variance and thus SGD is faster.</p>"},{"location":"books-and-courses/mfdnn/mfdnn_lecture/#reparameterization-trick-example_1","title":"Reparameterization trick example","text":"<pre><code>Dropout (0.5)\nLocal response normalization (preserves spatial dimension\\&amp;channel \\#s) (outdated technique)\nMax pool $f=3, s=2$ (overlapping max pool)\nFully connected layer+ReLU\n</code></pre> <p>[^1]:    *R. M. Schmidt, F. Schneider, and P. Hennig, Descending through a crowded valley \u2014 benchmarking deep learning optimizers, ICML, 2021.     \\({ }^{\\dagger}\\) M. Tan and Q. V. Le, EfficientNet: Rethinking model scaling for convolutional neural networks, ICML, 2019.     #A. C. Wilson, R. Roelofs, M. Stern, N. Srebro, and B. Recht, The marginal value of adaptive gradient methods in machine learning, NeurlPS, 2017.</p>"},{"location":"books-and-courses/mfdnn/format/","title":"Mathematical Foundations of Deep Neural Networks","text":""},{"location":"books-and-courses/mfdnn/format/#course-information","title":"Course Information","text":"<p>Mathematical Foundations of Deep Neural Networks, Spring 2024 (Ernest K. Ryu)</p>"},{"location":"books-and-courses/mfdnn/format/#table-of-contents","title":"Table of Contents","text":""},{"location":"books-and-courses/mfdnn/format/#ch-1-optimization-and-stochastic-gradient-descent","title":"Ch 1. Optimization and Stochastic Gradient Descent","text":"<ul> <li>1. Optimization Problem</li> <li>2. Gradient Descent</li> <li>Chapter 1 Code</li> </ul>"},{"location":"books-and-courses/mfdnn/format/#ch-2-shallow-neural-networks-to-multilayer-perceptrons","title":"Ch 2. Shallow Neural Networks to Multilayer Perceptrons","text":"<ul> <li>3. Shallow Neural Networks</li> <li>4. Deep Neural Networks</li> <li>Chapter 2 Code</li> </ul>"},{"location":"books-and-courses/mfdnn/format/#ch-3-convolutional-neural-networks","title":"Ch 3. Convolutional Neural Networks","text":"<ul> <li>5. Convolutional Neural Networks</li> <li>6. Foundations of Design and Training of Deep Neural Networks</li> <li>7. ImageNet Challenge</li> <li>Chapter 3 Code</li> </ul>"},{"location":"books-and-courses/mfdnn/format/#ch-4-cnns-for-other-supervised-learning-tasks","title":"Ch 4. CNNs for Other Supervised Learning Tasks","text":"<ul> <li>8. CNNs for Other Supervised Learning Tasks</li> <li>Chapter 4 Code</li> </ul>"},{"location":"books-and-courses/mfdnn/format/#ch-5-unsupervised-learning","title":"Ch 5. Unsupervised Learning","text":"<ul> <li>9. Autoencoder</li> <li>10. Flow Models</li> <li>11. Variational Autoencoders</li> <li>12. Generative Adversarial Networks</li> <li>Chapter 5 Code</li> </ul>"},{"location":"books-and-courses/mfdnn/format/#ch-a-appendix-basics-of-monte-carlo","title":"Ch A. Appendix - Basics of Monte Carlo","text":"<ul> <li>13. Basics of Monte Carlo</li> <li>Chapter A Code</li> </ul>"},{"location":"books-and-courses/mfdnn/format/#python-basics","title":"Python Basics","text":"<ul> <li>Python Lecture 1</li> <li>Python Lecture 2</li> <li>Python Lecture 3</li> </ul>"},{"location":"books-and-courses/mfdnn/format/#materials","title":"Materials","text":"<ul> <li>Latex PDF</li> </ul>"},{"location":"books-and-courses/mfdnn/format/1/","title":"\u00a7 1. Optimization Problem","text":""},{"location":"books-and-courses/mfdnn/format/1/#definition-of-optimization-problem","title":"Definition of Optimization Problem","text":"<p>Definition 1.1 : Optimization Problem</p> <p>In an optimization problem, we minimize or maximize a function value, possibly subject to constraints.</p> \\[ \\begin{array}{ll} \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} &amp; f(\\theta) \\\\ \\text { subject to } &amp; h_{1}(\\theta)=0, &amp; \\cdots &amp; , \\ h_{m}(\\theta)=0, \\\\ &amp; g_{1}(\\theta) \\le 0, &amp; \\cdots &amp; , \\ g_{n}(\\theta) \\le 0 \\end{array} \\] <ul> <li>Decision variable: \\(\\theta\\)</li> <li>Objective function: \\(f\\)</li> <li>Equality constraint: \\(h_{i}(\\theta)=0\\) for \\(i=1, \\ldots, m\\)</li> <li>Inequality constraint: \\(g_{j}(\\theta) \\leq 0\\) for \\(j=1, \\ldots, n\\)</li> </ul> <p>In machine learning (ML), we often minimize a \"loss\", but sometimes we maximize the \"likelihood\". In any case, minimization and maximization are equivalent since</p> \\[ \\text { maximize } f(\\theta) \\quad \\Leftrightarrow \\quad \\text { minimize }-f(\\theta). \\] <p>Definition 1.2 : Feasible Point and Constraints</p> <p>\\(\\theta \\in \\mathbb{R}^{p}\\) is a feasible point if it satisfies all constraints:</p> \\[ \\begin{array}{cc} h_{1}(\\theta)=0 &amp; g_{1}(\\theta) \\leq 0 \\\\ \\vdots &amp; \\vdots \\\\ h_{m}(\\theta)=0 &amp; g_{n}(\\theta) \\leq 0 \\end{array} \\] <p>Optimization problem is infeasible if there is no feasible point.</p> <p>An optimization problem with no constraint is called an unconstrained optimization problem. Optimization problems with constraints is called a constrained optimization problem.</p> <p>Definition 1.3 : Optimal Value and Solution</p> <p>Optimal value of an optimization problem is</p> \\[ p^{\\star}=\\inf \\left\\{f(\\theta) \\mid \\theta \\in \\mathbb{R}^{n}, \\theta \\text { feasible }\\right\\} \\] <ul> <li>\\(p^{\\star}=\\infty\\) if problem is infeasible</li> <li>\\(p^{\\star}=-\\infty\\) is possible</li> <li>In ML, it is often a priori clear that \\(0 \\leq p^{\\star}&lt;\\infty\\).</li> </ul> <p>If \\(f\\left(\\theta^{\\star}\\right)=p^{\\star}\\), we say \\(\\theta^{\\star}\\) is a solution or \\(\\theta^{\\star}\\) is optimal. A solution may or may not exist, and a solution may or may not be unique.</p>"},{"location":"books-and-courses/mfdnn/format/1/#examples-of-optimization-problem","title":"Examples of Optimization Problem","text":"<p>Example 1.4 : Curve Fitting</p> <p>Consider setup with data \\(X_{1}, \\ldots, X_{N}\\) and corresponding labels \\(Y_{1}, \\ldots, Y_{N}\\) satisfying the relationship</p> \\[ Y_{i}=f_{\\star}\\left(X_{i}\\right)+\\text { error } \\] <p>for \\(i=1, \\ldots, N\\). Hopefully, \"error\" is small. True function \\(f_{\\star}\\) is unknown.</p> <p>Goal is to find a function (curve) \\(f\\) such that \\(f \\approx f_{\\star}\\).</p> <p>Example 1.5 : Least-Squares Minimization</p> <ul> <li> <p>Problem</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\quad \\frac{1}{2}\\|X \\theta-Y\\|^{2} \\] <p>where \\(X \\in \\mathbb{R}^{N \\times p}\\) and \\(Y \\in \\mathbb{R}^{N}\\). Equivalent to</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\frac{1}{2} \\sum_{i=1}^{N}\\left(X_{i}^{\\top} \\theta-Y_{i}\\right)^{2} \\] <p>where \\(X=\\left[\\begin{array}{c}X_{1}^{\\top} \\\\ \\vdots \\\\ X_{N}^{\\top}\\end{array}\\right]\\) and \\(Y=\\left[\\begin{array}{c}Y_{1} \\\\ \\vdots \\\\ Y_{N}\\end{array}\\right]\\).</p> </li> <li> <p>Solution</p> <p>To solve</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\frac{1}{2}\\|X \\theta-Y\\|^{2} \\] <p>take gradient and set it to \\(0\\).</p> \\[ \\nabla_{\\theta} \\frac{1}{2}\\|X \\theta-Y\\|^{2}=X^{\\top}(X \\theta-Y) \\] \\[ \\begin{gathered} X^{\\top}\\left(X \\theta^{\\star}-Y\\right)=0 \\\\ \\theta^{\\star}=\\left(X^{\\top} X\\right)^{-1} X^{\\top} Y \\end{gathered} \\] <p>Here, we assume \\(X^{\\top} X\\) is invertible.</p> </li> </ul> <p>Concept 1.6 : Least squares is an instance of curve fitting.</p> <p>Define \\(f_{\\theta}(x)=x^{\\top} \\theta\\). Then LS becomes</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\frac{1}{2} \\sum_{i=1}^{N}\\left(f_{\\theta}\\left(X_{i}\\right)-Y_{i}\\right)^{2} \\] <p>and the solution hopefully satisfies</p> \\[ Y_{i}=f_{\\theta}\\left(X_{i}\\right)+\\text { small. } \\] <p>Since \\(X_{i}\\) and \\(Y_{i}\\) is assumed to satisfy</p> \\[ Y_{i}=f_{\\star}\\left(X_{i}\\right)+\\text { error } \\] <p>we are searching over linear functions (linear curves) \\(f_{\\theta}\\) that best fit (approximate) \\(f_{\\star}\\).</p>"},{"location":"books-and-courses/mfdnn/format/1/#local-and-global-minimum","title":"Local and Global Minimum","text":"<p>Definition 1.7 : Local vs Global Minima</p> <p>\\(\\theta^{\\star}\\) is a local minimum if \\(f(\\theta) \\geq f\\left(\\theta^{\\star}\\right)\\) for all feasible \\(\\theta\\) within a small neighborhood.</p> <p>\\(\\theta^{\\star}\\) is a global minimum if \\(f(\\theta) \\geq f\\left(\\theta^{\\star}\\right)\\) for all feasible \\(\\theta\\).</p> <p> </p> <p>In the worst case, finding the global minimum of an optimization problem is difficult. However, in deep learning, optimization problems are often \"solved\" without any guarantee of global optimality.</p>"},{"location":"books-and-courses/mfdnn/format/10/","title":"\u00a7 10. Flow Models","text":""},{"location":"books-and-courses/mfdnn/format/10/#probabilistic-generative-models","title":"Probabilistic Generative Models","text":"<p>Definition 10.1 : Probabilistic generative models</p> <p>A probabilistic generative model learns a distribution \\(p_{\\theta}\\) from \\(X_{1}, \\ldots, X_{N} \\sim p_{\\text {true }}\\) such that \\(p_{\\theta} \\approx p_{\\text {true }}\\) and such that we can generate new samples \\(X \\sim p_{\\theta}\\).</p> <p>The ability to generate new synthetic data is interesting, but by itself not very useful. (Generating fake images to use in fake social media accounts is the only direct application that I can think of.)</p> <p>The structure of the data learned through the unsupervised learning is of higher value. However, we won't talk about the downstream applications in this course.</p> <p>In this class, we will talk about flow models, VAEs, and GANs.</p> <p>Fit a probability density function \\(p_{\\theta}(x)\\) with continuous data \\(X_{1}, \\ldots, X_{N} \\sim p_{\\text {true }}(x)\\).</p> <ul> <li>We want to fit the data \\(X_{1}, \\ldots, X_{N}\\) (or really the underlying distribution \\(p_{\\text {true}}\\)) well.</li> <li>We want to be able to sample from \\(p_{\\theta}\\).</li> <li>(We want to get a good latent representation.)</li> </ul> <p>We first develop the mathematical discussion with 1D flows, and then generalize the discussion to high dimensions.</p> <p>Concept 10.2 : Example Density Model</p> \\[ p_{\\theta}(x)=\\sum_{i=1}^{k} \\pi_{i} \\mathcal{N}\\left(x ; \\mu_{i}, \\sigma_{i}^{2}\\right) \\] <p>Parameters: means and variances of components, mixture weights</p> \\[ \\theta=\\left(\\pi_{1}, \\ldots, \\pi_{k}, \\mu_{1}, \\ldots, \\mu_{k}, \\sigma_{1}, \\ldots, \\sigma_{k}\\right) \\] <p>Problems with GMM:</p> <ul> <li>Highly non-convex optimization problem. Can easily get stuck in local minima.</li> <li>It is does not have the representation power to express high-dimensional data.</li> </ul> <p> </p> <p>GMM doesn't work with high-dimensional data. The sampling process is:</p> <ol> <li>Pick a cluster center</li> <li>Add Gaussian noise</li> </ol> <p>If this is done with natural images, a realistic image can be generated only if it is a cluster center, i.e., the clusters must already be realistic images.</p> <p> </p> <p>So then how do we fit a general (complex) density model?</p>"},{"location":"books-and-courses/mfdnn/format/10/#1d-flow-models","title":"1D Flow Models","text":"<p>Concept 10.3 : Math Review</p> <p>A random variable \\(X\\) is continuous if there exists a probability density function (PDF) \\(p_{X}(x) \\geq 0\\) such that</p> \\[ \\mathbb{P}(a \\leq X \\leq b)=\\int_{a}^{b} p_{X}(x) d x \\] <p>In this case, we write \\(X \\sim p_{X}\\).</p> <p> </p> <p>The cumulative distribution function (CDF) of \\(X\\) is defined as</p> \\[ F_{X}(t)=\\mathbb{P}(X \\leq t)=\\int_{-\\infty}^{t} p_{X}(x) d x \\] <ul> <li>\\(F_{X}(t)\\) is a nondecreasing function.  </li> <li>\\(F_{X}(t)\\) is a continuous function if \\(X\\) is a continuous random variable.</li> </ul> <p> </p> <p>Concept 10.4 : Na\u00efve Approach</p> <p>Na\u00efve approach for fitting a density model. Represent \\(p_{\\theta}(x)\\) with DNN.</p> <p> </p> <p>There are some challenges:</p> <ol> <li> <p>How to ensure proper distribution?</p> \\[ \\int_{-\\infty}^{+\\infty} p_{\\theta}(x) d x=1, \\quad p_{\\theta}(x) \\geq 0, \\quad x \\in \\mathbb{R} \\] </li> <li> <p>How to sample?</p> </li> </ol> <p>Normalization of \\(p_{\\theta}\\)</p> <p>For discrete random variables, one can use the soft-max function \\(\\mu: \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{k}\\) defined as</p> \\[ \\mu_{i}(z)_{i}=\\frac{e^{z_{i}}}{\\sum_{j=1}^{k} e^{z_{j}}} \\] <p>to normalize probabilities.</p> <p>For continuous random variables, we can ensure \\(p_{\\theta} \\geq 0\\) with \\(p_{\\theta}(x)=e^{f_{\\theta}(x)}\\), where \\(f_{\\theta}\\) is the output of the neural network. However, ensuring the normalization</p> \\[ \\int_{-\\infty}^{+\\infty} p_{\\theta}(x) d x=1 \\] <p>is not a simple matter. (Any Bayesian statistician can tell you how difficult this is.)</p> <p>What happens if we ignore normalization?</p> <p>Do we really need this normalization thing? Yes, we do.</p> <p>Without normalization, one can just assign arbitrarily large probabilities everywhere when we perform maximum likelihood estimation:</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right) \\] <p>The solution is to set \\(p_{\\theta}(x)=M\\) with \\(M \\rightarrow \\infty\\).</p> <p>We want model to place large probability on data \\(X_{1}, \\ldots, X_{N}\\) while placing small probability elsewhere. Normalization forces model to place small probability where data doesn't reside.</p> <p>Definition 10.5 : Flow Model</p> <p>Key insight of normalizing flow: DNN outputs random variable \\(Z\\), rather than \\(p_{\\theta}(X)\\). We choose \\( Z \\) from a known distribution that is easy to sample from, such as \\( \\mathcal{N}(0,1) \\) or \\( \\operatorname{Uniform}([0,1]) \\).</p> <p> </p> <p>In normalizing flow, find \\(\\theta\\) such that the flow \\(f_{\\theta}\\) normalizes the random variable \\(X \\sim p_{X}\\) into \\(Z \\sim \\mathcal{N}(0,1)\\). Generally, we can consider \\(Z \\sim pZ\\). The choice of \\(pZ\\), however, does not seem to make a significant difference.</p> <p>Important questions to resolve:</p> <ol> <li>How to train? (How to evaluate \\(p_{\\theta}(x)\\) ? DNN outputs \\(f_{\\theta}\\), not \\(p_{\\theta}\\).) (Concept 10.7)</li> <li>How to sample \\(X\\) ? (Concept 10.8)</li> </ol> <p>Concept 10.6 : Math Review</p> <p>Assume \\(f\\) is invertible, \\(f\\) is differentiable, and \\(f^{-1}\\) is differentiable.</p> <p>If \\(X \\sim p_{X}\\), then \\(Z=f(X)\\) has pdf</p> \\[ p_{Z}(z)=p_{X}\\left(f^{-1}(z)\\right)\\left|\\frac{d x}{d z}\\right| \\] <p>If \\(Z \\sim p_{Z}\\), then \\(X=f^{-1}(Z)\\) has pdf</p> \\[ p_{X}(x)=p_{Z}(f(x))\\left|\\frac{d f(x)}{d x}\\right| \\] <p>Since \\(Z=f(X)\\), one might think \\(p_{X}(x)=p_{Z}(z)=p_{Z}(f(x))\\). \\(\\leftarrow\\) This is wrong.</p> <p>Invertibility of \\(f\\) is essential; it is not a minor technical issue.</p> <p>Definition 10.7 : Traning Flow Models</p> <p>Train model with MLE</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{Z}\\left(f_{\\theta}\\left(X_{i}\\right)\\right)+\\log \\left|\\frac{\\partial f_{\\theta}}{\\partial x}\\left(X_{i}\\right)\\right| \\] <p>where \\(f_{\\theta}\\) is invertible and differentiable, and \\(X=f_{\\theta}^{-1}(Z)\\) with \\(Z \\sim p_{Z}\\) so</p> \\[ p_{X}(x)=p_{Z}\\left(f_{\\theta}(x)\\right)\\left|\\frac{\\partial f_{\\theta}}{\\partial x}(x)\\right| \\] <p>Can optimize with SGD, if we know how to perform backprop on \\(\\left|\\frac{\\partial f_{\\theta}}{\\partial x}\\left(X_{i}\\right)\\right|\\). More on this later.</p> <p>Definition 10.8 : Sampling from Flow Models</p> <p> </p> <ol> <li>Sample \\(Z \\sim p_{Z}\\)</li> <li>Compute \\(X=f_{\\theta}^{-1}(Z)\\)</li> </ol> <p>Concept 10.9 : Requirements of Flow \\(f_{\\theta}\\)</p> <p>Theoretical requirement:</p> <ul> <li>\\(f_{\\theta}(x)\\) invertible and differentiable.</li> </ul> <p>Computational requirements:</p> <ul> <li>\\(f_{\\theta}(x)\\) and \\(\\nabla_{\\theta} f_{\\theta}(x)\\) efficient to evaluate (for training)</li> <li>\\(\\left|\\frac{\\partial f_{\\theta}}{\\partial x}(x)\\right|\\) and \\(\\nabla_{\\theta}\\left|\\frac{\\partial f_{\\theta}}{\\partial x}(x)\\right|\\) efficient to evaluate (for training)</li> <li>\\(f_{\\theta}^{-1}\\) efficient to evaluate (for sampling)</li> </ul> <p>Example 10.10 : Example of Trained Flow Models</p> <ul> <li>Flow to \\(Z\\) ~ \\(\\operatorname{Uniform}([0,1])\\)</li> </ul> <p> </p> <ul> <li>Flow to \\(Z \\sim \\operatorname{Beta}(5,5)\\)</li> </ul> <p> </p> <ul> <li>Flow to \\(Z \\sim \\mathcal{N}(0,1)\\)</li> </ul> <p> </p> <p>Concept 10.11 : Universality of Flows</p> <p>Are flows universal, i.e., can \\(f_{\\theta}^{-1}(Z) \\sim p_{X}\\) for any \\(X\\) provided that \\(f_{\\theta}\\) can represent any invertible function?</p> <p>Yes, 1D flows are universal due to the inverse CDF sampling technique. (Some basic conditions are being omitted.)</p> <p>Higher dimensional flows are also universal as shown by Huang et al.\\(^{\\star}\\) or earlier by the general theory of optimal transport. (link)</p> <p>\\(^{\\star}\\) C.-W. Huang, D. Krueger, A. Lacoste, and A. Courville, Neural Autoregressive Flows, ICML, 2018.</p> <p>Concept 10.12 : Math Review</p> <p>Inverse CDF sampling is a technique for sampling \\(X \\sim p_{X}\\). If \\(F_{X}(t)\\) is furthermore a strictly increasing function, then \\(F_{X}\\) is invertible, i.e., \\(F_{X}^{-1}\\) exists.</p> <p>Generate a random number \\(U \\sim \\operatorname{Uniform}([0,1])\\) and compute \\(F_{X}^{-1}(U)\\). Then</p> \\[ F_{X}^{-1}(U) \\sim p_{X} \\] <p>since</p> \\[ \\mathbb{P}\\left(F_{X}^{-1}(U) \\leq t\\right)=\\mathbb{P}\\left(U \\leq F_{X}(t)\\right)=F_{X}(t) \\] <p>Technique can be generalized to when \\(F_{X}\\) is not invertible.</p> <p>Inverse CDF can be seen as flow model from \\(X \\sim p_X\\) to \\(U \\sim \\operatorname{Uniform}([0,1])\\). As seen above, the flow is \\(F_X\\), so when we do not know the true distribution \\(p_X\\), we can train the flow model to learn \\(F_{X}^{-1}(U) \\sim p_{X}\\).</p> <p>Concept 10.13 : Universality of 1D Flows</p> <p>Composition of flows is a flow, and inverse of a flow is a flow.</p> <p>Universality of 1D flows:</p> <ul> <li>Use inverse CDF as flow to transform \\(X \\sim p_{X}\\) into \\(U \\sim \\operatorname{Uniform}([0,1])\\) and \\(Z \\sim \\mathcal{N}(0,1)\\) into \\(U \\sim \\operatorname{Uniform}([0,1])\\).</li> <li>Compose flow \\(X \\rightarrow U\\) and inverse flow \\(U \\rightarrow Z\\).</li> </ul> <p> </p>"},{"location":"books-and-courses/mfdnn/format/10/#high-dimensional-flow-models","title":"High Dimensional Flow Models","text":"<p>Concept 10.14 : Math Review</p> <p>Let \\(f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}\\), such that</p> \\[ f(x)=\\left[\\begin{array}{c} f_{1}(x) \\\\ f_{2}(x) \\\\ \\vdots \\\\ f_{n}(x) \\end{array}\\right] \\] <p>The Jacobian matrix is</p> \\[ \\frac{\\partial f}{\\partial x}(x)=\\left[\\begin{array}{cccc} \\frac{\\partial f_{1}}{\\partial x_{1}}(x) &amp; \\frac{\\partial f_{1}}{\\partial x_{2}}(x) &amp; \\cdots &amp; \\frac{\\partial f_{1}}{\\partial x_{n}}(x) \\\\ \\frac{\\partial f_{2}}{\\partial x_{1}}(x) &amp; \\frac{\\partial f_{2}}{\\partial x_{2}}(x) &amp; \\cdots &amp; \\frac{\\partial f_{2}}{\\partial x_{n}}(x) \\\\ \\vdots &amp; &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial f_{n}}{\\partial x_{1}}(x) &amp; \\frac{\\partial f_{n}}{\\partial x_{2}}(x) &amp; \\cdots &amp; \\frac{\\partial f_{n}}{\\partial x_{n}}(x) \\end{array}\\right]=\\left[\\begin{array}{c} \\left(\\nabla f_{1}(x)\\right)^{\\top} \\\\ \\left(\\nabla f_{2}(x)\\right)^{\\top} \\\\ \\vdots \\\\ \\left(\\nabla f_{n}(x)\\right)^{\\top} \\end{array}\\right] \\] <p>The Jacobian determinant is \\(\\operatorname{det}\\left(\\frac{\\partial f}{\\partial x}\\right)\\). We use the notation</p> \\[ \\left|\\frac{\\partial f}{\\partial x}(x)\\right|=\\left|\\operatorname{det}\\left(\\frac{\\partial f}{\\partial x}(x)\\right)\\right| \\] <p>where the second \\(|\\cdot|\\) is the absolute value of the determinant. (This notation is not completely standard.)</p> <p>Concept 10.15 : Math Review</p> <p>Let \\(f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}\\) be an invertible function such that both \\(f\\) and \\(f^{-1}\\) are differentiable. Let \\(U \\subseteq \\mathbb{R}^{n}\\). Then</p> \\[ \\int_{f(U)} h(v) d v=\\int_{U} h(f(u))\\left|\\frac{\\partial f}{\\partial u}(u)\\right| d u \\] <p>for any \\(h: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}\\). (Change of variable from \\(v=f(u)\\) to \\(u=f^{-1}(v)\\).)</p> <p>(The conditions for this change of variable formula can be further generalized.)</p> <p>Concept 10.16 : Math Review</p> <p>A multivariate random variable \\(X \\in \\mathbb{R}^{n}\\) is continuous if there exists a probability density function \\(p_{X}(x)\\) such that</p> \\[ \\mathbb{P}(X \\in A)=\\int_{A} p_{X}(x) d x \\] <p>where the integral is over the volume \\(A \\subseteq \\mathbb{R}^{n}\\). In this case, we write \\(X \\sim p_{X}\\).</p> <p>The joint cumulative distribution function (the copula) does not seem to be useful in the context of high-dimensional flow models.</p> <p>Concept 10.17 : Math Review</p> <p>Let \\(f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}\\) be an invertible function such that both \\(f\\) and \\(f^{-1}\\) are differentiable. Let \\(X\\) be a continuous random variable with probability density function \\(p_{X}\\) and let \\(Y=f(X)\\) have density \\(p_{Y}\\). Then</p> \\[ p_{X}(x)=p_{Y}(f(x))\\left|\\frac{\\partial f}{\\partial x}(x)\\right| \\] <p>Proof</p> \\[ \\mathbb{P}\\left(f^{-1}(Y) \\in A\\right)=\\mathbb{P}(Y \\in f(A))=\\int_{f(A)} p_{Y}(y) d y=\\int_{A} p_{Y}(f(x))\\left|\\frac{\\partial f}{\\partial x}(x)\\right| d x=\\mathbb{P}(X \\in A) \\] <p>Invertibility of \\(f\\) is essential; it is not a minor technical issue.</p> <p>Concept 10.18 : Math Review</p> <p>Fact: Determinant definitions in undergraduate linear algebra textbooks require exponentially many operations to compute:</p> \\[ \\operatorname{det}(A)=\\sum_{\\sigma \\in S_{n}}\\left(\\operatorname{sgn}(\\sigma) \\prod_{i=1}^{n} a_{i, \\sigma_{i}}\\right) \\] <p>Efficient computation of determinant for general matrices and performing backprop through the computation is difficult. Therefore, high-dimensional flow model are designed to compute determinants only on simple matrices.</p> <ul> <li>Product formula: if \\(A\\) and \\(B\\) are square, then</li> </ul> \\[ \\operatorname{det}(A B)=\\operatorname{det}(A) \\operatorname{det}(B) \\] <ul> <li>Block lower triangular formula: if \\(A \\in \\mathbb{R}^{n \\times n}\\) and \\(C \\in \\mathbb{R}^{m \\times m}\\), then</li> </ul> \\[ \\operatorname{det}\\left(\\begin{array}{ll} A &amp; 0 \\\\ B &amp; C \\end{array}\\right)=\\operatorname{det}(A) \\operatorname{det}(C) \\] <ul> <li>Lower triangular formula: if \\(a_{1}, \\ldots, a_{n} \\in \\mathbb{R}\\) and \\(*\\) represents arbitrary values, then</li> </ul> \\[ \\operatorname{det}\\left(\\begin{array}{cccc} a_{1} &amp; 0 &amp; \\cdots &amp; 0 \\\\ * &amp; a_{2} &amp; &amp; \\vdots \\\\ * &amp; * &amp; \\ddots &amp; 0 \\\\ * &amp; * &amp; * &amp; a_{n} \\end{array}\\right)=\\prod_{i=1}^{n} a_{i} \\] <ul> <li>Upper triangular formula: same as for lower triangular matrices.</li> </ul> <p>Definition 10.19 : Training High Dimensional Flow Models</p> <p>Train model with MLE</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{Z}\\left(f_{\\theta}\\left(X_{i}\\right)\\right)+\\log \\left|\\frac{\\partial f_{\\theta}}{\\partial x}\\left(X_{i}\\right)\\right| \\] <p>where \\(f_{\\theta}(z)\\) is invertible and differentiable, and \\(X=f^{-1}(Z)\\) with \\(Z \\sim p_{Z}\\) so</p> \\[ p_{X}(x)=p_{Z}\\left(f_{\\theta}(x)\\right)\\left|\\frac{\\partial f_{\\theta}}{\\partial x}(x)\\right| \\] <p>(Exactly the same formula as with 1D flow.)</p> <p>Can optimize with SGD, if we know how to perform backprop on \\(\\left|\\frac{\\partial f_{\\theta}}{\\partial x}\\left(X_{i}\\right)\\right|\\).</p>"},{"location":"books-and-courses/mfdnn/format/10/#coupling-flows","title":"Coupling Flows","text":"<p>Concept 10.20 : Composing Flows</p> <p>Flows can be composed to increase expressiveness. (Deep NN more expressive.) Consider composition of \\(k\\) flows</p> \\[ \\begin{aligned} &amp; x \\rightarrow f_{1} \\rightarrow f_{2} \\rightarrow \\cdots \\rightarrow f_{k} \\rightarrow z \\\\ &amp; z=f_{k} \\circ \\cdots \\circ f_{1}(x) \\\\ &amp; x=f_{1}^{-1} \\circ \\cdots \\circ f_{k}^{-1}(z) \\end{aligned} \\] <p>Determinant computation splits nicely due to chain rule and product formula</p> \\[ \\begin{aligned} &amp; \\operatorname{det}\\left(\\frac{\\partial z}{\\partial x}\\right)=\\operatorname{det}\\left(\\frac{\\partial f_{k}}{\\partial f_{k-1}} \\cdots \\frac{\\partial f_{1}}{\\partial f_{0}}\\right)=\\operatorname{det}\\left(\\frac{\\partial f_{k}}{\\partial f_{k-1}}\\right) \\cdots \\operatorname{det}\\left(\\frac{\\partial f_{1}}{\\partial f_{0}}\\right) \\\\ &amp; \\log p_{\\theta}(x)=\\log p_{\\theta}(z)+\\sum_{i=1}^{k} \\log \\left|\\frac{\\partial f_{i}}{\\partial f_{i-1}}\\right| \\end{aligned} \\] <p>Definition 10.21 : Affine Flows</p> <p>An affine (linear) transformation</p> \\[ f_{A, b}(x)=A^{-1}(x-b) \\] <p>is a flow if matrix \\(A\\) is invertible. Then</p> \\[ \\frac{\\partial f_{A, b}}{\\partial x}=A^{-1} \\] <p>and</p> \\[ \\left|\\frac{\\partial f_{A, b}}{\\partial x}\\right|=\\left|\\operatorname{det}\\left(A^{-1}\\right)\\right|=\\frac{1}{|\\operatorname{det}(A)|} \\] <p>Sampling: \\(X=A Z+b\\), where \\(Z \\sim \\mathcal{N}(0, I)\\).</p> <p>Problem with affine flows:</p> <ul> <li>Computing \\(|\\operatorname{det}(A)|\\) is expensive and performing backprop over it is difficult. We want \\(\\frac{\\partial f_{A, b}}{\\partial x}\\) to be further structured so that determinant is easy to compute.</li> <li>One affine flow is insufficient to generate complex data. However, composing multiple affine flows yields an affine flow and therefore is pointless. We need to introduce nonlinearities.</li> </ul> <p>Definition 10.22 : Coupling Flows</p> <p>A coupling flow is a general and practical approach for constructing non-linear flows.</p> <p>Partition input into two disjoint subsets \\(x=\\left(x^{A}, x^{B}\\right)\\). Then</p> \\[ f(x)=\\left(x^{A}, \\hat{f}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right)\\right) \\] <p>where \\(\\psi_{\\theta}\\) is a neural network and \\(\\hat{f}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right)\\) is another flow whose parameters depend on \\(x^{A}\\).</p> <p>Definition 10.23 : Evaluation of Coupling Flows</p> <ul> <li>Forward Evaluation</li> </ul> <p> </p> <ul> <li>Inverse Evaluation</li> </ul> <p> </p> <p>Concept 10.24 : Jacobian of Coupling Flows</p> <p>The Jacobian of a coupling flow has a nice block structure</p> \\[ \\frac{\\partial f_{\\theta}}{\\partial x}(x)=\\left[\\begin{array}{cc} I &amp; 0 \\\\ \\frac{\\partial \\hat{f}}{\\partial x^{A}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right) &amp; \\frac{\\partial \\hat{f}}{\\partial x^{B}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right) \\end{array}\\right] \\] <p>which leads to the simplified determinant formula</p> \\[ \\operatorname{det}\\left(\\frac{\\partial f_{\\theta}}{\\partial x}(x)\\right)=\\operatorname{det}\\left(\\frac{\\partial \\hat{f}}{\\partial x^{B}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right)\\right) \\] <p>Note \\(\\frac{\\partial \\hat{f}}{\\partial x^{A}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right)\\), which will be very complicated, does not appear in the determinant.</p> <p>Definition 10.25 : Coupling transformation \\(\\hat{f}(x \\mid \\psi)\\)</p> <ul> <li> <p>Additive transformations (NICE)</p> \\[ \\hat{f}(x \\mid \\psi)=x+t \\] <p>where \\(\\psi=t\\).</p> </li> <li> <p>Affine transformations (Real NVP)</p> \\[ \\hat{f}(x \\mid \\psi)=e^{s} \\odot x+t \\] <p>where \\(\\psi=(s, t)\\).</p> </li> </ul> <p>Other transformations studied throughout the literature.</p> <p>Definition 10.26 : NICE (Non-linear Independent Components Estimation)</p> <p>NICE uses additive coupling layers: Split variables in half: \\(x_{1: n / 2}, x_{n / 2: n}\\)</p> \\[ \\begin{aligned} &amp; z_{1: n / 2}=x_{1: n / 2} \\\\ &amp; z_{n / 2: n}=x_{n / 2: n}+t_{\\theta}\\left(x_{1: n / 2}\\right) \\end{aligned} \\] <p>Easily invertible:</p> \\[ \\begin{aligned} &amp; x_{1: n / 2}=z_{1: n / 2} \\\\ &amp; x_{n / 2: n}=z_{n / 2: n}-t_{\\theta}\\left(x_{1: n / 2}\\right) \\end{aligned} \\] <p>Jacobian determinant is easy to compute:</p> \\[ \\operatorname{det} \\frac{\\partial f_{\\theta}}{\\partial x}(x)=\\operatorname{det}\\left[\\begin{array}{cc}I &amp; 0 \\\\ \\frac{\\partial \\hat{f}}{\\partial x^{A}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right) &amp; \\frac{\\partial \\hat{f}}{\\partial x^{B}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right)\\end{array}\\right]=\\operatorname{det}\\left[\\begin{array}{cc}I &amp; 0 \\\\ \\frac{\\partial \\hat{f}}{\\partial x^{A}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right) &amp; I\\end{array}\\right]=1 \\] <p>(L. Dinh, D. Krueger, and Y. Bengio, NICE: Non-linear independent components estimation, ICLR Workshop, 2015.)</p> <p>Definition 10.27 : Real NVP (Real-valued Non-Volume Preserving)</p> <p>Real NVP uses affine coupling layers:</p> \\[ \\begin{aligned} &amp; z_{1: n / 2}=x_{1: n / 2} \\\\ &amp; z_{n / 2: n}=e^{s_{\\theta}\\left(x_{1: n / 2}\\right)} \\odot x_{n / 2: n}+t_{\\theta}\\left(x_{1: n / 2}\\right) \\end{aligned} \\] <p>Easily invertible:</p> \\[ \\begin{aligned} &amp; x_{1: n / 2}=z_{1: n / 2} \\\\ &amp; x_{n / 2: n}=\\left(z_{n / 2: n}-t_{\\theta}\\left(x_{1: n / 2}\\right)\\right) \\odot e^{-s_{\\theta}\\left(x_{1: n / 2}\\right)} \\end{aligned} \\] <p>Jacobian determinant is easy to compute:</p> \\[ \\begin{aligned} \\operatorname{det} \\frac{\\partial f_{\\theta}}{\\partial x}(x) &amp; =\\operatorname{det}\\left[\\begin{array}{cc} I &amp; 0 \\\\ \\frac{\\partial \\hat{f}}{\\partial x^{A}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right) &amp; \\frac{\\partial \\hat{f}}{\\partial x^{B}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right) \\end{array}\\right] \\\\ &amp; =\\operatorname{det}\\left[\\begin{array}{cc} I &amp; 0 \\\\ \\frac{\\partial \\hat{f}}{\\partial x^{A}}\\left(x^{B} \\mid \\psi_{\\theta}\\left(x^{A}\\right)\\right) &amp; \\operatorname{diag}\\left(e^{s_{\\theta}\\left(x_{1: n / 2}\\right)}\\right) \\end{array}\\right]=\\exp \\left(\\mathbf{1}_{n / 2}^{\\top} s_{\\theta}\\left(x_{1: n / 2}\\right)\\right) \\end{aligned} \\] <p>(L. Dinh, J. Sohl-Dickstein, and S. Bengio, Density estimation using Real NVP, ICLR, 2017.)</p> <ul> <li>Results of Real NVP</li> </ul>  ![](.././assets/10.14.png){: width=\"100%\"}  <p>Concept 10.28 : How to partition variables?</p> <p>Note that the additive and affine coupling layers of NICE and Real NVP are nonlinear mappings from \\(x_{1: n}\\) to \\(z_{1: n}\\), since \\(s_{\\theta}\\left(x_{1: n / 2}\\right)\\) and \\(t_{\\theta}\\left(x_{1: n / 2}\\right)\\) are nonlinear.</p> <p>Flow models compose multiple nonlinear flows. But if \\(x_{1: n / 2}\\) is always unchanged, then the full composition will leave it unchanged. Therefore, we change the partitioning for every coupling layer.</p> <p>Concept 10.29 : Real NVP Variable Partitioning</p> <p>Two partition strategies:</p> <ol> <li>Partition with checkerboard pattern.</li> <li>Reshape tensor and then partition channelwise.</li> </ol> <p> </p> <p>(L. Dinh, J. Sohl-Dickstein, and S. Bengio, Density estimation using Real NVP, ICLR, 2017.)</p> <p>Definition 10.30 : Real NVP Architecture</p> <p> </p> <p>Input \\(X\\) : \\(c \\times 32 \\times 32\\) image with \\(c=3\\)</p> <p>Layer 1: Input \\(X: c \\times 32 \\times 32\\)</p> <ul> <li>Checkerboard \\(\\times 3\\), channel reshape into \\(4 c \\times 16 \\times 16\\), channel \\(\\times 3\\)</li> <li>Output: Split result to get \\(X_{1}: 2 c \\times 16 \\times 16\\) and \\(Z_{1}: 2 c \\times 16 \\times 16\\) (fine-grained latents)</li> </ul> <p>Layer 2: Input \\(X_{1}: 2 c \\times 16 \\times 16\\) from layer 1</p> <ul> <li>Checkerboard \\(\\times 3\\), channel reshape into \\(8 c \\times 8 \\times 8\\), channel \\(\\times 3\\)</li> <li>Split result to get \\(X_{2}: 4 c \\times 8 \\times 8\\) and \\(Z_{2}: 4 c \\times 8 \\times 8\\) (coarser latents)</li> </ul> <p>Layer 3: Input \\(X_{2}: 4 c \\times 8 \\times 8\\) from layer 2</p> <ul> <li>Checkerboard \\(\\times 3\\), channel reshape into \\(16 c \\times 4 \\times 4\\), channel \\(\\times 3\\)</li> <li>Get \\(Z_{3}: 16 c \\times 4 \\times 4\\) (latents for highest-level details)</li> </ul> <p>Output \\(Z = (Z_1, Z_2, Z_3) \\in \\mathbb{R}^{c \\dot 32^2}\\)</p> <p>Concept 10.31 : Batch Normalization in Deep Flows</p> <p>To train deep flows, BN is helpful. However, the large model size forces the use of small batch sizes, and BN is not robust with small batch sizes. RealNVP uses a modified form of BN</p> \\[ x \\mapsto \\frac{x-\\tilde{\\mu}}{\\sqrt{\\tilde{\\sigma}^{2}+\\varepsilon}} \\] <p>(No \\(\\beta\\) and \\(\\gamma\\) parameters.) This layer has the log Jacobian determinant</p> \\[ -\\frac{1}{2} \\sum_{i} \\log \\left(\\tilde{\\sigma}_{i}^{2}+\\varepsilon\\right) \\] <p>The mean and variance parameters are updated with</p> \\[ \\begin{aligned} \\tilde{\\mu}_{k+1} &amp; =\\rho \\tilde{\\mu}_{k}+(1-\\rho) \\hat{\\mu}_{k} \\\\ \\tilde{\\sigma}_{k+1}^{2} &amp; =\\rho \\tilde{\\sigma}_{k}^{2}+(1-\\rho) \\hat{\\sigma}_{k}^{2} \\end{aligned} \\] <p>where \\(\\rho\\) is the momentum. During gradient computation, only backprop through the current batch statistics \\(\\hat{\\mu}_{k}\\) and \\(\\hat{\\sigma}_{k}^{2}\\).</p> <p>Concept 10.32 : \\(s_{\\theta}\\) and \\(t_{\\theta}\\) networks</p> <p>The \\(s_{\\theta}\\) and \\(t_{\\theta}\\) do not need to be invertible. The original RealNVP paper does not describe its construction.</p> <p>We let \\(\\left(s_{\\theta}, t_{\\theta}\\right)\\) be a deep (20-layer) convolutional neural network using residual connections and standard batch normalization.</p>"},{"location":"books-and-courses/mfdnn/format/10/#researches","title":"Researches","text":"<p>Definition 10.33 : Glow Paper</p> <p>The authors of the Glow paper also released a blog post. link</p> <p>(D. P. Kingma and P. Dhariwal, Glow: Generative flow with invertible 1x1 convolutions, NeurIPS, 2018.)</p> <p>Definition 10.34 : FFJORD</p> <p>Instead of a discrete composition of flows, what if we have a continuous-time flow?</p> \\[ \\begin{aligned} z_{0} &amp; =x \\\\ z_{t} &amp; =z_{0}+\\int_{0}^{t} h\\left(t, z_{t}\\right) d t \\\\ f(x) &amp; =z_{1} \\end{aligned} \\] <p>Inverse:</p> \\[ \\begin{aligned} z_{1} &amp; =z \\\\ z_{t} &amp; =z_{1}-\\int_{t}^{1} h\\left(t, z_{t}\\right) d t \\\\ f^{-1}(z) &amp; =z_{0} \\end{aligned} \\] <p>(R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud, Neural ordinary differential equations, NeurIPS, 2018. W. Grathwohl, R. T. Q. Chen, J. Bettencourt, I. Sutskever, and D. Duvenaud, FFJORD: Free-form continuous dynamics for scalable reversible generative models, ICLR, 2019.)</p>"},{"location":"books-and-courses/mfdnn/format/11/","title":"\u00a7 11. Variational Autoencoders","text":"<p>Prerequisites : Ch A. Appendix - Basics of Monte Carlo</p> <p>Concept 11.1 : Math Review</p> <p>Let \\(A\\) and \\(B\\) be probabilistic events. Assume \\(A\\) has nonzero probability.</p> <p>Conditional probability satisfies</p> \\[ \\mathbb{P}(B \\mid A) \\mathbb{P}(A)=\\mathbb{P}(A \\cap B) \\] <p>Bayes' theorem is an application of conditional probability:</p> \\[ \\mathbb{P}(B \\mid A)=\\frac{\\mathbb{P}(A \\mid B) \\mathbb{P}(B)}{\\mathbb{P}(A)} \\] <p>Concept 11.2 : Math Review</p> <p>Let \\(X \\in \\mathbb{R}^{m}\\) and \\(Z \\in \\mathbb{R}^{n}\\) be continuous random variables with joint density \\(p(x, z)\\).</p> <p>The marginal densities are defined by</p> \\[ p_{X}(x)=\\int_{\\mathbb{R}^{n}} p(x, z) d z, \\quad p_{Z}(z)=\\int_{\\mathbb{R}^{m}} p(x, z) d x \\] <p>The conditional density function \\(p(z \\mid x)\\) has the following properties</p> \\[ \\begin{gathered} \\mathbb{P}(Z \\in S \\mid X=x)=\\int_{S} p(z \\mid x) d z \\\\ p(z \\mid x) p_{X}(x)=p(x, z), \\quad p(z \\mid x)=\\frac{p(x \\mid z) p_{Z}(z)}{p_{X}(x)} \\end{gathered} \\] <p>Concept 11.3 : Introduction for Variational Autoencoders (VAE)</p> <p>Key idea of VAE:</p> <ul> <li>Latent variable model with conditional probability distribution represented by \\(p_{\\theta}(x \\mid z)\\).</li> <li>Efficiently estimate \\(p_{\\theta}(x)=\\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}(x \\mid Z)\\right]\\) by importance sampling with \\(Z \\sim q_{\\phi}(z \\mid x)\\).</li> </ul> <p>We can interpret \\(q_{\\phi}(z \\mid x)\\) as an encoder and \\(p_{\\theta}(x \\mid z)\\) as a decoder.</p> <p>VAEs differ from autoencoders as follows:</p> <ul> <li>Derivations (latent variable model vs. dimensionality reduction)</li> <li>VAE regularizes/controls latent distribution, while AE does not.</li> </ul>  ![](.././assets/11.1.png){: width=\"80%\"}  <p>These are synthetic (fake) images made with VAE.</p> <p>(A. Vahdat and J. Kautz, NVAE: A deep hierarchical variational autoencoder, NeurIPS, 2020.)</p>"},{"location":"books-and-courses/mfdnn/format/11/#latent-variable-model","title":"Latent Variable Model","text":"<ul> <li> <p>Assumption on data \\(X_{1}, \\ldots, X_{N}\\)</p> <p>Assumes there is an underlying latent variable \\(Z\\) representing the \"essential structure\" of the data and an observable variable \\(X\\) which generation is conditioned on \\(Z\\). Implicitly assumes the conditional randomness of \\(X \\sim p_{X \\mid Z}\\) is significantly smaller than the overall randomness \\(X \\sim p_{X}\\).</p> </li> <li> <p>Example</p> <p>\\(X\\) is a cat picture. \\(Z\\) encodes information about the body position, fur color, and facial expression of a cat. Latent variable \\(Z\\) encodes the overall content of the image, but \\(X\\) does contain details not specified in \\(Z\\).</p> </li> </ul> <p>Definition 11.4 : Latent Variable Model</p> <p>VAEs implements a latent variable model with a NN that generates \\(X\\) given \\(Z\\). More precisely, NN is a deterministic function that outputs the conditional distribution \\(p_{\\theta}(x \\mid Z)\\), and \\(X\\) is randomly generated according to this distribution. This structure may effectively learn the latent structure from data if the assumption on data is accurate.</p> <p> </p> <p>Sampling process:</p> \\[ X \\sim p_{\\theta}(x \\mid Z), \\quad Z \\sim p_{Z}(z) \\] <p>Usually \\(p_{Z}\\) is a Gaussian (fixed) and \\(p_{\\theta}(x \\mid z)\\) is a NN parameterized by \\(\\theta\\).</p> <p>Evaluating density (likelihood):</p> \\[ p_{\\theta}\\left(X_{i}\\right)=\\int_{z} p_{Z}(z) p_{\\theta}\\left(X_{i} \\mid z\\right) d z=\\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\] <p>Training via MLE:</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\] <p>When \\(p_{Z}\\) is a discrete:</p> \\[ p_{\\theta}(x)=\\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}(x \\mid Z)\\right]=\\sum_{z} p_{Z}(z) p_{\\theta}(x \\mid Z) \\] <p>When \\(p_{Z}\\) is a continuous:</p> \\[ p_{\\theta}(x)=\\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}(x \\mid Z)\\right]=\\int_{z} p_{Z}(z) p_{\\theta}(x \\mid z) d z \\] <p>To clarify, specification of \\(p_{Z}(z)\\) and \\(p_{\\theta}(x \\mid z)\\) fully determines \\(p_{\\theta}(x)\\) (as above) and</p> \\[ p_{\\theta}(z \\mid x)=\\frac{p_{\\theta}(x \\mid z) p_{Z}(z)}{p_{\\theta}(x)} \\] <p>Training</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\] <p>requires evaluation \\(\\mathbb{E}_{Z}\\).</p> <ul> <li>Scenario 1: If \\(Z\\) is discrete and takes a few of values, then compute \\(\\sum_{z}\\) exactly.</li> <li>Scenario 2: If \\(Z\\) takes many values or if it is a continuous, then \\(\\sum_{z}\\) or \\(\\mathbb{E}_{Z}\\) is impractical to compute. In this case, approximate expectation with Monte Carlo and importance sampling.</li> </ul> <p>Example 11.5 : Example Latent Variable Model</p> <p>Mixture of 3 Gaussians in \\(\\mathbb{R}^{2}\\), uniform prior over components. (We can make the mixture weights a trainable parameter.)</p> \\[ \\begin{gathered} p_{Z}(Z=A)=p_{Z}(Z=B)=p_{Z}(Z=C)=\\frac{1}{3} \\\\ p_{\\theta}(x \\mid Z=k)=\\frac{1}{2 \\pi\\left|\\Sigma_{k}\\right|^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2}\\left(x-\\mu_{k}\\right)^{\\top} \\Sigma_{k}^{-1}\\left(x-\\mu_{k}\\right)\\right) \\end{gathered} \\] <p>Training objective:</p> \\[ \\begin{aligned} \\underset{\\mu, \\Sigma}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\mu, \\Sigma}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log &amp; \\left[ \\frac{1}{3} \\frac{1}{2 \\pi\\left|\\Sigma_{A}\\right|^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2}\\left(X_{i}-\\mu_{A}\\right)^{\\top} \\Sigma_{A}^{-1}\\left(X_{i}-\\mu_{A}\\right)\\right) \\right.\\\\ &amp; +\\frac{1}{3} \\frac{1}{2 \\pi\\left|\\Sigma_{B}\\right|^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2}\\left(X_{i}-\\mu_{B}\\right)^{\\top} \\Sigma_{B}^{-1}\\left(X_{i}-\\mu_{B}\\right)\\right) \\\\ &amp; \\left.+\\frac{1}{3} \\frac{1}{2 \\pi\\left|\\Sigma_{C}\\right|^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2}\\left(X_{i}-\\mu_{C}\\right)^{\\top} \\Sigma_{C}^{-1}\\left(X_{i}-\\mu_{C}\\right)\\right)\\right] \\end{aligned} \\] <p> </p>"},{"location":"books-and-courses/mfdnn/format/11/#training-latent-variable-model-with-importance-sampling","title":"Training Latent Variable Model with Importance Sampling","text":"<p>From now on, we will focus on HOW to train latent variable model with MLE,</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\] <p>Concept 11.6 : VAE Outline</p> <p>Outline of variational autoencoder (VAE):</p> <ol> <li> <p>(Choice 1) Approximate intractable objective with a single \\(Z\\) sample</p> \\[ \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\approx \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i} \\mid Z_{i}\\right), \\quad Z_{i} \\sim p_{Z} \\] </li> <li> <p>(Choice 2) Improve accuracy of approximation by sampling \\(Z_{i}\\) with importance sampling</p> \\[ \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\approx \\sum_{i=1}^{N} \\log \\frac{p_{\\theta}\\left(X_{i} \\mid Z_{i}\\right) p_{Z}\\left(Z_{i}\\right)}{q_{i}\\left(Z_{i}\\right)}, \\quad Z_{i} \\sim q_{i} \\] </li> <li> <p>Optimize approximate objective with SGD.</p> </li> </ol> <p>(D. Kingma and M. Welling, VAE: Auto-encoding variational Bayes, ICLR, 2014.)</p> <p>Concept 11.7 : IWAE Outline</p> <p>Importance weighted autoencoders (IWAE) approximates intractable with \\(K\\) samples of \\(Z\\) :</p> \\[ \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\approx \\sum_{i=1}^{N} \\log \\frac{1}{K} \\sum_{k=1}^{K} \\frac{p_{\\theta}\\left(X_{i} \\mid Z_{i, k}\\right) p_{Z}\\left(Z_{i, k}\\right)}{q_{i}\\left(Z_{i, k}\\right)}, \\quad Z_{i, 1}, \\ldots, Z_{i, K} \\sim q_{i} \\] <p>(Y. Burda, R. Grosse, and R. Salakhutdinov, Importance weighted autoencoders, ICLR, 2016.)</p> <p>Concept 11.8 : Why does VAE need IS?</p> <p>Among the two choices given in Concept 11.6, VAEs improve the accuracy of latent variable model with IS (Choice 2).</p> <p>Sampling \\(Z_{i} \\sim p_{Z}\\) (Choice 1) results in a high-variance estimator:</p> \\[ \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\approx p_{\\theta}\\left(X_{i} \\mid Z_{i}\\right), \\] <p>In the Gaussian mixture example (Example 11.5), only \\(1 / 3\\) of the \\(Z\\) samples meaningfully contribute to the estimate. More specifically, if \\(X_{i}\\) is near \\(\\mu_{A}\\) but is far from \\(\\mu_{B}\\) and \\(\\mu_{C}\\), then \\(p_{\\theta}\\left(X_{i} \\mid Z=A\\right) \\gg 0\\) but \\(p_{\\theta}\\left(X_{i} \\mid Z=B\\right) \\approx 0\\) and \\(p_{\\theta}\\left(X_{i} \\mid Z=C\\right) \\approx 0\\).</p> <p>The issue worsens as the observable and latent variable dimension increases.</p> <p>Concept 11.9 : Na\u00efve Approach</p> <p>To improve estimation of \\(\\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]\\), consider importance sampling (IS) with sampling distribution \\(Z_{i} \\sim q_{i}(z)\\) :</p> \\[ \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\approx p_{\\theta}\\left(X_{i} \\mid Z_{i}\\right) \\frac{p_{Z}\\left(Z_{i}\\right)}{q_{i}\\left(Z_{i}\\right)} \\] <p>Optimal IS sampling distribution</p> \\[ q_{i}^{\\star}(z)=\\frac{p_{\\theta}\\left(X_{i} \\mid z\\right) p_{Z}(z)}{\\textcolor{red}{p_{\\theta}\\left(X_{i}\\right)}}=\\textcolor{red}{p_{\\theta}\\left(z \\mid X_{i}\\right)} \\] <p>To clarify, optimal sampling distribution depends on \\(X_{i}\\). To clarify, \\(\\textcolor{red}{p_{\\theta}\\left(X_{i}\\right)}\\) is the unkown normalizing factor so \\(\\textcolor{red}{p_{\\theta}\\left(z \\mid X_{i}\\right)}\\) is also unkown. We call \\(q_{i}^{\\star}(z)=p_{\\theta}\\left(z \\mid X_{i}\\right)\\) the true posterior distribution and we will soon consider the approximation \\(q_{\\phi}(z \\mid x) \\approx p_{\\theta}(z \\mid x)\\), which we call the approximate posterior.</p> <p>For each \\(X_{i}\\), let \\(q_i(z)\\) be the optimal approximate posterior dependent on \\(X_i\\), and consider</p> \\[ \\begin{gathered} \\underset{q_{i}}{\\operatorname{minimize}} D_{\\mathrm{KL}}\\left(q_{i}(\\cdot) \\| \\textcolor{red}{p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)} \\right)\\\\ =\\underset{q_{i}}{\\operatorname{minimize}} \\mathbb{E}_{Z \\sim q_{i}} \\log \\left(\\frac{q_{i}(Z)}{\\textcolor{red}{p_{\\theta}\\left(Z \\mid X_{i}\\right)}}\\right) \\\\ =\\underset{q_{i}}{\\operatorname{minimize}} \\mathbb{E}_{Z \\sim q_{i}} \\log \\left(\\frac{q_{i}(Z)}{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z) / \\textcolor{red}{p_{\\theta}\\left(X_{i}\\right)}}\\right) \\\\ =\\underset{Z \\sim q_{i}}{\\operatorname{minimize}}\\left[\\log q_{i}(Z)-\\log p_{Z}(Z)-\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]+\\textcolor{red}{\\log p_{\\theta}\\left(X_{i}\\right)} \\end{gathered} \\] <p>Note, \\(q_{i}(z), p_{Z}(z)\\), and \\(p_{\\theta}(x \\mid z)\\) are tractable/known while \\(\\textcolor{red}{p_{\\theta}\\left(X_{i}\\right)}\\) and \\(\\textcolor{red}{p_{\\theta}\\left(z \\mid X_{i}\\right)}\\) are intractable/unknown. Since \\(\\textcolor{red}{\\log p_{\\theta}\\left(X_{i}\\right)}\\) does not depend on \\(q_{i}\\), all quantities needed in the optimization problems are tractable. However, solving this minimization problem to obtain each \\(q_{i}\\) for each data point \\(X_{i}\\) is computationally too expensive.</p> <p>Individual inference (not amortized): For each \\(X_{1}, \\ldots, X_{N}\\), find corresponding optimal \\(q_{1}, \\ldots, q_{N}\\) by solving</p> \\[ \\underset{q_{i}}{\\operatorname{minimize}} \\quad D_{\\mathrm{KL}}\\left(q_{i}(\\cdot) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right) \\] <p>This is expensive as it requires solving \\(N\\) separate optimization problems.</p> <p>We need variational approach and amortized inference.</p> <p>Concept 11.10 : Variational Approach and Amortized Inference</p> <p>General principle of variational approach: We can't directly use the \\(q\\) we want. So, instead, we propose a parameterized distribution \\(q_{\\phi}\\) that we can work with easily (in this case, sample from easily), and find a parameter setting that makes it as good as possible.</p> <p>Parametrization of VAE:</p> \\[ q_{\\phi}\\left(z \\mid X_{i}\\right) \\approx q_{i}^{\\star}(z)=p_{\\theta}\\left(z \\mid X_{i}\\right) \\quad \\text { for all } i=1, \\ldots, N \\] <p>Amortized inference: Train a neural network \\(q_{\\phi}(\\cdot \\mid x)\\) such that \\(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right)\\) approximates the optimal \\(q_{i}(\\cdot)\\).</p> \\[ \\underset{\\phi \\in \\Phi}{\\operatorname{minimize}} \\sum_{i=1}^{N} D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right) \\] <p>Approximation \\(q_{\\phi}\\left(z \\mid X_{i}\\right) \\approx p_{\\theta}\\left(z \\mid X_{i}\\right)\\) is often less precise than that of individual inference \\(q_{i}(z) \\approx\\) \\(p_{\\theta}\\left(z \\mid X_{i}\\right)\\), but amortized inference is often significantly faster.</p> <p>Concept 11.11 : Encoder \\(q_{\\phi}\\) Optimization</p> <p>In analogy with autoencoders, we call \\(q_{\\phi}\\) the encoder.</p> <p>Optimization problem for encoder (derived from Concept 11.9) :</p> \\[ \\begin{aligned} &amp; \\underset{\\phi \\in \\Phi}{\\operatorname{minimize}} \\sum_{i=1}^{N} D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right) \\\\ = &amp; \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right)\\right]+\\text { constant independent of } \\phi \\\\ = &amp; \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]-D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right) \\end{aligned} \\] <p>Concept 11.12 : Decoder \\(p_{\\theta}\\) Optimization</p> <p>In analogy with autoencoders, we call \\(p_{\\theta}\\) the decoder. Perform approximate MLE (derived from IS, Choice 2 of Concept 11.6) :</p> \\[ \\begin{aligned} &amp; \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\\\ \\stackrel{(a)}{\\approx} &amp; \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z_{i}\\right) p_{Z}\\left(Z_{i}\\right)}{q_{\\phi}\\left(Z_{i} \\mid X_{i}\\right)}\\right), \\quad Z_{i} \\sim q_{\\phi}\\left(z \\mid X_{i}\\right) \\\\ \\stackrel{(b)}{\\approx} &amp; \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right)\\right] \\\\ = &amp; \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]-D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right) \\end{aligned} \\] <p>The \\(\\stackrel{(a)}{\\approx}\\) step replaces expectation inside the log with an estimate with \\(Z_{i}\\). The \\(\\stackrel{(b)}{\\approx}\\) step replaces the random variable with the expectation. These steps take \\(\\mathbb{E}_{Z}\\) outside of the log (which can not be normally done). More on this later (Concept 11.14).</p>"},{"location":"books-and-courses/mfdnn/format/11/#definition-of-vae","title":"Definition of VAE","text":"<p>Definition 11.13 : Variational Lower Bound (VLB)</p> <p>The optimization objectives for the encoder (Concept 11.11) and decoder (Concept 11.12) are the same!</p> <p>Simultaneously train \\(p_{\\theta}\\) and \\(q_{\\phi}\\) by solving</p> \\[ \\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\underbrace{\\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]-D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right)}_{\\stackrel{\\text { def }}{=} \\mathrm{VLB}_{\\theta, \\phi}\\left(X_{i}\\right)} \\] <p>We refer to the optimization objective as the variational lower bound (VLB) or evidence lower bound (ELBO) for reasons that will be explained soon (Concept 11.14).</p> <p>Concept 11.14 : How tight lower bound is the VLB?</p> <p>How accurate is the approximation?</p> \\[ \\begin{aligned} &amp; \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\frac{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right] \\\\ &amp; \\stackrel{?}{\\approx} \\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right)\\right] \\\\ &amp;=\\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right) \\end{aligned} \\] <p>This turns out that </p> \\[ \\log p_{\\theta}\\left(X_{i}\\right) \\geq \\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right) \\] <p>So we are maximizing a lower bound of the log likelihood. How large is the gap?</p> <ul> <li>Log-likelihood \\(\\geq\\) VLB: Derivation 1</li> </ul> <p>Proof</p> <p>Derivation via Jensen inequality:</p> \\[ \\begin{aligned} \\log p_{\\theta}\\left(X_{i}\\right) &amp; =\\log \\mathbb{E}_{Z \\sim p_{Z}}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right] \\\\ &amp; =\\log \\left(\\mathbb{E}_{Z \\sim q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\left[p_{\\theta}\\left(X_{i} \\mid Z\\right) \\frac{p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right]\\right) \\\\ &amp; \\geq \\mathbb{E}_{Z \\sim q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\left[\\log \\left(p_{\\theta}\\left(X_{i} \\mid Z\\right) \\frac{p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right)\\right] \\\\ &amp; \\stackrel{\\text { def }}{=} \\mathrm{VLB}_{\\theta, \\phi}\\left(X_{i}\\right) \\end{aligned} \\] <p>Does not explicitly characterize gap.</p> <ul> <li>Log-likelihood \\(\\geq\\) VLB: Derivation 2</li> </ul> <p>Proof</p> <p>Derivation via KL divergence:</p> \\[ \\begin{aligned} D_{\\mathrm{KL}}\\left[q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right] &amp; =\\mathbb{E}_{Z \\sim q_{\\theta}\\left(z \\mid X_{i}\\right)}\\left[\\log q_{\\theta}\\left(Z \\mid X_{i}\\right)-\\log p_{\\theta}\\left(Z \\mid X_{i}\\right)\\right] \\\\ &amp; =\\underbrace{\\mathbb{E}_{Z \\sim q_{\\theta}\\left(z \\mid X_{i}\\right)}\\left[\\log q_{\\theta}\\left(Z \\mid X_{i}\\right)-\\log p_{Z}(Z)-\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]}_{=-\\mathrm{VLB}_{\\theta, \\phi}\\left(X_{i}\\right)}+\\log p_{\\theta}\\left(X_{i}\\right) \\end{aligned} \\] <p>and</p> \\[ \\begin{aligned} \\log p_{\\theta}\\left(X_{i}\\right) &amp;= \\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right)+\\underbrace{D_{\\mathrm{KL}}\\left[q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right]}_{\\geq 0} \\\\ &amp; \\geq \\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right) \\end{aligned} \\] <p>This derivation explicitly characterizes the gap as \\(D_{\\mathrm{KL}}\\left[q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right]\\).</p> \\[ \\log p_{\\theta}\\left(X_{i}\\right) - \\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right) = D_{\\mathrm{KL}}\\left[q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right] \\] <p>Concept 11.15 : VLB is tight if encoder is infinitely powerful.</p> <p>If the encoder \\(q_{\\phi}\\) is powerful enough such that there is a \\(\\phi^{\\star}\\) achieving</p> \\[ q_{\\phi^{\\star}}\\left(\\cdot \\mid X_{i}\\right)=p_{\\theta}\\left(\\cdot \\mid X_{i}\\right) \\] <p>or equivalently</p> \\[ D_{\\mathrm{KL}}\\left[q_{\\phi^{\\star}}\\left(\\cdot \\mid X_{i}\\right) \\| p_{\\theta}\\left(\\cdot \\mid X_{i}\\right)\\right]=0 \\] <p>Then</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\log p_{\\theta}\\left(X_{i}\\right)=\\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right) \\] <p>Definition 11.16 : Variational Autoencoder (VAE) Terminology</p> <p> </p> <ul> <li>Likelihood : \\(\\textcolor{red}{p_{\\theta}(x)}\\) (exact evaluation intractable)</li> <li>Prior : \\(p_{Z}(z)\\)</li> <li>Conditional distribution (decoder) : \\(p_{\\theta}(x \\mid z)\\)</li> <li>True posterior : \\(\\textcolor{red}{p_{\\theta}(z \\mid x)}\\) (exact evaluation intractable)</li> <li>Approximate posterior (encoder) : \\(q_{\\phi}(z \\mid x)\\)</li> </ul> <p>Conditional distribution \\(p_{\\theta}(x \\mid z)\\) and prior \\(p_{Z}(z)\\) determines the posterior \\(p_{\\theta}(z \\mid x)\\).</p> <p>There is no easy way to evaluate \\(p_{\\theta}(x)\\), but we can sample \\(X \\sim p_{\\theta}(x)\\) easily: \\(Z \\sim p_{Z}(z)\\) then \\(X \\sim p_{\\theta}(x \\mid Z)\\).</p> <p>NN in VAE do not directly generate random output. NN outputs parameters for random sampling.</p>"},{"location":"books-and-courses/mfdnn/format/11/#vae-standard-instance","title":"VAE Standard Instance","text":"<p>Definition 11.17 : VAE Standard Instance</p> <p>A standard VAE setup:</p> \\[ \\begin{aligned} &amp; p_{Z}=\\mathcal{N}(0, I) \\\\ &amp; q_{\\phi}(z \\mid x)=\\mathcal{N}\\left(\\mu_{\\phi}(x), \\Sigma_{\\phi}(x)\\right) \\text { with diagonal } \\Sigma_{\\phi} \\\\ &amp; p_{\\theta}(x \\mid z)=\\mathcal{N}\\left(f_{\\theta}(z), \\sigma^{2} I\\right) \\end{aligned} \\] <p>\\(\\mu_{\\phi}(x), \\Sigma_{\\phi}^{2}(x)\\), and \\(f_{\\theta}(z)\\) are deterministic NN.</p> <p>Using the following equation,</p> \\[ \\begin{aligned} &amp; D_{\\mathrm{KL}}\\left(\\mathcal{N}\\left(\\mu_{\\phi}(X), \\Sigma_{\\phi}(X)\\right) \\| \\mathcal{N}(0, I)\\right) \\\\ = &amp; \\frac{1}{2}\\left(\\operatorname{tr}\\left(\\Sigma_{\\phi}(X)\\right)+\\left\\|\\mu_{\\phi}(X)\\right\\|^{2}-d-\\log \\operatorname{det}\\left(\\Sigma_{\\phi}(X)\\right)\\right) \\\\ \\end{aligned} \\] <p>the training objective</p> \\[ \\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]-D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right) \\] <p>becomes</p> \\[ \\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{minimize}} \\sum_{i=1}^{N} \\frac{1}{\\sigma^{2}} \\mathbb{E}_{Z \\sim \\mathcal{N}\\left(\\mu_{\\phi}\\left(X_{i}\\right), \\Sigma_{\\phi}\\left(X_{i}\\right)\\right)}\\left\\|X_{i}-f_{\\theta}(Z)\\right\\|^{2}+\\operatorname{tr}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right)+\\left\\|\\mu_{\\phi}\\left(X_{i}\\right)\\right\\|^{2}-\\log \\operatorname{det}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right) \\] <p>Concept 11.18 : VAE Standard Instance with Reparameterization Trick</p> <p>The standard instance of VAE</p> \\[ \\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{minimize}} \\sum_{i=1}^{N} \\frac{1}{\\sigma^{2}} \\mathbb{E}_{Z \\sim \\mathcal{N}\\left(\\mu_{\\phi}\\left(X_{i}\\right), \\Sigma_{\\phi}\\left(X_{i}\\right)\\right)}\\left\\|X_{i}-f_{\\theta}(Z)\\right\\|^{2}+\\operatorname{tr}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right)+\\left\\|\\mu_{\\phi}\\left(X_{i}\\right)\\right\\|^{2}-\\log \\operatorname{det}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right) \\] <p>can be equivalently written with the reparameterization trick</p> \\[ \\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{minimize}} \\sum_{i=1}^{N} \\frac{1}{\\sigma^{2}} \\mathbb{E}_{\\varepsilon \\sim \\mathcal{N}(0, I)}\\left\\|X_{i}-f_{\\theta}\\left(\\mu_{\\phi}\\left(X_{i}\\right)+\\Sigma_{\\phi}^{1 / 2}\\left(X_{i}\\right) \\varepsilon\\right)\\right\\|^{2}+\\operatorname{tr}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right)+\\left\\|\\mu_{\\phi}\\left(X_{i}\\right)\\right\\|^{2}-\\log \\operatorname{det}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right) \\] <p>where \\(\\Sigma_{\\phi}^{1 / 2}\\) is diagonal with \\(\\sqrt{\\cdot}\\) of the diagonal elements of \\(\\Sigma_{\\phi}\\). (Remember, \\(\\Sigma_{\\phi}\\) is diagonal.)</p> <p>To clarify \\(Z \\stackrel{\\mathcal{D}}{=} \\mu_{\\phi}\\left(X_{i}\\right)+\\Sigma_{\\phi}^{1 / 2}\\left(X_{i}\\right) \\varepsilon\\), where \\(\\stackrel{\\mathcal{D}}{=}\\) denotes equality in distribution.</p> <p>We now have an objective amenable to stochastic optimization.</p> <p>Concept 11.19 : VAE Standard Instance Architecture</p> <ul> <li>Training (Without reparameterization trick)</li> </ul> <p> </p> <ul> <li>Training (With reparameterization trick)</li> </ul> <p> </p> <ul> <li> <p>Sampling</p> <p>During sampling, only the decoder network is used.</p> </li> </ul> <p> </p> <p>Concept 11.20 : Why variational</p> <p>VAE loss (VLB) contains a reconstruction loss resembling that of an autoencoder.</p> \\[ \\begin{aligned} \\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right) &amp; =\\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]-D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right) \\\\ &amp; =-\\frac{1}{2 \\sigma^{2}} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\left\\|X_{i}-f_{\\theta}(Z)\\right\\|^{2}\\right]-D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right) \\\\ &amp; =-\\underbrace{\\frac{1}{2 \\sigma^{2}} \\mathbb{E}_{\\varepsilon \\sim \\mathcal{N}(0, I)}\\left\\|X_{i}-f_{\\theta}\\left(\\mu_{\\phi}\\left(X_{i}\\right)+\\Sigma_{\\phi}^{1 / 2}\\left(X_{i}\\right) \\varepsilon\\right)\\right\\|^{2}}_{\\text {Reconstruction loss }}-\\underbrace{D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right)}_{\\text {Regularization }} \\end{aligned} \\] <p>VLB also contains a regularization term on the output of the encoder, which is not present in standard autoencoder losses.</p> <p>The choice of \\(\\sigma\\) determines the relative weight between the reconstruction loss and the regularization.</p>"},{"location":"books-and-courses/mfdnn/format/11/#training-vae","title":"Training VAE","text":"<p>Concept 11.21 : Training VAE with RT</p> <p>To obtain stochastic gradients of the VAE standard instance </p> \\[ \\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{minimize}} \\sum_{i=1}^{N} \\frac{1}{\\sigma^{2}} \\mathbb{E}_{\\varepsilon \\sim \\mathcal{N}(0, I)}\\left\\|X_{i}-f_{\\theta}\\left(\\mu_{\\phi}\\left(X_{i}\\right)+\\Sigma_{\\phi}^{1 / 2}\\left(X_{i}\\right) \\varepsilon\\right)\\right\\|^{2}+\\operatorname{tr}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right)+\\left\\|\\mu_{\\phi}\\left(X_{i}\\right)\\right\\|^{2}-\\log \\operatorname{det}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right) \\] <p>select a data \\(X_{i}\\), sample \\(\\varepsilon_{i} \\sim \\mathcal{N}(0, I)\\), evaluate</p> \\[ -\\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}, \\varepsilon_{i}\\right) \\stackrel{\\text { def }}{=} \\frac{1}{\\sigma^{2}}\\left\\|X_{i}-f_{\\theta}\\left(\\mu_{\\phi}\\left(X_{i}\\right)+\\Sigma_{\\phi}^{1 / 2}\\left(X_{i}\\right) \\varepsilon_{i}\\right)\\right\\|^{2}+\\operatorname{tr}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right)+\\left\\|\\mu_{\\phi}\\left(X_{i}\\right)\\right\\|^{2}-\\log \\operatorname{det}\\left(\\Sigma_{\\phi}\\left(X_{i}\\right)\\right) \\] <p>and backprop on \\(\\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}, \\varepsilon_{i}\\right)\\).</p> <p>Usually, batch of \\(X_{i}\\) is selected. One can sample multiple \\(Z_{i, 1}, \\ldots, Z_{i, K}\\) (equivalently \\(\\varepsilon_{i, 1}, \\ldots, \\varepsilon_{i, K}\\) ) for each \\(X_{i}\\).</p> <p>Concept 11.22 : Traning VAE with Log-Derivative Trick</p> <p>Computing stochastic gradients without the reparameterization trick.</p> \\[ \\underset{\\theta \\in \\Theta, \\phi \\in \\Phi}{\\operatorname{maximize}} \\sum_{i=1}^{N} \\underbrace{\\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right)\\right]}_{\\stackrel{\\text { def }}{=} \\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right)} \\] <p>To obtain unbiased estimates of \\(\\nabla_{\\theta}\\), compute</p> \\[ \\frac{1}{K} \\sum_{k=1}^{K} \\log p_{\\theta}\\left(X_{i} \\mid Z_{i, k}\\right), \\quad Z_{i, 1}, \\ldots, Z_{i, K} \\sim q_{\\phi}\\left(z \\mid X_{i}\\right) \\] <p>and backprop with respect to \\(\\theta\\).</p> <p>We differentiate the VLB objectives</p> \\[ \\begin{aligned} \\nabla_{\\phi} \\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right)\\right] &amp; =\\nabla_{\\phi} \\int \\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid z\\right) p_{Z}(z)}{q_{\\phi}\\left(z \\mid X_{i}\\right)}\\right) q_{\\phi}\\left(z \\mid X_{i}\\right) d z \\\\ &amp; =\\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\left(\\nabla_{\\phi} \\log q_{\\phi}\\left(Z \\mid X_{i}\\right)\\right) \\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z\\right) p_{Z}(Z)}{q_{\\phi}\\left(Z \\mid X_{i}\\right)}\\right)\\right] \\end{aligned} \\] <p>To obtain unbiased estimates of \\(\\nabla_{\\phi}\\), compute</p> \\[ \\frac{1}{K} \\sum_{k=1}^{K}\\left(\\nabla_{\\phi} \\log q_{\\phi}\\left(Z_{i, k} \\mid X_{i}\\right)\\right) \\log \\left(\\frac{p_{\\theta}\\left(X_{i} \\mid Z_{i, k}\\right) p_{Z}\\left(Z_{i, k}\\right)}{q_{\\phi}\\left(Z_{i, k} \\mid X_{i}\\right)}\\right), \\quad Z_{i, 1}, \\ldots, Z_{i, K} \\sim q_{\\phi}\\left(z \\mid X_{i}\\right) \\]"},{"location":"books-and-courses/mfdnn/format/11/#researches","title":"Researches","text":"<p>Concept 11.23 : VQ-VAE</p> <p> </p> <p> </p> <p>(A. van den Oord, O. Vinyals, and K. Kavukcuoglu, Neural discrete representation learning, NeurIPS, 2017.)</p> <p>Concept 11.24 : VQ-VAE-2</p> <p> </p> <p> </p> <p> </p> <p>(A. Razavi, A. van den Oord, and O. Vinyals, Generating diverse high-fidelity images with VQ-VAE-2, NeurIPS, 2019.)</p> <p>Concept 11.25 : \\(\\beta\\)-VAE</p> <p>Uses the loss</p> \\[ \\ell_{\\theta, \\phi}\\left(X_{i}\\right)=\\mathbb{E}_{Z \\sim q_{\\phi}\\left(z \\mid X_{i}\\right)}\\left[\\log p_{\\theta}\\left(X_{i} \\mid Z\\right)\\right]-\\beta D_{\\mathrm{KL}}\\left(q_{\\phi}\\left(\\cdot \\mid X_{i}\\right) \\| p_{Z}(\\cdot)\\right) \\] <p>when \\(\\beta=1, \\ell_{\\theta, \\phi}\\left(X_{i}\\right)=\\operatorname{VLB}_{\\theta, \\phi}\\left(X_{i}\\right)\\), i.e., \\(\\beta\\)-VAE coincides with VAE when \\(\\beta=1\\).</p> <p>With \\(\\beta&gt;1\\), authors observed better feature disentanglement.</p> <p>(I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner, \u03b2-VAE: Learning basic visual concepts with a constrained variational framework, ICLR, 2017.)</p>"},{"location":"books-and-courses/mfdnn/format/12/","title":"\u00a7 12. Generative Adversarial Networks","text":""},{"location":"books-and-courses/mfdnn/format/12/#minimax-optimization","title":"Minimax Optimization","text":"<p>Definition 12.1 : Minimax Optimization Problem</p> <p>In a minimax optimization problem we minimize with respect to one variable and maximize with respect to another:</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathcal{L}(\\theta, \\phi) \\] <p>We say \\(\\left(\\theta^{\\star}, \\phi^{\\star}\\right)\\) is a solution to the minimax problem if \\(\\theta^{\\star} \\in \\Theta, \\phi^{\\star} \\in \\Phi\\), and</p> \\[ \\mathcal{L}\\left(\\theta^{\\star}, \\phi\\right) \\leq \\mathcal{L}\\left(\\theta^{\\star}, \\phi^{\\star}\\right) \\leq \\mathcal{L}\\left(\\theta, \\phi^{\\star}\\right), \\quad \\forall \\theta \\in \\Theta, \\phi \\in \\Phi . \\] <p>In other words, unilaterally deviating from \\(\\theta^{\\star} \\in \\Theta\\) increases the value of \\(\\mathcal{L}(\\theta, \\phi)\\) while unilaterally deviating from \\(\\phi^{\\star} \\in \\Phi\\) decreases the value of \\(\\mathcal{L}(\\theta, \\phi)\\). In yet other words, the solution is defined as a Nash equilibrium in a 2-player zero-sum game.</p> <p>(There are other broader definitions of a \u201csolution\u201d in minimax optimization problems. Our definition is, in a sense, the strictest definition.)</p> <p>Concept 12.2 : Minimax vs. Maximin</p> <p>When a solution (as we defined in Definition 12.1) does not exist, then min-max is not the same as max-min:</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathcal{L}(\\theta, \\phi) \\neq \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\mathcal{L}(\\theta, \\phi) \\] <p>This is a technical distinction that we will not explore in this class.</p> <p>Concept 12.3 : Minimax Optimization</p> <p>So far, we trained NN by solving minimization problems.</p> <p>However, GANs are trained by solving minimax problems. Since the advent of GANs, minimax training has become more widely used in all areas of deep learning.</p> <p>Examples:</p> <ul> <li>Adversarial training to make NN robust against adversarial attacks.</li> <li>Domain adversarial networks to train NN to make fair decisions (e.g. not base its decision on a persons race or gender).</li> </ul> <p>Definition 12.4 : Minimax Optimization Algorithm</p> <p>First, consider deterministic gradient setup. Let \\(\\alpha\\) and \\(\\beta\\) be the stepsizes (learning rates) for the descent and ascent steps respectively.</p> <ul> <li> <p>Simultaneous gradient ascent-descent</p> \\[ \\begin{aligned} \\phi^{k+1} &amp; =\\phi^{k}+\\beta \\nabla_{\\phi} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k}\\right) \\\\ \\theta^{k+1} &amp; =\\theta^{k}-\\alpha \\nabla_{\\theta} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k}\\right) \\end{aligned} \\] </li> <li> <p>Alternating gradient ascent-descent</p> \\[ \\begin{aligned} \\phi^{k+1} &amp; =\\phi^{k}+\\beta \\nabla_{\\phi} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k}\\right) \\\\ \\theta^{k+1} &amp; =\\theta^{k}-\\alpha \\nabla_{\\theta} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k+1}\\right) \\end{aligned} \\] </li> <li> <p>Gradient multi-ascent-single-descent</p> \\[ \\begin{aligned} \\phi_{0}^{k+1} &amp; =\\phi_{n_{\\mathrm{dis}}}^{k} \\\\ \\phi_{i+1}^{k+1} &amp; =\\phi_{i}^{k+1}+\\beta \\nabla_{\\phi} \\mathcal{L}\\left(\\theta^{k}, \\phi_{i}^{k+1}\\right), \\quad \\text { for } i=0, \\ldots, n_{\\mathrm{dis}}-1 \\\\ \\theta^{k+1} &amp; =\\theta^{k}-\\alpha \\nabla_{\\theta} \\mathcal{L}\\left(\\theta^{k}, \\phi_{n_{\\mathrm{dis}}}^{k+1}\\right) \\end{aligned} \\] <p>(\\(n_{\\text {dis }}\\) stands for number of discriminator updates.) When \\(n_{\\text {dis }}=1\\), this algorithm reduces to alternating ascent-descent.</p> </li> </ul> <p>Definition 12.5 : Stochastic Minimax Optimization</p> <p>In deep learning, however, we have access to stochastic gradients.</p> <ul> <li>Stochastic gradient simultaneous ascent-descent</li> </ul> \\[ \\begin{aligned} \\phi^{k+1} &amp; =\\phi^{k}+\\beta g_{\\phi}^{k}, &amp; \\mathbb{E}\\left[g_{\\phi}^{k}\\right]=\\nabla_{\\phi} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k}\\right) \\\\ \\theta^{k+1} &amp; =\\theta^{k}-\\alpha g_{\\theta}^{k}, &amp; \\mathbb{E}\\left[g_{\\theta}^{k}\\right]=\\nabla_{\\theta} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k}\\right) \\end{aligned} \\] <ul> <li>Stochastic gradient alternating ascent-descent</li> </ul> \\[ \\begin{aligned} \\phi^{k+1} &amp; =\\phi^{k}+\\beta g_{\\phi}^{k}, &amp; \\mathbb{E}\\left[g_{\\phi}^{k}\\right]=\\nabla_{\\phi} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k}\\right) \\\\ \\theta^{k+1} &amp; =\\theta^{k}-\\alpha g_{\\theta}^{k}, &amp; \\mathbb{E}\\left[g_{\\theta}^{k}\\right]=\\nabla_{\\theta} \\mathcal{L}\\left(\\theta^{k}, \\phi^{k+1}\\right) \\end{aligned} \\] <ul> <li>Stochastic gradient multi-ascent-single-descent</li> </ul> \\[ \\begin{aligned} \\phi_{0}^{k+1} &amp; =\\phi_{n_{\\text {dis }}}^{k} \\\\ \\phi_{i+1}^{k+1} &amp; =\\phi_{i}^{k+1}+\\beta \\nabla_{\\phi} g_{\\phi}^{k, i}, \\quad \\mathbb{E}\\left[g_{\\phi}^{k, i}\\right]=\\nabla_{\\phi} \\mathcal{L}\\left(\\theta^{k}, \\phi_{i}^{k+1}\\right), \\quad \\text { for } i=0, \\ldots, n_{\\text {dis }}-1 \\\\ \\theta^{k+1} &amp; =\\theta^{k}-\\alpha g_{\\theta}^{k}, \\quad \\mathbb{E}\\left[g_{\\theta}^{k}\\right]=\\nabla_{\\theta} \\mathcal{L}\\left(\\theta^{k}, \\phi_{n_{\\text {dis }}^{k}}^{k+1}\\right) \\end{aligned} \\] <p>Concept 12.6 : Minimax Optimization in Pytorch</p> <p>To perform minimax optimization in PyTorch, we maintain two separate optimizers, one for the ascent, one for the descent. The <code>OPTIMIZER</code> can be anything like <code>SGD</code> or <code>Adam</code>.</p> <pre><code>G = Generator(...).to(device)\nD = Discriminator(...).to(device)\nD_optimizer = optim.OPTIMIZER(D.parameters(), lr = beta)\nG_optimizer = optim.OPTIMIZER(G.parameters(), lr = alpha)\n</code></pre> <ul> <li> <p>Simultaneous ascent-descent:</p> <pre><code>Evaluate D_loss\nD_loss.backward()\nEvaluate G_loss\nG_loss.backward()\nD_optimizer.step()\nG_optimizer.step()\n</code></pre> </li> <li> <p>Alternating ascent-descent</p> <pre><code>Evaluate D_loss\nD_loss.backward()\nD_optimizer.step()\nEvaluate G_loss\nG_loss.backward()\nG_optimizer.step()\n</code></pre> </li> <li> <p>Multi-ascent-single-descent</p> <pre><code>for _ in range(ndis) :\n    Evaluate D loss\n    D_loss.backward()\n    D_optimizer.step()\nEvaluate G_loss\nG_loss.backward()\nG_optimizer.step()\n</code></pre> </li> </ul>"},{"location":"books-and-courses/mfdnn/format/12/#definition-of-gan","title":"Definition of GAN","text":"![](.././assets/12.1.png){: width=\"100%\"}  <p>These are synthetic (fake) images made with GAN.</p> <p>(A. Brock, J. Donahue, and K. Simonyan, Large scale GAN training for high fidelity natural image synthesis, ICLR, 2019.)</p> <p>Definition 12.7 : Generative Adversarial Networks (GAN)</p> <p>In generative adversarial networks (GAN) a generator network and a discriminator network compete adversarially.</p> <p> </p> <p>Given data \\(X_{1}, \\ldots, X_{N} \\sim p_{\\text {true }}\\), GAN aims to learn \\(p_{\\theta} \\approx p_{\\text {true }}\\).</p> <p>Generator aims to generate fake data similar to training data.</p> <p>Discriminator aims to distinguish the training data from fake data.</p> <p>Analogy: Criminal creating fake money vs. police distinguishing fake money from real.</p> <p>(I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, Generative adversarial networks, NeurIPS, 2014.)</p> <p>Definition 12.8 : Generator Network</p> <p>The generator \\(G_{\\theta}: \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{n}\\) is a neural network parameterized by \\(\\theta \\in \\Theta\\). The generator takes a random latent vector \\(Z \\sim p_{Z}\\) as input and outputs generated (fake) data \\(\\tilde{X}=G_{\\theta}(Z)\\). The latent distribution is usually \\(p_{Z}=\\mathcal{N}(0, I)\\).</p> <p>Write \\(p_{\\theta}\\) for the probability distribution of \\(\\tilde{X}=G_{\\theta}(Z)\\). Although we can't evaluate the density \\(p_{\\theta}(x)\\), neither exactly nor approximately, we can sample from \\(\\tilde{X} \\sim p_{\\theta}\\).</p> <p>Definition 12.9 : Discriminator Network</p> <p>The discriminator \\(D_{\\phi}: \\mathbb{R}^{n} \\rightarrow(0,1)\\) is a neural network parameterized by \\(\\phi \\in \\Phi\\). The discriminator takes an image \\(X\\) as input and outputs whether \\(X\\) is a real or fake. (Real : \\(X\\) comes from a data set, i.e., \\(X \\sim p_{\\text{true}}\\). Fake : generated by \\(G_{\\theta}\\), i.e., \\(X \\sim p_{\\theta}\\).)</p> <ul> <li>\\(D_{\\phi}(X) \\approx 1\\) : discriminator confidently predicts \\(X\\) is real.</li> <li>\\(D_{\\phi}(X) \\approx 0\\) : discriminator confidently predicts \\(X\\) is fake.</li> <li>\\(D_{\\phi}(X) \\approx 0.5\\) : discriminator is unsure whether \\(X\\) is real or fake.</li> </ul> <p>Concept 12.10 : Discriminator Loss</p> <p>Cost of incorrectly classifying real as fake (type I error):</p> \\[ \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[-\\log D_{\\phi}(X)\\right] \\] <p>Cost of incorrectly classifying fake as real (type II error):</p> \\[ \\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}\\left[-\\log \\left(1-D_{\\phi}(\\tilde{X})\\right)\\right]=\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[-\\log \\left(1-D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\] <p>Discriminator solves</p> \\[ \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\quad \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log D_{\\phi}(X)\\right]+\\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}\\left[\\log \\left(1-D_{\\phi}(\\tilde{X})\\right)\\right] \\] <p>which is equivalent to</p> \\[ \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\quad \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log D_{\\phi}(X)\\right]+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[\\log \\left(1-D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\] <p>We can view</p> \\[ \\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}\\left[\\log \\left(1-D_{\\phi}(\\tilde{X})\\right)\\right]=\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[\\log \\left(1-D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\] <p>as an instance of the reparameterization technique.</p> <p>The loss</p> \\[ \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log D_{\\phi}(X)\\right]+\\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}\\left[\\log \\left(1-D_{\\phi}(\\tilde{X})\\right)\\right] \\] <p>puts equal weight on type I and type II errors. Alternatively, one can use the loss</p> \\[ \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log D_{\\phi}(X)\\right]+\\lambda \\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}\\left[\\log \\left(1-D_{\\phi}(\\tilde{X})\\right)\\right] \\] <p>where \\(\\lambda&gt;0\\) represents the relative significance of a type II error over a type I error.</p> <p>Concept 12.11 : Generator Loss</p> <p>Since the goal of the generator is to deceive the discriminator, the generator minimizes the same loss as Concept 12.10.</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log D_{\\phi}(X)\\right]+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[\\log \\left(1-D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\] <p>(The generator and discriminator operate under a zero-sum game.)</p> <p>Note, only the second term depend on \\(\\theta\\), while the both terms depend on \\(\\phi\\).</p> <p>Concept 12.12 : Empirical Risk Minimization for Discriminator / Generator</p> <p>In practice, we have finite samples \\(X_{1}, \\ldots, X_{N}\\), so we instead use the loss</p> \\[ \\frac{1}{N} \\sum_{i=1}^{N} \\log D_{\\phi}\\left(X_{i}\\right)+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[\\log \\left(1-D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\] <p>Since \\(\\tilde{X}=G_{\\theta}(Z)\\) is generated with \\(Z \\sim p_{Z}\\), we have unlimited \\(\\tilde{X}\\) samples. So we replace \\(\\mathbb{E}_{X} \\approx \\frac{1}{N} \\sum\\) while leaving \\(\\mathbb{E}_{Z}\\) as is.</p> <p>Definition 12.13 : Minimax Training (Zero-Sum Game) for GAN</p> <p>Train generator and discriminator simultaneously by solving</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathcal{L}(\\theta, \\phi) \\] <p>where</p> \\[ \\mathcal{L}(\\theta, \\phi)=\\frac{1}{N} \\sum_{i=1}^{N} \\log D_{\\phi}\\left(X_{i}\\right)+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[\\log \\left(1-D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\] <p>It remains to specify the architectures for \\(G_{\\theta}\\) and \\(D_{\\phi}\\).</p> <p>Definition 12.14 : DCGAN</p> <p>The original GAN was also deep and convolutional. However, Radford et al.'s Deep Convolutional Generative Adversarial Networks (DCGAN) paper proposed the following architectures, which crucially utilize batchnorm.</p> <p>Use batchnorm in both the generator and the discriminator after transposed conv and conv layers.</p> <p> </p> <p>(A. Radford, L. Metz, and S. Chintala, Unsupervised representation learning with deep convolutional generative adversarial networks, ICLR, 2016.)</p>"},{"location":"books-and-courses/mfdnn/format/12/#f-gan","title":"f-GAN","text":"<p>Definition 12.15 : f-Divergence</p> <p>The f-divergence of \\(p\\) from \\(q\\), where \\(f\\) is a convex function such that \\(f(1)=0\\), is</p> \\[ D_{f}(p \\| q)=\\int f\\left(\\frac{p(x)}{q(x)}\\right) q(x) d x, \\] <p>This includes the KL divergence:</p> <ul> <li>If \\(f(u)=u \\log u\\), then \\(D_{f}(p \\| q)=D_{\\mathrm{KL}}(p \\| q)\\).</li> <li>If \\(f(u)=-\\log u\\), then \\(D_{f}(p \\| q)=D_{\\text {KL }}(q \\| p)\\).</li> </ul> <p>Definition 12.16 : JS-Divergence</p> <p>Jensen-Shannon-divergence (JS-divergence) is</p> \\[ D_{\\mathrm{JS}}(p, q)=\\frac{1}{2} D_{\\mathrm{KL}}\\left(p \\| \\frac{1}{2}(p+q)\\right)+\\frac{1}{2} D_{\\mathrm{KL}}\\left(q \\| \\frac{1}{2}(p+q)\\right) \\] <p>With, \\(f(u)=\\left\\{\\begin{array}{ll}\\frac{1}{2}\\left(u \\log u-(u+1) \\log \\frac{u+1}{2}\\right) &amp; \\text { for } u \\geq 0 \\\\ \\infty &amp; \\text { otherwise }\\end{array}\\right.\\) we have \\(D_{f}=D_{\\mathrm{JS}}\\).</p> <p>With, \\(f(u)=\\left\\{\\begin{array}{ll}u \\log u-(u+1) \\log (u+1)+\\log 4 &amp; \\text { for } u \\geq 0 \\\\ \\infty &amp; \\text { otherwise }\\end{array}\\right.\\) we have \\(D_{f}=2 D_{\\mathrm{JS}}\\).</p> <p>Concept 12.17 : GAN \\(\\approx\\) JSD minimization</p> <p>Let us understand the minimax problem</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathcal{L}(\\theta, \\phi) \\] <p>via the minimization problem</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\mathcal{J}(\\theta) \\] <p>where</p> \\[ \\mathcal{J}(\\theta)=\\sup _{\\phi \\in \\Phi} \\mathcal{L}(\\theta, \\phi) \\] <p>For simplicity, assume the discriminator is infinitely powerful, i.e., \\(D_{\\phi}(x)\\) can represent any arbitrary function.</p> <p>Note</p> \\[ \\begin{aligned} \\mathcal{L}(\\theta, \\phi) &amp; =\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log D_{\\phi}(X)\\right]+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[\\log \\left(1-D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\\\ &amp; =\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log D_{\\phi}(X)\\right]+\\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}\\left[\\log \\left(1-D_{\\phi}(\\tilde{X})\\right)\\right] \\\\ &amp; =\\int p_{\\text {true }}(x) \\log D_{\\phi}(x)+p_{\\theta}(x) \\log \\left(1-D_{\\phi}(x)\\right) d x \\end{aligned} \\] <p>Since</p> \\[ \\frac{d}{d y}(a \\log y+b \\log (1-y))=0 \\quad \\Rightarrow \\quad y^{\\star}=\\frac{a}{a+b} \\] <p>The integral is maximized by</p> \\[ D_{\\phi^{\\star}}(x)=\\frac{p_{\\text {true }}(x)}{p_{\\text {true }}(x)+p_{\\theta}(x)} \\] <p>If we plug in the optimal discriminator,</p> \\[ D_{\\phi^{\\star}}(x)=\\frac{p_{\\text {true }}(x)}{p_{\\text {true }}(x)+p_{\\theta}(x)} \\] <p>we get</p> \\[ \\begin{aligned} \\mathcal{L}\\left(\\theta, \\phi^{\\star}\\right) &amp; =\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log \\frac{p_{\\text {true }}(X)}{p_{\\text {true }}(X)+p_{\\theta}(X)}\\right]+\\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}\\left[\\log \\frac{p_{\\theta}(\\tilde{X})}{p_{\\text {true }}(\\tilde{X})+p_{\\theta}(\\tilde{X})}\\right] \\\\ &amp; =2 D_{\\mathrm{JS}}\\left(p_{\\text {true }}, p_{\\theta}\\right)-\\log (4) \\end{aligned} \\] <p>Therefore,</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathcal{L}(\\theta, \\phi) \\approx \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} D_{\\mathrm{JS}}\\left(p_{\\text {true }}, p_{\\theta}\\right) \\] <p>Concept 12.18 : Motivation for f-GAN</p> <p>With GANs, we started from a minimax formulation and later reinterpreted it as minimizing the JS-divergence. (Concept 12.17)</p> <p>Let us instead the start from an f-divergence minimization</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad D_{f}\\left(p_{\\text {true }} \\| p_{\\theta}\\right) \\] <p>and then variationally approximate \\(D_{f}\\) to obtain a minimax formulation.</p> <p>Variational approach: Evaluating \\(D_{f}\\) directly is difficult, so we pose it as a maximization problem and parameterize the maximizing function as a \"discriminator\" neural network.</p> <p>For simplicity, however, we only consider the order</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad D_{f}\\left(p_{\\text {true }} \\| p_{\\theta}\\right) \\] <p>However, one can also consider</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad D_{f}\\left(p_{\\theta} \\| p_{\\text {true }}\\right) \\] <p>to obtain similar results.</p> <p>Definition 12.19 : Convex Conjugate</p> <p>Let \\(f: \\mathbb{R} \\rightarrow \\mathbb{R} \\cup\\{\\infty\\}\\). Define the convex conjugate of \\(f\\) as</p> \\[ f^{*}(t)=\\sup _{u \\in \\mathbb{R}}\\{t u-f(u)\\} \\] <p>where \\(f^{*}: \\mathbb{R} \\rightarrow \\mathbb{R} \\cup\\{\\infty\\}\\). This is also referred to as the Legendre transform.</p> <p>If \\(f\\) is a nice (closed and proper) convex function, then \\(f^{*}\\) is convex and \\(f^{* *}=f\\), i.e., the conjugate of the conjugate is the original function. (So conjugacy is an involution in the space of convex functions.) So</p> \\[ f(u)=\\sup _{t \\in \\mathbb{R}}\\left\\{t u-f^{*}(t)\\right\\} \\] <p>Example 12.20 : Examples of Convex Conjugate</p> <ul> <li> <p>KL</p> \\[ f_{\\mathrm{KL}}(u)= \\begin{cases} u \\log u &amp; \\text { for } u \\geq 0 \\\\ \\infty &amp; \\text { otherwise } \\\\ \\end{cases} \\] \\[ f_{\\mathrm{KL}}^{*}(t)=\\exp(t-1) \\] </li> <li> <p>LK (Reversed KL)</p> \\[ f_{\\mathrm{LK}}(u)= \\begin{cases} -      \\log u &amp; \\text { for } u &gt; 0 \\\\ \\infty &amp; \\text { otherwise } \\\\ \\end{cases} \\] \\[ f_{\\mathrm{LK}}^{*}(u)= \\begin{cases} -1-\\log(-t) &amp; \\text { for } t &lt; 0 \\\\ \\infty &amp; \\text { otherwise } \\\\ \\end{cases} \\] </li> <li> <p>SH (Squared Hellinger Distance)</p> \\[ f_{\\mathrm{SH}}(u)=(\\sqrt{u}-1)^2 \\] \\[ f_{\\mathrm{SH}}^{*}(t)= \\begin{cases} \\frac{1}{1/t-1} &amp; \\text { for } t &lt; 1 \\\\ \\infty &amp; \\text { otherwise } \\\\ \\end{cases} \\] </li> <li> <p>JS</p> \\[ f_{\\mathrm{JS}}(u)= \\begin{cases} u\\log u-(u+1) \\log (u+1)+\\log 4 &amp; \\text { for } u \\geq 0 \\\\ \\infty &amp; \\text { otherwise } \\\\ \\end{cases} \\] \\[ f_{\\mathrm{JS}}^{*}(u)= \\begin{cases} -\\log (1-\\exp (t))-\\log 4 &amp; \\text { for } t &lt; 0 \\\\ \\infty &amp; \\text { otherwise } \\\\ \\end{cases} \\] </li> </ul> <p>We get the following f-divergences:</p> \\[ \\begin{aligned} &amp; D_{f_{\\mathrm{KL}}}(p \\| q)=D_{\\mathrm{KL}}(p \\| q) \\\\ &amp; D_{f_{\\mathrm{LK}}}(p \\| q)=D_{\\mathrm{KL}}(q \\| p) \\\\ &amp; D_{f_{\\mathrm{SH}}}(p \\| q)=D_{\\mathrm{SH}}(q, p) \\\\ &amp; D_{f_{\\mathrm{JS}}}(p \\| q)=2 D_{\\mathrm{JS}}(q, p) \\end{aligned} \\] <p>We don't use the following property, but it's interesting so we mention it. If \\(f\\) and \\(f^{*}\\) are differentiable, then</p> \\[ \\left(f^{\\prime}\\right)^{-1}=\\left(f^{*}\\right)^{\\prime} \\] \\[ \\begin{array}{ll} \\frac{d}{d u} f_{\\mathrm{KL}}(u)=1+\\log u &amp; \\frac{d}{d t} f_{\\mathrm{KL}}^{*}(t)=\\exp (t-1) \\\\ \\frac{d}{d u} f_{\\mathrm{LK}}(u)=-\\frac{1}{u} &amp; \\frac{d}{d t} f_{\\mathrm{LK}}^{*}(t)=-\\frac{1}{t} \\\\ \\frac{d}{d u} f_{\\mathrm{SH}}(u)=1-\\frac{1}{\\sqrt{u}} &amp; \\frac{d}{d t} f_{\\mathrm{SH}}^{*}(t)=\\frac{1}{(1-t)^{2}} \\\\ \\frac{d}{d u} f_{\\mathrm{JS}}(u)=\\log \\frac{u}{1+u} &amp; \\frac{d}{d t} f_{\\mathrm{JS}}^{*}(t)=\\frac{1}{e^{-t}-1} \\end{array} \\] <p>Concept 12.21 : Variational Formulation of f-Divergence</p> <p>Variational formulation of f-divergence:</p> \\[ \\begin{aligned} D_{f}(p \\| q) &amp; =\\int q(x) f\\left(\\frac{p(x)}{q(x)}\\right) d x \\\\ &amp; =\\int q(x) \\sup _{t}\\left\\{t \\frac{p(x)}{q(x)}-f^{*}(t)\\right\\} d x=\\int q(x) T^{\\star}(x) \\frac{p(x)}{q(x)}-q(x) f^{*}\\left(T^{\\star}(x)\\right) d x \\\\ &amp; =\\sup _{T \\in \\mathcal{T}}\\left(\\int p(x) T(x) d x-\\int q(x) f^{*}(T(x)) d x\\right)=\\sup _{T \\in \\mathcal{T}}\\left(\\mathbb{E}_{X \\sim p}[T(X)]-\\mathbb{E}_{\\tilde{X} \\sim q}\\left[f^{*}(T(\\tilde{X}))\\right]\\right) \\\\ &amp; \\geq \\sup _{\\phi \\in \\Phi}\\left(\\mathbb{E}_{X \\sim p}\\left[D_{\\phi}(X)\\right]-\\mathbb{E}_{\\tilde{X} \\sim q}\\left[f^{*}\\left(D_{\\phi}(\\tilde{X})\\right)\\right]\\right) \\end{aligned} \\] <p>where \\(\\mathcal{T}\\) is the set of all measurable functions. In particular, \\(\\mathcal{T}\\) contains \\(T^{\\star}(x)=\\underset{t}{\\operatorname{argmax}}\\left\\{t \\frac{p(x)}{q(x)}-f^*(t)\\right\\}\\) \\(D_{\\phi}\\) is a neural network parameterized by \\(\\phi\\).</p> <p>Definition 12.22 : f-GAN Minimax Formulation</p> <p>Minimax formulation of f-GANs.</p> \\[ \\begin{gathered} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} D_{f}\\left(p_{\\text {true }} \\| p_{\\theta}\\right) \\\\ \\approx \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[D_{\\phi}(X)\\right]-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[f^{*}\\left(D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right] \\end{gathered} \\] <p>Concept 21.23 : f-GAN with KL Divergence</p> <p>Instantiate f-GAN with KL-divergence: </p> \\[ f^{*}(t)=e^{t-1} \\] \\[ \\begin{aligned} &amp; \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad D_{\\mathrm{KL}}\\left(p_{\\text {true }} \\| p_{\\theta}\\right) \\\\ &amp; \\approx \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[D_{\\phi}(X)\\right]-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[e^{D_{\\phi}\\left(G_{\\theta}(Z)\\right)-1}\\right] \\\\ &amp; \\stackrel{(*)}{=} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} 1+\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[D_{\\phi}(X)\\right]-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[e^{D_{\\phi}\\left(G_{\\theta}(Z)\\right)}\\right] \\\\ &amp; =\\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[D_{\\phi}(X)\\right]-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[e^{D_{\\phi}\\left(G_{\\theta}(Z)\\right)}\\right] \\end{aligned} \\] <p>Step (*) uses the substitution \\(D_{\\phi} \\mapsto D_{\\phi}+1\\), which is valid if the final layer of \\(D_{\\phi}\\) has a trainable bias term. ( \\(D_{\\phi}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}\\).)</p> <p>Note that most of the time, the convex conjugate \\(f^{*}(t)\\) has a constraint on it. When the constraint is violated, the \\(f^{*}(t)=\\infty\\) case makes the maximization objective \\(-\\infty\\). However, directly enforcing the neural networks to satisfy \\(f^{*}(D_{\\phi}\\left(G_{\\theta}(z)\\right))&lt;\\infty\\) is awkward.</p> <p>Concept 21.24 : Resolving Infinity with Output Activation</p> <p>When \\(D_{\\phi}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}\\) and \\(\\left\\{t \\mid f^{*}(t)&lt;\\infty\\right\\} \\neq \\mathbb{R}\\), then \\(f^{*}\\left(D_{\\phi}(\\tilde{X})\\right)=\\infty\\) is possible. To prevent this, substitute \\(T(x) \\mapsto \\rho(\\tilde{T}(x))\\), where \\(\\rho: \\mathbb{R} \\rightarrow\\left\\{t \\mid f^{*}(t)&lt;\\infty\\right\\}\\) is a one-to-one function:</p> \\[ \\begin{aligned} D_{f}(p \\| q) &amp; =\\sup _{T \\in \\mathcal{T}}\\left\\{\\mathbb{E}_{X \\sim p}[T(X)]-\\mathbb{E}_{\\tilde{X} \\sim q}\\left[f^{*}(T(\\tilde{X}))\\right]\\right\\} \\\\ &amp; \\stackrel{(*)}{=} \\sup _{\\substack{T \\in \\mathcal{T}}}\\left\\{\\mathbb{E}_{X \\sim p}[T(X)]-\\mathbb{E}_{\\tilde{X} \\sim q}\\left[f^{*}(T(\\tilde{X}))\\right]\\right\\} \\\\ &amp; \\stackrel{(* *)}{=} \\sup _{\\tilde{T} \\in \\mathcal{T}}\\left\\{\\mathbb{E}_{X \\sim p}[\\rho(\\tilde{T}(X))]-\\mathbb{E}_{\\tilde{X} \\sim q}\\left[f^{*}(\\rho(\\tilde{T}(\\tilde{X})))\\right]\\right\\} \\\\ &amp; \\geq \\sup _{\\phi \\in \\Phi}\\left\\{\\mathbb{E}_{X \\sim p}\\left[\\rho\\left(D_{\\phi}(X)\\right)\\right]-\\mathbb{E}_{\\tilde{X} \\sim q}\\left[f^{*}\\left(\\rho\\left(D_{\\phi}(\\tilde{X})\\right)\\right)\\right]\\right\\} \\end{aligned} \\] <p>(\\(*\\)) We can restrict the search over \\(T\\) since if \\(f^{*}(T(x))=\\infty\\), then the objective becomes \\(-\\infty\\).</p> <p>(\\(**\\)) With \\(T=\\rho \\circ \\tilde{T}\\), have \\(\\left[T \\in \\mathcal{T}\\right.\\) and \\(\\left.f^{*}(T(x))&lt;\\infty\\right] \\Leftrightarrow[\\tilde{T} \\in \\mathcal{T}]\\) since \\(\\rho\\) is one-to-one.</p> <p>Definition 21.25 : f-GAN with Output Activation</p> <p>Formulate f-GAN with output activation function \\(\\rho\\) (\\(\\rho: \\mathbb{R} \\rightarrow\\left\\{t \\mid f^{*}(t)&lt;\\infty\\right\\}\\) is a one-to-one function) :</p> \\[ \\begin{gathered} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad D_{f}\\left(p_{\\text {true }} \\| p_{\\theta}\\right) \\\\ \\approx \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\quad \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\rho\\left(D_{\\phi}(X)\\right)\\right]-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[f^{*}\\left(\\rho\\left(D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right)\\right)\\right] \\end{gathered} \\] <p>Concept 21.26 : f-GAN with Squared Hellinger</p> <p>Instantiate f-GAN with squared Hellinger distance using \\(\\rho(r)=1-e^{-r}\\) and</p> \\[ f^{*}(t)= \\begin{cases}\\frac{1}{1 / t-1} &amp; \\text { if } t&lt;1 \\\\ \\infty &amp; \\text { otherwise }\\end{cases} \\] <p>Note that \\(f^{*}(\\rho(r))=-1+e^{r}\\).</p> \\[ \\begin{gathered} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad D_{\\mathrm{SH}}\\left(p_{\\text {true }}, p_{\\theta}\\right) \\\\ \\approx \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} 2-\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[e^{-D_{\\phi}(X)}\\right]-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[e^{D_{\\phi}\\left(G_{\\theta}(Z)\\right)}\\right] \\\\ = \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} -\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[e^{-D_{\\phi}(X)}\\right]-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[e^{D_{\\phi}\\left(G_{\\theta}(Z)\\right)}\\right] \\end{gathered} \\] <p>Concept 21.27 : f-GAN with Reversed KL</p> <p>Instantiate f-GAN with reverse KL using \\(\\rho(r)=-e^{r}\\) and</p> \\[ f^{*}(t)= \\begin{cases}-1-\\log (-t) &amp; \\text { if } t&lt;0 \\\\ \\infty &amp; \\text { otherwise }\\end{cases} \\] <p>Note that \\(f^{*}(\\rho(r))=-1-r\\).</p> \\[ \\begin{gathered} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad D_{\\mathrm{KL}}\\left(p_{\\theta} \\| p_{\\text {true }}\\right) \\\\ \\approx \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} 1-\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[e^{D_{\\phi}(X)}\\right]+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right] \\\\ =\\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}}-\\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[e^{D_{\\phi}(X)}\\right]+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right] \\end{gathered} \\] <p>Concept 21.28 : f-GAN with JS (Recovering Standard GAN)</p> <p>We recover standard GAN with</p> \\[ \\rho(r)=\\log (\\sigma(r)), \\quad \\sigma(r)=\\frac{1}{1+e^{-r}}, \\quad f^{*}(t)= \\begin{cases}-\\log (1-\\exp (t))-\\log 4 &amp; \\text { for } t&lt;0 \\\\ \\infty &amp; \\text { otherwise }\\end{cases} \\] <p>Note that \\(\\sigma\\) is the familiar sigmoid and</p> \\[ f^{*}(\\rho(r))=-\\log (1-\\sigma(r))-\\log 4 \\] \\[ \\begin{gathered} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} D_{\\mathrm{JS}}\\left(p_{\\text {true }}, p_{\\theta}\\right) \\\\ \\approx \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathbb{E}_{X \\sim p_{\\text {true }}}\\left[\\log \\sigma\\left(D_{\\phi}(X)\\right)\\right]+\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[\\log \\left(1-\\sigma\\left(D_{\\phi}\\left(G_{\\theta}(X)\\right)\\right)\\right)\\right] \\end{gathered} \\] <p>where \\(D_{\\phi}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}\\).</p> <p>(Standard GAN has \\(D_{\\phi}: \\mathbb{R}^{n} \\rightarrow(0,1)\\). Here, \\(\\left(\\sigma \\circ D_{\\phi}\\right): \\mathbb{R}^{n} \\rightarrow(0,1)\\) serves the same purpose.)</p> <p>Definition 21.29 : WGAN</p> <p>The Wasserstein GAN (WGAN) minimizes the Wasserstein distance:</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad W\\left(p_{\\text {true }}, p_{\\theta}\\right) \\] <p>The \\(W(p, q)\\) is a distance (metric) on probability distributions defined as</p> \\[ W(p, q)=\\inf _{f} \\mathbb{E}_{(X, Y) \\sim f(x, y)}\\|X-Y\\| \\] <p>where the infimum is taken over joint probability distributions \\(f\\) with marginals \\(p\\) and \\(q\\), i.e.,</p> \\[ p(x)=\\int f(x, y) d y, \\quad q(y)=\\int f(x, y) d x \\] <p>(M. Arjovsky, S. Chintala, and L. Bottou, Wasserstein GAN, ICML, 2017.)</p> <p>Another equivalent formulation of the Wasserstein distance is by the theory of optimal transport. Given distributions \\(p\\) and \\(q\\) (initial and target)</p> \\[ W(p, q)=\\inf _{T} \\int\\|x-T(x)\\| p(x) d x \\] <p>where \\(T\\) is a transport plan that transports \\(p\\) to \\(q\\). Figuratively speaking, we are transporting grains of sand from one pile to another, and we wan to minimize the aggregate transport distance.</p> <p>(Image from W. Li, E. K. Ryu, S. Osher, W. Yin, and W. Gangbo, A parallel method for earth mover\u2019s distance, J. Sci. Comput., 2018.)</p> <p>Kantorovich-Rubinstein duality \\(^{\\#}\\) establishes:</p> \\[ W\\left(p_{\\text {true }}, p_{\\theta}\\right)=\\sup _{\\|T\\|_{L \\leq 1}} \\mathbb{E}_{X \\sim p_{\\text {true }}}[T(X)]-\\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}[T(\\tilde{X})] \\] <p>Minimax formulation of WGAN:</p> \\[ \\begin{gathered} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} W\\left(p_{\\text {true }}, p_{\\theta}\\right) \\\\ \\approx \\quad \\begin{aligned} &amp; \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} \\mathbb{E}_{X \\sim p_{\\text {true }}}[D_{\\phi}(X)]-\\mathbb{E}_{\\tilde{X} \\sim p_{\\theta}}[D_{\\phi}(\\tilde{X})] \\\\ &amp; \\text { subject to } D_{\\phi} \\text{ is } 1 \\text{-Lipschitz} \\end{aligned} \\end{gathered} \\] <p>(\\(^{\\#}\\) L.V. Kantorovich and G. Rubinstein, On a space of completely additive functions, Vestnik Leningradskogo Universiteta, 1958. The Kantorovich\u2013Rubinstein dual as the convex (Lagrange) dual of a \u201cflux\u201d formulation of the optimal transport.)</p> <p>Concept 21.30 : Spectral Normalization</p> <p>How do we enforce the constraint that \\(D_{\\phi}\\) is 1-Lipschitz? Consider an MLP:</p> \\[ \\begin{aligned} y_{L}= &amp; A_{L} y_{L-1}+b_{L} \\\\ y_{L-1}= &amp; \\sigma\\left(A_{L-1} y_{L-2}+b_{L-1}\\right) \\\\ &amp; \\vdots \\\\ y_{2}= &amp; \\sigma\\left(A_{2} y_{1}+b_{2}\\right) \\\\ y_{1}= &amp; \\sigma\\left(A_{1} x+b_{1}\\right), \\end{aligned} \\] <p>where \\(\\sigma\\) is a 1-Lipschitz continuous activation function, such as ReLU and tanh. If</p> \\[ \\left\\|A_{i}\\right\\|_{\\mathrm{op}}=\\sigma_{\\max }\\left(A_{i}\\right) \\leq 1 \\] <p>for \\(i=1, \\ldots, L\\), where \\(\\sigma_{\\text {max }}\\) denotes the largest singular value, then each layer is 1-Lipschitz continuous and the entire mapping \\(x \\mapsto y_{L}\\) is 1 -Lipschitz. (A sufficient, but not a necessary, condition.)</p> <p>Replace Lipschitz constraint with a singular-value constraint</p> \\[ \\begin{array}{rll} \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} &amp; \\underset{\\phi \\in \\Phi}{\\operatorname{maximize}} &amp; \\frac{1}{N} \\sum_{i=1}^{N} D_{\\phi}\\left(X_{i}\\right)-\\mathbb{E}_{Z \\sim \\mathcal{N}(0, I)}\\left[D_{\\phi}\\left(G_{\\theta}(Z)\\right)\\right] \\\\ &amp; \\text { subject to } &amp; \\sigma_{\\max }\\left(A_{i}\\right) \\leq 1, \\quad i=1, \\ldots, L \\end{array} \\] <p>Constraint is handled with a projected gradient method. (Note that \\(A_{1}, \\ldots, A_{L}\\) are part of the discriminator parameters \\(\\phi\\).)</p> <p>(Specifically, one performs an (approximate) projection after the ascent step in the stochastic gradient ascent-descent methods. The approximate projection involves computing the largest singular with the power iteration.)</p> <p>(T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida, Spectral normalization for generative adversarial networks, ICLR, 2018.)</p>"},{"location":"books-and-courses/mfdnn/format/13/","title":"\u00a7 13. Basics of Monte Carlo","text":""},{"location":"books-and-courses/mfdnn/format/13/#monte-carlo-estimation","title":"Monte Carlo Estimation","text":"<p>Definition 13.1 : Monte Carlo Estimation</p> <p>Consider IID data \\(X_{1}, \\ldots, X_{N} \\sim f\\). Let \\(\\phi(X) \\geq 0\\) be some function. (The assumption \\(\\phi(X) \\geq 0\\) can be relaxed.) Consider the problem of estimating</p> \\[ I=\\mathbb{E}_{X \\sim f}[\\phi(X)]=\\int \\phi(x) f(x) d x \\] <p>One commonly uses</p> \\[ \\hat{I}_{N}=\\frac{1}{N} \\sum_{i=1}^{N} \\phi\\left(X_{i}\\right) \\] <p>to estimate \\(I\\), which is called monte carlo estimation. After all, \\(\\mathbb{E}\\left[\\hat{I}_{N}\\right]=I\\) and \\(\\hat{I}_{N} \\rightarrow I\\) by the law of large numbers. (Convergence in probability by weak law of large numbers and almost sure convergence by strong law of large numbers.)</p> <p>Concept 13.2 : Evidence of Convergence for Monte Carlo Estimation</p> <p>We can quantify convergence with variance:</p> \\[ \\operatorname{Var}_{X \\sim f}\\left(\\hat{I}_{N}\\right)=\\sum_{i=1}^{N} \\operatorname{Var}_{X_{i} \\sim f}\\left(\\frac{\\phi\\left(X_{i}\\right)}{N}\\right)=\\frac{1}{N} \\operatorname{Var}_{X \\sim f}(\\phi(X)) \\] <p>In other words</p> \\[ \\mathbb{E}\\left[\\left(\\hat{I}_{N}-I\\right)^{2}\\right]=\\frac{1}{N} \\operatorname{Var}_{X \\sim f}(\\phi(X)) \\] <p>and</p> \\[ \\mathbb{E}\\left[\\left(\\hat{I}_{N}-I\\right)^{2}\\right] \\rightarrow 0 \\] <p>as \\(N \\rightarrow \\infty\\). So, \\(\\hat{I}_{N} \\rightarrow I\\) in \\(L^2\\) provided that \\(\\operatorname{Var}_{X \\sim f}(\\phi(X)) &lt; \\infty\\).</p> <p>Definition 13.3 : Empirical Risk Minimization (ERM)</p> <p>In machine learning and statistics, we often wish to solve</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad \\mathcal{L}(\\theta) \\] <p>where the objective function</p> \\[ \\mathcal{L}(\\theta)=\\mathbb{E}_{X \\sim p_{X}}\\left[\\ell\\left(f_{\\theta}(X), f_{\\star}(X)\\right)\\right] \\] <p>Is the (true) risk. However, the evaluation of \\(\\mathbb{E}_{X \\sim p_{X}}\\) is impossible (if \\(p_{X}\\) is unknown) or intractable (if \\(p_{X}\\) is known but the expectation has no closed-form solution). Therefore, we define the proxy loss function</p> \\[ \\mathcal{L}_{N}(\\theta)=\\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(f_{\\theta}\\left(X_{i}\\right), f_{\\star}\\left(X_{i}\\right)\\right) \\] <p>which we call the empirical risk, and solve</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\quad \\mathcal{L}_{N}(\\theta) \\] <p>This is called empirical risk minimization (ERM). The idea is that</p> \\[ \\mathcal{L}_{N}(\\theta) \\approx \\mathcal{L}(\\theta) \\] <p>with high probability, so minimizing \\(\\mathcal{L}_{N}(\\theta)\\) should be similar to minimizing \\(\\mathcal{L}(\\theta)\\).</p> <p>Concept 13.4 : Evidence of Convergence for Empirical Risk Minimization</p> <p>Technical note) The law of large numbers tells us that</p> \\[ \\mathbb{P}\\left(\\left|\\mathcal{L}_{N}(\\theta)-\\mathcal{L}(\\theta)\\right|&gt;\\varepsilon\\right)=\\text { small } \\] <p>for any given \\(\\theta\\), but we need</p> \\[ \\mathbb{P}\\left(\\sup _{\\theta \\in \\Theta}\\left|\\mathcal{L}_{N}(\\theta)-\\mathcal{L}(\\theta)\\right|&gt;\\varepsilon\\right)=\\text { small } \\] <p>for all compact \\(\\Theta\\) in order to conclude that the argmins of the two losses to be similar. These types of results are established by a uniform law of large numbers.</p>"},{"location":"books-and-courses/mfdnn/format/13/#importance-sampling","title":"Importance Sampling","text":"<p>Definition 13.5 : Importance Sampling (IS)</p> <p>Importance sampling (IS) is a technique for reducing the variance of a Monte Carlo estimator.</p> <p>Key insight of important sampling:</p> \\[ I=\\mathbb{E}_{X \\sim f}[\\phi(X)]=\\int \\phi(x) f(x) d x=\\int \\frac{\\phi(x) f(x)}{g(x)} g(x) d x=\\mathbb{E}_{X \\sim g}\\left[\\frac{\\phi(X) f(X)}{g(X)}\\right] \\] <p>(We do have to be mindful of division by 0.) Then</p> \\[ \\hat{I}_{N}=\\frac{1}{N} \\sum_{i=1}^{N} \\phi\\left(X_{i}\\right) \\frac{f\\left(X_{i}\\right)}{g\\left(X_{i}\\right)} \\] <p>with \\(X_{1}, \\ldots, X_{N} \\sim g\\) is also an estimator of \\(I\\). Indeed, \\(\\mathbb{E}\\left[\\hat{I}_{N}\\right]=I\\) and \\(\\hat{I}_{N} \\rightarrow I\\). The weight \\(\\frac{f(x)}{g(x)}\\) is called the likelihood ratio or the Radon-Nikodym derivative.</p> <p>So we can use samples from \\(g\\) to compute expectation with respect to \\(f\\).</p> <p>Example 13.6 : IS Example</p> <p>Consider the setup of estimating the probability</p> \\[ \\mathbb{P}(X&gt;3)=0.00135 \\] <p>where \\(X \\sim \\mathcal{N}(0,1)\\). If we use the regular Monte Carlo estimator</p> \\[ \\hat{I}_{N}=\\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}_{\\left\\{X_{i}&gt;3\\right\\}} \\] <p>where \\(X_{i} \\sim \\mathcal{N}(0,1)\\), if \\(N\\) is not sufficiently large, we can have \\(\\hat{I}_{N}=0\\). Inaccurate estimate.</p> <p>If we use the IS estimator</p> \\[ \\hat{I}_{N}=\\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}_{\\left\\{Y_{i}&gt;3\\right\\}} \\exp \\left(\\frac{\\left(Y_{i}-3\\right)^{2}-Y_{i}^{2}}{2}\\right) \\] <p>where \\(Y_{i} \\sim \\mathcal{N}(3,1)\\), having \\(\\hat{I}_{N}=0\\) is much less likely. Estimate is much more accurate.</p> <p>Concept 13.7 : Optimal Sampling Distribution</p> <p>Benefit of IS quantified by with variance:</p> \\[ \\operatorname{Var}_{X \\sim g}\\left(\\hat{I}_{N}\\right)=\\sum_{i=1}^{N} \\operatorname{Var}_{X \\sim g}\\left(\\frac{\\phi\\left(X_{i}\\right) f\\left(X_{i}\\right)}{n g\\left(X_{i}\\right)}\\right)=\\frac{1}{N} \\operatorname{Var}_{X \\sim g}\\left(\\frac{\\phi(X) f(X)}{g(X)}\\right) \\] <p>If \\(\\operatorname{Var}_{X \\sim g}\\left(\\frac{\\phi(X) f(X)}{g(X)}\\right)&lt;\\operatorname{Var}_{X \\sim f}(\\phi(X))\\), then IS provides variance reduction.</p> <p>We call \\(g\\) the importance or sampling distribution. Choosing \\(g\\) poorly can increase the variance. What is the best choice of \\(g\\) ?</p> <p>The sampling distribution</p> \\[ g(x)=\\frac{\\phi(x) f(x)}{I} \\] <p>makes \\(\\operatorname{Var}_{X \\sim g}\\left(\\frac{\\phi(X) f(X)}{g(X)}\\right)=\\operatorname{Var}_{X \\sim g}(I)=0\\) and therefore is optimal. (\\(I\\) serves as the normalizing factor that ensures the density \\(g\\) integrates to 1.)</p> <p>Problem: Since we do not know the normalizing factor \\(I\\), the answer we wish to estimate, sampling from \\(g\\) is usually difficult.</p> <p>Concept 13.8 : Optimized / Trained Sampling Distribution</p> <p>Instead, we consider the optimization problem</p> \\[ \\underset{g \\in \\mathcal{G}}{\\operatorname{minimize}} \\quad D_{\\mathrm{KL}}\\left(g \\| \\frac{\\phi f}{I}\\right) \\] <p>and compute a suboptimal, but good, sampling distribution within a class of sampling distributions \\(\\mathcal{G}\\). (In ML, \\(\\mathcal{G}=\\left\\{g_{\\theta} \\mid \\theta \\in \\Theta\\right\\}\\) is parameterized by neural networks.)</p> <p>Importantly, this optimization problem does not require knowledge of \\(I\\).</p> \\[ \\begin{aligned} D_{\\mathrm{KL}}\\left(g_{\\theta} \\| \\phi f / I\\right) &amp; =\\mathbb{E}_{X \\sim g_{\\theta}}\\left[\\log \\left(\\frac{I g_{\\theta}(X)}{\\phi(X) f(X)}\\right)\\right] \\\\ &amp; =\\mathbb{E}_{X \\sim g_{\\theta}}\\left[\\log \\left(\\frac{g_{\\theta}(X)}{\\phi(X) f(X)}\\right)\\right]+\\log I \\\\ &amp; =\\mathbb{E}_{X \\sim g_{\\theta}}\\left[\\log \\left(\\frac{g_{\\theta}(X)}{\\phi(X) f(X)}\\right)\\right]+\\text { constant independent of } \\theta \\end{aligned} \\] <p>How do we compute stochastic gradients?</p>"},{"location":"books-and-courses/mfdnn/format/13/#log-derivative-trick","title":"Log-Derivative Trick","text":"<p>Definition 13.9 : Log-Derivative Trick</p> <p>Generally, consider the setup where we wish to solve</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\mathbb{E}_{X \\sim f_{\\theta}}[\\phi(X)] \\] <p>with SGD. (Previous situation (Concept 13.8) had \\(\\theta\\)-dependence both on and inside the expectation. For now, let's simplify the problem so that \\(\\phi\\) does not depend on \\(\\theta\\).)</p> <p>Incorrect gradient computation:</p> \\[ \\nabla_{\\theta} \\mathbb{E}_{X \\sim f_{\\theta}}[\\phi(X)] \\stackrel{?}{=} \\mathbb{E}_{X \\sim f_{\\theta}}\\left[\\nabla_{\\theta} \\phi(X)\\right]=\\mathbb{E}_{X \\sim f_{\\theta}}[0]=0 \\] <p>Correct gradient computation:</p> \\[ \\begin{aligned} \\nabla_{\\theta} \\mathbb{E}_{X \\sim f_{\\theta}}[\\phi(X)] &amp; =\\nabla_{\\theta} \\int \\phi(x) f_{\\theta}(x) d x=\\int \\phi(x) \\nabla_{\\theta} f_{\\theta}(x) d x \\\\ &amp; =\\int \\phi(x) \\frac{\\nabla_{\\theta} f_{\\theta}(x)}{f_{\\theta}(x)} f_{\\theta}(x) d x=\\mathbb{E}_{X \\sim f_{\\theta}}\\left[\\phi(X) \\frac{\\nabla_{\\theta} f_{\\theta}(X)}{f_{\\theta}(X)}\\right] \\\\ &amp; =\\mathbb{E}_{X \\sim f_{\\theta}}\\left[\\phi(X) \\nabla_{\\theta} \\log \\left(f_{\\theta}(X)\\right)\\right] \\end{aligned} \\] <p>Therefore, \\(\\phi(X) \\nabla_{\\theta} \\log \\left(f_{\\theta}(X)\\right)\\) with \\(X \\sim f_{\\theta}\\) is a stochastic gradient of the loss function. This technique is called the log-derivative trick, the likelihood ratio gradient\\(^{\\#}\\), or REINFORCE\\(^{\\star}\\).</p> <p>Formula with the log-derivative \\(\\left(\\nabla_{\\theta} \\log (\\cdot)\\right)\\) is convenient when dealing with Gaussians, or more generally exponential families, since the densities are of the form</p> \\[ f_{\\theta}(x)=h(x) \\exp (\\text {function of } \\theta) \\] <p>(\\({}^{\\#}\\)P. W. Glynn, Likelihood ratio gradient estimation for stochastic systems, Communications of the ACM, 1990. \\({}^{\\star}\\)R. J. Williams, Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 1992.)</p> <p>Example 13.10 : Log-Derivative Trick Example</p> <p>Learn \\(\\mu \\in \\mathbb{R}^{2}\\) to minimize the objective below.</p> \\[ \\underset{\\mu \\in \\mathbb{R}^{2}}{\\operatorname{minimize}} \\mathbb{E}_{X \\sim \\mathcal{N}(\\mu, I)}\\left\\|X-\\binom{5}{5}\\right\\|^{2} \\] <p>Then the loss function is</p> \\[ \\mathcal{L}(\\mu)=\\mathbb{E}_{X \\sim \\mathcal{N}(\\mu, I)}\\left\\|X-\\binom{5}{5}\\right\\|^{2}=\\int\\left\\|x-\\binom{5}{5}\\right\\|^{2} \\frac{1}{2 \\pi} \\exp \\left(-\\frac{1}{2}\\|x-\\mu\\|^{2}\\right) d x \\] <p>And, using \\(X_{1}, \\ldots, X_{B} \\sim \\mathcal{N}(\\mu, I)\\), we have stochastic gradients</p> \\[ \\nabla_{\\mu} \\mathcal{L}(\\mu)=\\mathbb{E}_{X \\sim \\mathcal{N}(\\mu, I)}\\left[\\left\\|x-\\binom{5}{5}\\right\\|^{2} \\nabla_{\\mu}\\left(-\\frac{1}{2}\\|x-\\mu\\|^{2}\\right)\\right] \\approx \\frac{1}{B} \\sum_{i=1}^{B}\\left\\|X_{i}-\\binom{5}{5}\\right\\|^{2}\\left(X_{i}-\\mu\\right) \\] <p>These stochastic gradients have large variance and thus SGD is slow.</p> <p> </p>"},{"location":"books-and-courses/mfdnn/format/13/#reparameterization-trick","title":"Reparameterization Trick","text":"<p>Definition 13.11 : Reparameterization Trick</p> <p>The reparameterization trick (RT) or the pathwise derivative (PD) relies on the key insight.</p> \\[ \\mathbb{E}_{X \\sim \\mathcal{N}\\left(\\mu, \\sigma^{2}\\right)}[\\phi(X)]=\\mathbb{E}_{Y \\sim \\mathcal{N}(0,1)}[\\phi(\\mu+\\sigma Y)] \\] <p>Gradient computation:</p> \\[ \\begin{aligned} \\nabla_{\\mu, \\sigma} \\mathbb{E}_{X \\sim \\mathcal{N}\\left(\\mu, \\sigma^{2}\\right)}[\\phi(X)] &amp; =\\mathbb{E}_{Y \\sim \\mathcal{N}(0,1)}\\left[\\nabla_{\\mu, \\sigma} \\phi(\\mu+\\sigma Y)\\right]=\\mathbb{E}_{Y \\sim \\mathcal{N}(0,1)}\\left[\\phi^{\\prime}(\\mu+\\sigma Y)\\left[\\begin{array}{c} 1 \\\\ Y \\end{array}\\right]\\right] \\\\ &amp; \\approx \\frac{1}{B} \\sum_{i=1}^{B} \\phi^{\\prime}\\left(\\mu+\\sigma Y_{i}\\right)\\left[\\begin{array}{c} 1 \\\\ Y_{i} \\end{array}\\right], \\quad Y_{1}, \\ldots, Y_{B} \\sim \\mathcal{N}(0, I) \\end{aligned} \\] <p>RT is less general than log-derivative trick, but it usually produces stochastic gradients with lower variance.</p> <p>Example 13.12 : Reparameterization Trick Example</p> <p>Consider the same example as before</p> \\[ \\mathcal{L}(\\mu)=\\mathbb{E}_{X \\sim \\mathcal{N}(\\mu, I)}\\left\\|X-\\binom{5}{5}\\right\\|^{2}=\\mathbb{E}_{Y \\sim \\mathcal{N}(0, I)}\\left\\|Y+\\mu-\\binom{5}{5}\\right\\|^{2} \\] <p>Gradient computation:</p> \\[ \\begin{aligned} \\nabla_{\\mu} \\mathcal{L}(\\mu) &amp; =\\mathbb{E}_{Y \\sim \\mathcal{N}(0, I)} \\nabla_{\\mu}\\left\\|Y+\\mu-\\binom{5}{5}\\right\\|^{2}=2 \\mathbb{E}_{Y \\sim \\mathcal{N}(0, I)}\\left(Y+\\mu-\\binom{5}{5}\\right) \\\\ &amp; \\approx \\frac{2}{B} \\sum_{i=1}^{B}\\left(Y_{i}+\\mu-\\binom{5}{5}\\right), \\quad Y_{1}, \\ldots, Y_{B} \\sim \\mathcal{N}(0, I) \\end{aligned} \\] <p>These stochastic gradients have smaller variance and thus SGD is faster.</p> <p>Example 13.13 : Log Derivative Trick vs Reparameterization Trick</p> <p>The image below is the result of SGD with the computed gradients by Example 13.10 and Example 13.12.</p> <p> </p>"},{"location":"books-and-courses/mfdnn/format/2/","title":"\u00a7 2. Gradient Descent","text":""},{"location":"books-and-courses/mfdnn/format/2/#gradient-descent","title":"Gradient Descent","text":"<p>Definition 2.1 : Gradient Descent</p> <p>Consider the unconstrained optimization problem</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{P}}{\\operatorname{minimize}} f(\\theta) \\] <p>where \\(f\\) is differentiable.</p> <p>Gradient Descent (GD) algorithm:</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\nabla f\\left(\\theta^{k}\\right) \\quad \\text { for } k=0,1, \\ldots, \\] <p>where \\(\\theta^{0} \\in \\mathbb{R}^{p}\\) is the initial point and \\(\\alpha_{k}&gt;0\\) is the learning rate or the stepsize.</p> <p>The terminology learning rate is common in the machine learning literature while stepsize is more common in the optimization literature.</p> <p>In math, a function is \"differentiable\" if its derivative exists everywhere.</p> <p>In deep learning (DL), a function is often said to be differentiable if its derivative exists almost everywhere and the function is nice. ReLU activation functions are said to be differentiable.</p> <p>Concept 2.2 : Efficiency of gradient descent can be expected using the first-order Taylor expansion of \\(f\\).</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\nabla f\\left(\\theta^{k}\\right) \\] <p>Taylor expansion of \\(f\\) about \\(\\theta^{k}\\) :</p> \\[ f(\\theta)=f\\left(\\theta^{k}\\right)+\\nabla f\\left(\\theta^{k}\\right)^{\\top}\\left(\\theta-\\theta^{k}\\right)+\\mathcal{O}\\left(\\left\\|\\theta-\\theta^{k}\\right\\|^{2}\\right) \\] <p>Plug in \\(\\theta^{k+1}\\) :</p> \\[ f\\left(\\theta^{k+1}\\right)=f\\left(\\theta^{k}\\right)-\\alpha_{k}\\left\\|\\nabla f\\left(\\theta^{k}\\right)\\right\\|^{2}+\\mathcal{O}\\left(\\alpha_{k}^{2}\\right) \\] <p>\\(-\\nabla f\\left(\\theta^{k}\\right)\\) is steepest descent direction. For small (cautious) \\(\\alpha_{k}\\), GD step reduces function value.</p> <p>However, note that a step of GD need not result in descent, i.e., \\(f\\left(\\theta^{k+1}\\right)&gt;f\\left(\\theta^{k}\\right)\\) is possible.</p>  ![](.././assets/2.1.png){: width=\"40%\"}  <p>We need an assumption that ensures the first-order Taylor expansion is a good approximation within a sufficiently large neighborhood.</p>"},{"location":"books-and-courses/mfdnn/format/2/#convergence-of-gradient-descent","title":"Convergence of Gradient Descent","text":"<p>Without further assumptions, there is no hope of finding the global minimum.</p> <p>We cannot prove the function value converges to global optimum. We instead prove \\(\\nabla f\\left(\\theta^{k}\\right) \\rightarrow 0\\). Roughly speaking, this is similar, but weaker than proving that \\(\\theta^{k}\\) converges to a local minimum.</p>  ![](.././assets/2.2.png){: width=\"70%\"}  <p>Without further assumptions, we cannot show that \\(\\theta^{k}\\) converges to a limit, and even \\(\\theta^{k}\\) does converge to a limit, we cannot guarantee that that limit is not a saddle point or even a local maximum. Nevertheless, people commonly use the argument that \\(\\theta^{k}\\) usually converges and that it is unlikely that the limit is a local maximum or a saddle point.</p> <p>Definition 2.3 : \\(L\\)-Lipschitz</p> <p>We say \\(\\nabla f: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}^{p}\\) is \\(L\\)-Lipschitz if</p> \\[ \\|\\nabla f(x)-\\nabla f(y)\\| \\leq L\\|x-y\\| \\quad \\forall x, y \\in \\mathbb{R}^{p} . \\] <p>Roughly, this means \\(\\nabla f\\) does not change rapidly. As a consequence, we can trust the first-order Taylor expansion on a non-infinitesimal neighborhood.</p> <p>Theorem 2.4 : Lipschitz Gradient Lemma</p> <p>Let \\(f: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}\\) be differentiable and \\(\\nabla f: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}^{p}\\) be L-Lipschitz. Then</p> \\[ f(\\theta+\\delta) \\leq f(\\theta)+\\nabla f(\\theta)^{\\top} \\delta+\\frac{L}{2}\\|\\delta\\|^{2} \\quad \\forall \\theta, \\delta \\in \\mathbb{R}^{p} \\] <p>\\(f(\\theta)+\\nabla f(\\theta)^{\\top} \\delta-\\frac{L}{2}\\|\\delta\\|^{2} \\leq f(\\theta+\\delta)\\) is also true, but we do not need this other direction. Together the inequalities imply</p> \\[ \\left|f(\\theta+\\delta)-\\left(f(\\theta)+\\nabla f(\\theta)^{\\top} \\delta\\right)\\right| \\leq \\frac{L}{2}\\|\\delta\\|^{2} \\quad \\forall \\theta, \\delta \\in \\mathbb{R}^{p} \\] <p>Proof</p> <p>Define \\(g: \\mathbb{R} \\rightarrow \\mathbb{R}\\) as \\(g(t)=f(\\theta+t \\delta)\\). Then \\(g\\) is differentiable and</p> \\[ g^{\\prime}(t)=\\nabla f(\\theta+t \\delta)^{\\top} \\delta \\] <p>Note \\(g^{\\prime}\\) is \\(\\left(L\\|\\delta\\|^{2}\\right)\\)-Lipschitz continuous since</p> \\[ \\begin{gathered} \\left|g^{\\prime}\\left(t_{1}\\right)-g^{\\prime}\\left(t_{0}\\right)\\right|=\\left|\\left(\\nabla f\\left(\\theta+t_{1} \\delta\\right)-\\nabla f\\left(\\theta+t_{0} \\delta\\right)\\right)^{\\top} \\delta\\right| \\\\ \\leq\\left\\|\\nabla f\\left(\\theta+t_{1} \\delta\\right)-\\nabla f\\left(\\theta+t_{0} \\delta\\right)\\right\\|\\| \\| \\delta \\| \\\\ \\leq L\\left\\|t_{1} \\delta-t_{0} \\delta\\right\\|\\|\\delta\\| \\\\ =L\\|\\delta\\|^{2}\\left|t_{1}-t_{0}\\right| \\end{gathered} \\] <p>Finally, we conclude with</p> \\[ \\begin{gathered} f(\\theta+\\delta)=g(1)=g(0)+\\int_{0}^{1} g^{\\prime}(t) \\mathrm{d} t \\\\ \\leq f(\\theta)+\\int_{0}^{1}\\left(g^{\\prime}(0)+L\\|\\delta\\|^{2} t\\right) \\mathrm{d} t \\\\ =f(\\theta)+\\nabla f(\\theta)^{\\top} \\delta+\\frac{L}{2}\\|\\delta\\|^{2} \\end{gathered} \\] <p>Theorem 2.5 : Summability Lemma</p> <p>Let \\(V^{0}, V^{1}, \\ldots \\in \\mathbb{R}\\) and \\(S^{0}, S^{1}, \\ldots \\in \\mathbb{R}\\) be nonnegative sequences satisfying</p> \\[ V^{k+1} \\leq V^{k}-S^{k} \\] <p>for \\(k=0,1,2, \\ldots\\) Then \\(S^{k} \\rightarrow 0\\).</p> <p>Proof</p> <p>Key idea. \\(S^{k}\\) measures progress (decrease) made in iteration \\(k\\). Since \\(V^{k} \\geq 0, V^{k}\\) cannot decrease forever, so the progress (magnitude of \\(S^{k}\\) ) must diminish to 0.</p> <p>Sum the inequality from \\(i=0\\) to \\(k\\)</p> \\[ V^{k+1}+\\sum_{i=0}^{k} S^{i} \\leq V^{0} \\] <p>Let \\(k \\rightarrow \\infty\\)</p> \\[ \\sum_{i=0}^{\\infty} S^{i} \\leq V^{0}-\\lim _{k \\rightarrow \\infty} V^{k} \\leq V^{0} \\] <p>Since \\(\\sum_{i=0}^{\\infty} S^{i}&lt;\\infty, S^{i} \\rightarrow 0\\).</p> <p>Theorem 2.6 : Convergence of GD</p> <p>Assume \\(f: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}\\) is differentiable, \\(\\nabla f\\) is \\(L\\)-Lipschitz continuous, and \\(\\inf _{\\theta \\in \\mathbb{R}^{p}} f(\\theta)&gt;-\\infty\\). Then</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha \\nabla f\\left(\\theta^{k}\\right) \\] <p>with \\(\\alpha \\in\\left(0, \\frac{2}{L}\\right)\\) satisfies \\(\\nabla f\\left(\\theta^{k}\\right) \\rightarrow 0\\).</p> <p>Proof</p> <p>Use Lipschitz gradient lemma with \\(\\theta=\\theta^{k}\\) and \\(\\delta=-\\alpha \\nabla f\\left(\\theta^{k}\\right)\\) to get</p> \\[ f\\left(\\theta^{k+1}\\right) \\leq f\\left(\\theta^{k}\\right)-\\alpha\\left(1-\\frac{\\alpha L}{2}\\right)\\left\\|\\nabla f\\left(\\theta^{k}\\right)\\right\\|^{2} \\] <p>and</p> \\[ \\left(f\\left(\\theta^{k+1}\\right)-\\inf _{\\theta} f(\\theta)\\right) \\leq\\left(f\\left(\\theta^{k}\\right)-\\inf _{\\theta} f(\\theta)\\right)-\\alpha\\left(1-\\frac{\\alpha L}{2}\\right)\\left\\|\\nabla f\\left(\\theta^{k}\\right)\\right\\|^{2} \\\\ \\] \\[ \\left(f\\left(\\theta^{k+1}\\right)-\\inf _{\\theta} f(\\theta)\\right) \\ge 0,  \\left(f\\left(\\theta^{k}\\right)-\\inf _{\\theta} f(\\theta)\\right) \\ge 0,  \\alpha\\left(1-\\frac{\\alpha L}{2}\\right)\\left\\|\\nabla f\\left(\\theta^{k}\\right)\\right\\|^{2} &gt; 0 \\text{ for } \\alpha \\in\\left(0, \\frac{2}{L}\\right) \\] <p>By the summability lemma, \\(\\left\\|\\nabla f\\left(\\theta^{k}\\right)\\right\\|^{2} \\rightarrow 0\\) and thus \\(\\nabla f\\left(\\theta^{k}\\right) \\rightarrow 0\\).</p> <p>In deep learning, the condition that \\(\\nabla f\\) is \\(L\\)-Lipschitz is usually not true (due to the use of ReLU activation functions).</p> <p>Rather, the purpose of these mathematical analyses is to obtain qualitative insights; this convergence proof are meant to provide you with intuition on the training dynamics of GD and SGD.</p> <p>Because analyzing deep learning systems as is rigorously is usually difficult, people usually analyze modified (simplified) setups rigorously or analyze the full setup heuristically.</p> <p>In both cases, the goal is to obtain qualitative insights, rather than theoretical guarantees.</p>"},{"location":"books-and-courses/mfdnn/format/2/#stochastic-gradient-descent","title":"Stochastic Gradient Descent","text":"<p>Definition 2.7 : Finite-Sum Optimization Problem</p> <p>A finite-sum optimization problem has the structure</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} f_{i}(\\theta):=F(\\theta) \\] <p>Finite-sum is ubiquitous in ML. \\(N\\) usually corresponds to the number of data points.</p> <p>In finite-sum problem, using GD</p> \\[ \\theta^{k+1}=\\theta^{k}-\\frac{\\alpha_{k}}{N} \\sum_{i=1}^{N} \\nabla f_{i}\\left(\\theta^{k}\\right) \\] <p>is impractical when \\(N\\) is large since \\(\\frac{1}{N} \\sum_{i=1}^{N} \\nabla f_{i}\\left(\\theta^{k}\\right)\\) takes too long to compute.</p> <p>Concept 2.8 : Finite-sum problem can be reformulated with expectation.</p> <p>Although the finite-sum optimization problem has no inherent randomness, we can reformulate this problem with randomness:</p> \\[ \\operatorname{minimize}_{\\theta \\in \\mathbb{R}^{p}} \\quad \\mathbb{E}_{I}\\left[f_{I}(\\theta)\\right] \\] <p>where \\(I \\sim\\) Uniform \\(\\{1, \\ldots, N\\}\\). To see the equivalence,</p> \\[ \\mathbb{E}_{I}\\left[f_{I}(\\theta)\\right]=\\sum_{i=1}^{N} f_{i}(\\theta) \\mathbb{P}(I=i)=\\frac{1}{N} \\sum_{i=1}^{N} f_{i}(\\theta)=F(\\theta) \\] <p>Definition 2.9 : Stochastic Gradient Descent</p> <p>Stochastic gradient descent (SGD)</p> \\[ \\begin{gathered} i(k) \\sim \\operatorname{Uniform}\\{1, \\ldots, N\\} \\\\ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\nabla f_{i(k)}\\left(\\theta^{k}\\right) \\end{gathered} \\] <p>for \\(k=0,1, \\ldots\\), where \\(\\theta^{0} \\in \\mathbb{R}^{p}\\) is the initial point and \\(\\alpha_{k}&gt;0\\) is the learning rate.</p> <p>\\(\\nabla f_{i(k)}\\left(\\theta^{k}\\right)\\) is a stochastic gradient of \\(F\\) at \\(\\theta^{k}\\), i.e.,</p> \\[ \\mathbb{E}\\left[\\nabla f_{i(k)}\\left(\\theta^{k}\\right)\\right]=\\nabla \\mathbb{E}\\left[f_{i(k)}\\left(\\theta^{k}\\right)\\right]=\\nabla F\\left(\\theta^{k}\\right) \\] <p>Concept 2.10 : SGD is more efficient than GD.</p> <p>GD uses all indices \\(i=1, \\ldots, N\\) every iteration</p> \\[ \\theta^{k+1}=\\theta^{k}-\\frac{\\alpha_{k}}{N} \\sum_{i=1}^{N} \\nabla f_{i}\\left(\\theta^{k}\\right) \\] <p>SGD uses only a single random index \\(i(k)\\) every iteration</p> \\[ \\begin{gathered} i(k) \\sim \\text { Uniform }\\{1, \\ldots, N\\} \\\\ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\nabla f_{i(k)}\\left(\\theta^{k}\\right) \\end{gathered} \\] <p>When size of the data \\(N\\) is large, SGD is often more effective than GD.</p> <p>Concept 2.11 : Efficiency of stochastic gradient descent can be expected using the first-order Taylor expansion of \\(F\\).</p> <p>Plug \\(\\theta^{k+1}\\) into Taylor expansion of \\(F\\) about \\(\\theta^{k}\\) :</p> \\[ F\\left(\\theta^{k+1}\\right)=F\\left(\\theta^{k}\\right)-\\alpha_{k} \\nabla F\\left(\\theta^{k}\\right)^{\\top} \\nabla f_{i(k)}\\left(\\theta^{k}\\right)+\\mathcal{O}\\left(\\alpha_{k}^{2}\\right) \\] <p>Take expectation on both sides:</p> \\[ \\mathbb{E}_{k}\\left[F\\left(\\theta^{k+1}\\right)\\right]=F\\left(\\theta^{k}\\right)-\\alpha_{k}\\left\\|\\nabla F\\left(\\theta^{k}\\right)\\right\\|^{2}+\\mathcal{O}\\left(\\alpha_{k}^{2}\\right) \\] <p>( \\(\\mathbb{E}_{k}\\) is expectation conditioned on \\(\\theta^{k}\\) )</p> <p>\\(-\\nabla f_{i(k)}\\left(\\theta^{k}\\right)\\) is descent direction in expectation. For small (cautious) \\(\\alpha_{k}\\), SGD step reduces function value in expectation.</p>"},{"location":"books-and-courses/mfdnn/format/2/#variants-of-stochastic-gradient-descent","title":"Variants of Stochastic Gradient Descent","text":"<p>Consider</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} f_{i}(\\theta) \\] <p>SGD can be generalized to</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} g^{k} \\] <p>where \\(g^{k}\\) is a stochastic gradient. The choice \\(g^{k}=\\nabla f_{i(k)}\\left(\\theta^{k}\\right)\\) is just one option.</p> <p>Theorem 2.12 : Sampling with Replacement Lemma</p> <p>Let \\(X_{1}, \\ldots, X_{N} \\in \\mathbb{R}^{p}\\) be given (non-random) vectors. Let \\(\\frac{1}{N} \\sum_{i=1}^{N} X_{i}=\\mu\\). Let \\(i(1), \\ldots, i(B) \\subseteq\\{1, \\ldots, N\\}\\) be random indices. Then</p> \\[ \\mathbb{E} \\frac{1}{B} \\sum_{b=1}^{B} X_{i(b)}=\\mu \\] <p>Proof</p> \\[ \\mathbb{E} \\frac{1}{B} \\sum_{b=1}^{B} X_{i(b)}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{E} X_{i(b)}=\\frac{1}{B} \\sum_{b=1}^{B} \\mu=\\mu \\] <p>Definition 2.13 : Minibatch SGD with Replacement</p> <p>Minibatch SGD with replacement</p> \\[ \\begin{gathered} i(k, 1), \\ldots, i(k, B) \\sim \\text { Uniform }\\{1, \\ldots, N\\} \\\\ \\theta^{k+1}=\\theta^{k}-\\frac{\\alpha_{k}}{B} \\sum_{b=1}^{B} \\nabla f_{i(k, b)}\\left(\\theta^{k}\\right) \\end{gathered} \\] <p>To clarify, we sample \\(B\\) out of \\(N\\) indices with replacement, i.e., the same index can be sampled multiple times.</p> <p>By Theorem 2.12, \\(\\frac{1}{B} \\sum_{b=1}^{B} \\nabla f_{i(k, b)}\\left(\\theta^{k}\\right)\\) is a stochastic gradient of \\(F\\) at \\(\\theta^{k}\\).</p> <p>Theorem 2.14 : Sampling without Replacement Lemma</p> <p>Let \\(X_{1}, \\ldots, X_{N} \\in \\mathbb{R}^{p}\\) be given (non-random) vectors. Let \\(\\frac{1}{N} \\sum_{i=1}^{N} X_{i}=\\mu\\). Let \\(\\sigma\\) be a random permutation. Then</p> \\[ \\mathbb{E} \\frac{1}{B} \\sum_{b=1}^{B} X_{\\sigma(b)}=\\mu \\] <p>Proof</p> \\[ \\mathbb{E} \\frac{1}{B} \\sum_{b=1}^{B} X_{\\sigma(b)}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{E} X_{\\sigma(b)}=\\frac{1}{B} \\sum_{b=1}^{B} \\mu=\\mu \\] <p>Definition 2.15 : Minibatch SGD without Replacement</p> <p>Minibatch SGD without replacement</p> \\[ \\begin{gathered} \\sigma^{k} \\sim \\operatorname{permutation}(N) \\\\ \\theta^{k+1}=\\theta^{k}-\\frac{\\alpha_{k}}{B} \\sum_{b=1}^{B} \\nabla f_{\\sigma^{k}(b)}\\left(\\theta^{k}\\right) \\end{gathered} \\] <p>We assume \\(B \\leq N\\). To clarify, we sample \\(B\\) out of \\(N\\) indices without replacement, i.e., the same index cannot be sampled multiple times.</p> <p>By Theorem 2.14, \\(\\frac{1}{B} \\sum_{b=1}^{B} \\nabla f_{\\sigma^{k}(b)}\\left(\\theta^{k}\\right)\\) is a stochastic gradient of \\(F\\) at \\(\\theta^{k}\\).</p> <p>Concept 2.16 : How to choose batch size \\(B\\)?</p> <p>Note \\(B=1\\) minibatch SGD becomes SGD.</p> <p>Mathematically (measuring performance per iteration)</p> <ul> <li>Use large batch is when noise/randomness is large.</li> <li>Use small batch is when noise/randomness is small.</li> </ul> <p>Practically (measuring performance per unit time)</p> <ul> <li>Large batch allows more efficient computation on GPUs.</li> <li>Often best to increase batch size up to the GPU memory limit.</li> </ul> <p>In DL, SGD is applied to nice continuous but non-differentiable functions that are differentiable almost everywhere.</p> <p>In this case, if we choose \\(\\theta^{0} \\in \\mathbb{R}^{n}\\) randomly and run</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\nabla f\\left(\\theta^{k}\\right) \\] <p>the algorithm is usually well-defined, i.e., \\(\\theta^{k}\\) never hits a point of non-differentiability.</p> <p>With a proof or not, GD and SGD are applied to non-differentiable minimization in ML. The absence of differentiability does not seem to cause serious problems.</p> <p>Definition 2.17 : Cyclic SGD</p> <p>Consider the sequence of indices</p> \\[ \\{\\bmod (k, N)+1\\}_{k=0,1, \\ldots}=1,2, \\ldots, N, 1,2, \\ldots, N, \\ldots \\] <p>Here, \\(\\bmod (k, N)\\) is the remainder of \\(k\\) when divided by \\(N\\).</p> <p>Cyclic SGD:</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{\\mathbf{k}} \\nabla f_{\\bmod (k, N)+1}\\left(\\theta^{k}\\right) \\] <p>To clarify, this samples the indices in a (deterministic) cyclic order.</p> <p>Concept 2.18 : Pros and Cons of Cyclic SGD</p> <p>Strictly speaking, cyclic SGD is not an instance of SGD as unbiased estimation property lost.</p> <p>Advantage:</p> <ul> <li>Uses all indices (data) every \\(N\\) iterations.</li> </ul> <p>Disadvantage:</p> <ul> <li>Worse than SGD in some cases, theoretically and empirically.</li> <li>In DL, neural networks can learn to anticipate cyclic order.</li> </ul> <p>Definition 2.19 : Shuffled Cyclic SGD</p> <p>Shuffled Cyclic SGD:</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\nabla f_{\\sigma^{\\left \\lfloor \\frac{k}{N} \\right \\rfloor} (\\bmod (k, N)+1)}\\left(\\theta^{k}\\right) \\] <p>where \\(\\sigma^{0}, \\sigma^{1}, \\ldots\\) is a sequence of random permutations, i.e., we shuffle the order every cycle.</p> <p>Concept 2.19 : Pros and Cons of Shuffled Cyclic SGD</p> <p>Again, strictly speaking, shuffled cyclic SGD is not an instance of SGD as unbiased estimation property lost.</p> <p>Advantages :</p> <ul> <li>Uses all indices (data) every \\(N\\) iterations.</li> <li>Neural network cannot learn to anticipate data order.</li> <li>Empirically best performance.</li> </ul> <p>Disadvantages:</p> <ul> <li>Theory not as strong as regular SGD.</li> </ul> <p>Concept 2.20 : Which variant of SGD to use?</p> <p>Theoretical comparison of SGD variants:</p> <ul> <li>Not that easy.</li> <li>Result does not strongly correlate with practical performance in DL.</li> </ul> <p>In DL, the most common choice is</p> <ul> <li>shuffled cyclic minibatch SGD (without replacement) and</li> <li>batchsize \\(B\\) is as large as possible within the GPU memory limit.</li> </ul> <p>One can generally consider this to be the default option.</p> <p>Definition 2.21 : Epoch in finite-sum optimization and machine learning training</p> <p>An epoch is loosely defined as the unit of optimization or training progress of processing all indices or data once.</p> <ul> <li>1 iteration of GD constitutes an epoch.</li> <li>\\(N\\) iterations of SGD, cyclic SGD, or shuffled cyclic SGD constitute an epoch.</li> <li>\\(N / B\\) iterations of minibatch SGD constitute an epoch.</li> </ul> <p>Epoch is often a convenient unit for counting iterations compared to directly counting the iteration number.</p> <p>Concept 2.22 : SGD with General Expectation</p> <p>Consider an optimization problem with its objective defined with a general expectation</p> \\[ \\operatorname{minimize}_{\\theta \\in \\mathbb{R}^{p}} \\quad \\mathbb{E}_{\\omega}\\left[f_{\\omega}(\\theta)\\right]:=F(\\theta) \\] <p>Here, \\(\\omega\\) is a random variable. We will encounter these expectations (non-finite sum) when we talk about generative models.</p> <p>For this setup, the SGD algorithm is</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\nabla f_{\\omega^{k}}\\left(\\theta^{k}\\right) \\] <p>where \\(\\omega^{0}, \\omega^{1}, \\ldots\\) are IID random samples of \\(\\omega\\). If \\(\\nabla_{\\theta} \\mathbb{E}_{\\omega}\\left[f_{\\omega}(\\theta)\\right]=\\mathbb{E}_{\\omega}\\left[\\nabla_{\\theta} f_{\\omega}(\\theta)\\right]\\), then \\(\\nabla f_{\\omega^{k}}\\left(\\theta^{k}\\right)\\) is a stochastic gradient of \\(F(\\theta)\\) at \\(\\theta^{k}\\). (Make sure you understand why the previous SGD for the finite-sum setup is a special case of this.)</p> <p>GD for this setup is</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha_{k} \\mathbb{E}_{\\omega}\\left[\\nabla_{\\theta} f_{\\omega}\\left(\\theta^{k}\\right)\\right] \\] <p>However, if the expectation is difficult to compute GD is impractical and SGD is preferred.</p>"},{"location":"books-and-courses/mfdnn/format/3/","title":"\u00a7 3. Shallow Neural Networks","text":""},{"location":"books-and-courses/mfdnn/format/3/#supervised-learning-problem","title":"Supervised Learning Problem","text":"<p>Definition 3.1 : Supervised Learning Setup</p> <p>We have data \\(X_{1}, \\ldots, X_{N} \\in \\mathcal{X}\\) and corresponding labels \\(Y_{1}, \\ldots, Y_{N} \\in \\mathcal{Y}\\).</p> <ul> <li>Example) \\(X_{i}\\) is the \\(i\\) th email and \\(Y_{i} \\in\\{-1,+1\\}\\) denotes whether \\(X_{i}\\) is a spam email.</li> <li>Example) \\(X_{i}\\) is the \\(i\\) th image and \\(Y_{i} \\in\\{0, \\ldots, 9\\}\\) denotes handwritten digit.</li> </ul> <p>Assume there is a true unknown function</p> \\[ f_{\\star}: x \\rightarrow y \\] <p>mapping data to its label. In particular, \\(Y_{i}=f_{\\star}\\left(X_{i}\\right)\\) for \\(i=1, \\ldots, N\\).</p> <p>The goal of supervised learning is to use \\(X_{1}, \\ldots, X_{N}\\) and \\(Y_{1}, \\ldots, Y_{N}\\) to find \\(f \\approx f_{\\star}\\).</p> <p>Definition 3.2 : Supervised Learning Objective</p> <p>Assume a loss function such that \\(\\ell\\left(y_{1}, y_{2}\\right)=0\\) if \\(y_{1}=y_{2}\\) and \\(\\ell\\left(y_{1}, y_{2}\\right)&gt;0\\) if \\(y_{1} \\neq y_{2}\\).</p> <p>Restrict search to a class of parametrized functions \\(f_{\\theta}(x)\\) where \\(\\theta \\in \\Theta \\subseteq \\mathbb{R}^{p}\\), i.e., only consider \\(f \\in\\left\\{f_{\\theta} \\mid \\theta \\in \\Theta\\right\\}\\) where \\(\\Theta \\subseteq \\mathbb{R}^{p}\\).</p> <p>Take a finite sample \\(X_{1}, \\ldots, X_{N} \\in \\mathcal{X}\\) and corresponding labels \\(Y_{1}, \\ldots, Y_{N} \\in \\mathcal{Y}\\). Then solve</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(f_{\\theta}\\left(X_{i}\\right), f_{\\star}\\left(X_{i}\\right)\\right) \\] <p>which is equivalent to</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(f_{\\theta}\\left(X_{i}\\right), Y_{i}\\right) \\] <p>This is the standard form of the optimization problem (except regularizers) we consider in the supervised learning. We will talk about regularizers later.</p> <p>Concept 3.3 : Training is optimization.</p> <p>In machine learning, the anthropomorphized word \"training\" refers to solving an optimization problem such as</p> \\[ \\underset{\\theta \\in \\Theta}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(f_{\\theta}\\left(X_{i}\\right), Y_{i}\\right) \\] <p>In most cases, SGD or variants of SGD are used.</p> <p>We call \\(f_{\\theta}\\) the machine learning model or the neural network.</p> <p>Example 3.4 : Least-Squares Regression</p> <p>In LS, \\(\\mathcal{X}=\\mathbb{R}^{p}, \\mathcal{Y}=\\mathbb{R}, \\Theta=\\mathbb{R}^{p}, f_{\\theta}(x)=x^{\\top} \\theta\\), and \\(\\ell\\left(y_{1}, y_{2}\\right)=\\frac{1}{2}\\left(y_{1}-y_{2}\\right)^{2}\\).</p> <p>So we solve</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{2}\\left(f_{\\theta}\\left(X_{i}\\right)-Y_{i}\\right)^{2}=\\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{2}\\left(X_{i}^{\\top} \\theta-Y_{i}\\right)^{2}=\\frac{1}{2 N}\\|X \\theta-Y\\|^{2} \\] <p>where \\(X=\\left[\\begin{array}{c}X_{1}^{\\top} \\\\ \\vdots \\\\ X_{N}^{\\top}\\end{array}\\right]\\) and \\(Y=\\left[\\begin{array}{c}Y_{1} \\\\ \\vdots \\\\ Y_{N}\\end{array}\\right]\\).</p> <p>The model \\(f_{\\theta}(x)=x^{\\top} \\theta\\) is a shallow neural network. (The terminology will makes sense when contrasted with deep neural networks.)</p>"},{"location":"books-and-courses/mfdnn/format/3/#linear-classification","title":"Linear Classification","text":"<p>Definition 3.5 : Binary Classification and Linear Separability</p> <p>In binary classification, we have \\(\\mathcal{X}=\\mathbb{R}^{p}\\) and \\(\\mathcal{Y}=\\{-1,+1\\}\\).</p> <p>The data is linearly separable if there is a hyperplane defined by \\((a_{\\text {true }}, b_{\\text {true }} )\\) such that</p> \\[ y=\\left\\{\\begin{array}{cl} 1 &amp; \\text { if } a_{\\text {true }}^{\\top} x+b_{\\text {true }}&gt;0 \\\\ -1 &amp; \\text { otherwise. } \\end{array}\\right. \\] <p> </p>"},{"location":"books-and-courses/mfdnn/format/3/#support-vector-machine","title":"Support Vector Machine","text":"<p>Consider linear (affine) models</p> \\[ f_{a, b}(x)= \\begin{cases}+1 &amp; \\text { if } a^{\\top} x+b&gt;0 \\\\ -1 &amp; \\text { otherwise }\\end{cases} \\] <p>Consider the loss function</p> \\[ \\ell\\left(y_{1}, y_{2}\\right)=\\frac{1}{2}\\left|1-y_{1} y_{2}\\right|= \\begin{cases}0 &amp; \\text { if } y_{1}=y_{2} \\\\ 1 &amp; \\text { if } y_{1} \\neq y_{2}\\end{cases} \\] <p>The optimization problem</p> \\[ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(f_{a, b}\\left(X_{i}\\right), Y_{i}\\right) \\] <p>has a solution with optimal value 0 when the data is linearly separable.</p> <p>Problem: Optimization problem is discontinuous and thus cannot be solved with SGD.</p> <p>Motivation for SVM</p> <p>Even if the underlying function or phenomenon to approximate is discontinuous, the model needs to be continuous in its parameters. The loss function also needs to be continuous. (The prediction need not be continuous.)</p> <p>We consider a relaxation, is a continuous proxy of the discontinuous thing. Specifically, consider</p> \\[ f_{a, b}(x)=a^{\\top} x+b \\] <p>Once trained, \\(f_{a, b}(x)&gt;0\\) means the neural network is predicting \\(y=+1\\) to be \"more likely\", and \\(f_{a, b}(x)&lt;0\\) means the neural network is predicting \\(y=-1\\) to be \"more likely\".</p> <p>Therefore, we train the model to satisfy</p> \\[ Y_{i} f_{a, b}\\left(X_{i}\\right)&gt;0 \\text { for } i=1, \\ldots, N . \\] <p>Problem with strict inequality \\(Y_{i} f_{a, b}\\left(X_{i}\\right)&gt;0\\) :</p> <ul> <li>Strict inequality has numerical problems with round-off error.</li> <li>The magnitude \\(\\left|f_{a, b}(x)\\right|\\) can be viewed as the confidence of the prediction, but having a small positive value for \\(Y_{i} f_{a, b}\\left(X_{i}\\right)\\) indicates small confidence of the neural network.</li> </ul> <p>We modify our model's desired goal to be \\(Y_{i} f_{a, b}\\left(X_{i}\\right) \\geq 1\\).</p> <p>To train the neural network to satisfy</p> \\[ 0 \\geq 1-Y_{i} f_{a, b}\\left(X_{i}\\right) \\text { for } i=1, \\ldots, N . \\] <p>we minimize the excess positive component of the RHS</p> \\[ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\max \\left\\{0,1-Y_{i} f_{a, b}\\left(X_{i}\\right)\\right\\} \\] <p>which is equivalent to</p> \\[ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\max \\left\\{0,1-Y_{i}\\left(a^{\\top} X_{i}+b\\right)\\right\\} \\] <p>If the optimal value is 0, then the data is linearly separable.</p> <p>Definition 3.6 : Support Vector Machine (SVM)</p> <p>Use the model</p> \\[ f_{a, b}(x)=a^{\\top} x+b \\] <p>This following formulation is called the support vector machine (SVM)</p> \\[ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\max \\left\\{0,1-Y_{i} f_{a, b}\\left(X_{i}\\right)\\right\\} \\] \\[ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\max \\left\\{0,1-Y_{i}\\left(a^{\\top} X_{i}+b\\right)\\right\\} \\] <p>It is also common to add a regularizer</p> \\[ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\max \\left\\{0,1-Y_{i}\\left(a^{\\top} X_{i}+b\\right)\\right\\}+\\frac{\\lambda}{2}\\|a\\|^{2} \\] <p>Concept 3.7 : Prediction with SVM</p> <p>Once the SVM is trained, make predictions with</p> \\[ \\operatorname{sign}\\left(f_{a, b}(x)\\right)=\\operatorname{sign}\\left(a^{\\top} x+b\\right) \\] <p>when \\(f_{a, b}(x)=0\\), we assign \\(\\operatorname{sign}\\left(f_{a, b}(x)\\right)\\) arbitrarily.</p> <p>Note that the prediction is discontinuous, but predictions are in \\(\\{-1,+1\\}\\) so it must be discontinuous.</p> <p>If \\(\\sum_{i=1}^{N} \\max \\left\\{0,1-Y_{i} f_{a, b}\\left(X_{i}\\right)\\right\\}=0\\), then \\(\\operatorname{sign}\\left(f_{a, b}\\left(X_{i}\\right)\\right)=Y_{i}\\) for \\(i=1, \\ldots, N\\), i.e., the neural network predicts the known labels perfectly. Of course, it is a priori not clear how accurate the prediction will be for new unseen data.</p>"},{"location":"books-and-courses/mfdnn/format/3/#logistic-regression","title":"Logistic Regression","text":"<p>Concept 3.8 : Relaxed Supervised Learning Setup</p> <p>We relax the supervised learning setup to predict probabilities, rather than make point predictions. So, labels are generated based on data, perhaps randomly. Consider data \\(X_{1}, \\ldots, X_{N} \\in \\mathcal{X}\\) and labels \\(Y_{1}, \\ldots, Y_{N} \\in \\mathcal{Y}\\). Assume there exists a function</p> \\[ f_{\\star}: \\mathcal{X} \\rightarrow \\mathcal{P}(\\mathcal{Y}) \\] <p>where \\(\\mathcal{P}(\\mathcal{Y})\\) denotes the set of probability distributions on \\(\\mathcal{Y}\\). Assume the generation of \\(Y_{i}\\) given \\(X_{i}\\) is independent of \\(Y_{j}\\) and \\(X_{j}\\) for \\(j \\neq i\\).</p> <ul> <li>Example) \\(f(X)=\\left[\\begin{array}{l}0.8 \\\\ 0.2\\end{array}\\right]\\) in dog vs. cat classifier.</li> <li>Example) An email saying \"Buy this thing at our store!\" may be spam to some people, but it may not be spam to others.</li> </ul> <p>The relaxed SL setup is more general and further realistic.</p> <p>Definition 3.9 : Empirical Distribution for Binary Classification</p> <p>In basic binary classification, define the empirical distribution</p> \\[ \\mathcal{P}(y)= \\begin{cases}{\\left[\\begin{array}{l} 1 \\\\ 0 \\end{array}\\right]} &amp; \\text { if } y=-1 \\\\ {\\left[\\begin{array}{l} 0 \\\\ 1 \\end{array}\\right]} &amp; \\text { if } y=+1\\end{cases} \\] <p>More generally, the empirical distribution describes the data we have seen. In this context, we have only seen one label per datapoint, so our empirical distributions are one-hot vectors.</p> <p>(If there are multiple annotations per data point \\(x\\) and they don't agree, then the empirical distribution may not be one-hot vectors. For example, given the same email, some users may flag it as spam while others consider it useful information.)</p> <p>Definition 3.10 : KL-Divergence, Cross Entropy</p> <p>Let \\(p, q \\in \\mathbb{R}^{n}\\) represent probability masses, i.e., \\(p_{i} \\geq 0\\) for \\(i=1, \\ldots, n\\) and \\(\\sum_{i=1}^{n} p_{i}=1\\) and the same for \\(q\\). The Kullback-Leibler-divergence (KL-divergence) from \\(q\\) to \\(p\\) is</p> \\[ \\begin{array}{ll} D_{\\mathrm{KL}}(p \\| q)=\\displaystyle \\sum_{i=1}^{n} p_{i} \\log \\left(\\frac{p_{i}}{q_{i}}\\right)= &amp; -\\displaystyle \\sum_{i=1}^{n} p_{i} \\log \\left(q_{i}\\right) &amp; +\\displaystyle \\sum_{i=1}^{n} p_{i} \\log \\left(p_{i}\\right) \\\\ &amp; =H(p, q) &amp; =-H(p) \\\\ &amp; \\text { cross entropy of } q &amp; =-H \\\\ &amp; \\text { relative to } p &amp; \\text { entropy of } p \\end{array} \\] <p>The cross entropy of \\(q\\) relative to \\(p\\) is</p> \\[ H(p, q) = -\\sum_{i=1}^{n} p_{i} \\log \\left(q_{i}\\right) \\] <p>Theorem 3.11 : Properties of KL-Divergence</p> \\[ D_{\\mathrm{KL}}(p \\| q)=\\sum_{i=1}^{n} p_{i} \\log \\left(\\frac{p_{i}}{q_{i}}\\right) \\] <ul> <li>Not symmetric, i.e., \\(D_{\\mathrm{KL}}(p \\| q) \\neq D_{\\mathrm{KL}}(q \\| p)\\).</li> <li>\\(D_{\\mathrm{KL}}(p \\| q)&gt;0\\) if \\(p \\neq q\\) and \\(D_{\\mathrm{KL}}(p \\| q)=0\\) if \\(p=q\\).</li> <li>\\(D_{\\mathrm{KL}}(p \\| q)=\\infty\\) is possible. (Further detail below.)</li> </ul> <p>Often used as a \"distance\" between \\(p\\) and \\(q\\) despite not being a metric.</p> <p>Clarification: Use the convention</p> <ul> <li>\\(0 \\log \\left(\\frac{0}{0}\\right)=0\\left(\\right.\\) when \\(\\left.p_{i}=q_{i}=0\\right)\\)</li> <li>\\(0 \\log \\left(\\frac{0}{q_{i}}\\right)=0\\) if \\(q_{i}&gt;0\\)</li> <li>\\(p_{i} \\log \\left(\\frac{p_{i}}{0}\\right)=\\infty\\) if \\(p_{i}&gt;0\\)</li> </ul> <p>Probabilistic interpretation:</p> \\[ D_{\\mathrm{KL}}(p \\| q)=\\mathbb{E}_{I}\\left[\\log \\left(\\frac{p_{I}}{q_{I}}\\right)\\right] \\] <p>with the random variable \\(I\\) such that \\(\\mathbb{P}(I=i)=p_{i}\\).</p> <p>Definition 3.12 : Logistic Regression (LR)</p> <p>Logistic regression (LR), is another model for binary classification:</p> <p>Use the model</p> \\[ f_{a, b}(x)=\\left[\\begin{array}{c} \\frac{1}{1+e^{a^{\\top} x+b}} \\\\ \\frac{e^{a^{\\top} x+b}}{1+e^{a^{\\top} x+b}} \\end{array}\\right]=\\left[\\begin{array}{c} \\frac{1}{1+e^{a^{\\top} x+b}} \\\\ \\frac{1}{1+e^{-\\left(a^{\\top} x+b\\right)}} \\end{array}\\right]\\begin{array}{c} = \\mathbb{P}(y=-1) \\\\ = \\mathbb{P}(y=+1) \\end{array} \\] <p>Minimize KL-Divergence (or cross entropy) from the model \\(f_{a, b}\\left(X_{i}\\right)\\) output probabilities to the empirical distribution \\(\\mathcal{P}\\left(Y_{i}\\right)\\).</p> \\[ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\sum_{i=1}^{N} D_{\\mathrm{KL}}\\left(\\mathcal{P}\\left(Y_{i}\\right) \\| f_{a, b}\\left(X_{i}\\right)\\right) \\] <p>Concept 3.13 : Other Expression of Logistic Regression</p> \\[ \\begin{gathered} \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\sum_{i=1}^{N} D_{\\mathrm{KL}}\\left(\\mathcal{P}\\left(Y_{i}\\right) \\| f_{a, b}\\left(X_{i}\\right)\\right) \\\\ \\mathbb{\\Updownarrow} \\\\ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\sum_{i=1}^{N} H\\left(\\mathcal{P}\\left(Y_{i}\\right), f_{a, b}\\left(X_{i}\\right)\\right)+(\\text { terms independent of } a, b) \\\\ \\mathbb{\\Updownarrow} \\\\ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\sum_{i=1}^{N} \\log \\left(1+\\exp \\left(-Y_{i}\\left(a^{\\top} X_{i}+b\\right)\\right)\\right) \\\\ \\mathbb{\\Updownarrow} \\\\ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(Y_{i}\\left(a^{\\top} X_{i}+b\\right)\\right) \\end{gathered} \\] <p>where \\(\\ell(z)=\\log \\left(1+e^{-z}\\right)\\).</p> <p>Concept 3.14 : Prediction with LR</p> <p>When performing point prediction with \\(\\mathrm{LR}, a^{\\top} x+b&gt;0\\) means \\(\\mathbb{P}(y=+1)&gt;0.5\\) and vice versa.</p> <p>Once the LR is trained, make predictions with</p> \\[ \\operatorname{sign}\\left(a^{\\top} x+b\\right) \\] <p>when \\(a^{\\top} x+b=0\\), we assign \\(\\operatorname{sign}\\left(a^{\\top} x+b\\right)\\) arbitrarily. This is the same as SVM.</p> <p>Again, it is a priori not clear how accurate the prediction will be for new unseen data.</p> <p>Concept 3.15 : SVM vs LR</p> <p>Both support vector machine and logistic regression can be written as</p> \\[ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(Y_{i}\\left(a^{\\top} X_{i}+b\\right)\\right) \\] <ul> <li>SVM uses \\(\\ell(z)=\\max \\{0,1-z\\}\\). Obtained from relaxing the discontinuous prediction loss.</li> <li>LR uses \\(\\ell(z)=\\log \\left(1+e^{-z}\\right)\\). Obtained from relaxing the supervised learning setup from predicting the label to predicting the label probabilities.</li> </ul> <p> </p> <p>SVM and LR are both \"linear\" classifiers:</p> <ul> <li>Decision boundary \\(a^{\\top} x+b=0\\) is linear.</li> <li>Model completely ignores information perpendicular to \\(a\\).</li> </ul> <p>LR naturally generalizes to multi-class classification via softmax regression. Generalizing SVM to multi-class classification is trickier and less common.</p> <p>Concept 3.16 : Maximum Likelihood Estimation \\(\\cong\\) minimizing KL divergence</p> <p>Consider the setup where you have IID discrete random variables \\(X_{1}, \\ldots, X_{N}\\) that can take values \\(1, \\ldots, k\\). We model the probability masses with \\(\\mathbb{P}_{\\theta}(X=1), \\ldots, \\mathbb{P}_{\\theta}(X=k)\\). The maximum likelihood estimation (MLE) is obtained by solving</p> \\[ \\underset{\\theta}{\\operatorname{maximize}} \\frac{1}{N} \\sum_{i=1}^{N} \\log \\left(\\mathbb{P}_{\\theta}\\left(X_{i}\\right)\\right) \\] <p>Next, define</p> \\[ f_{\\theta}=\\left[\\begin{array}{c} \\mathbb{P}_{\\theta}(X=1) \\\\ \\vdots \\\\ \\mathbb{P}_{\\theta}(X=k) \\end{array}\\right], \\quad \\mathcal{P}\\left(X_{1}, \\ldots, X_{N}\\right)=\\frac{1}{N}\\left[\\begin{array}{c} \\#\\left(X_{i}=1\\right) \\\\ \\vdots \\\\ \\#\\left(X_{i}=k\\right) \\end{array}\\right] . \\] <p>Then MLE is equivalent to minimizing the KL divergence from the model to the empirical distribution.</p> \\[ \\begin{gathered} \\text{MLE} \\\\ \\mathbb{\\Updownarrow} \\\\ \\underset{\\theta}{\\operatorname{minimize}} H \\left( \\mathcal{P}\\left(X_{1}, \\ldots, X_{N}\\right), f_{\\theta}\\right) \\\\ \\mathbb{\\Updownarrow} \\\\ \\underset{\\theta}{\\operatorname{minimize}} D_{\\mathrm{KL}} \\left( \\mathcal{P}\\left(X_{1}, \\ldots, X_{N}\\right), f_{\\theta}\\right) \\\\ \\end{gathered} \\] <p>One can also derive LR equivalently as the MLE.</p> <p>Generally, one can view the MLE as minimizing the KL divergence between the model and the empirical distribution. (For continuous random variables like the Gaussian, this requires extra work, since we haven't defined the KL divergence for continuous random variables.)</p> <p>In deep learning, the distance measure need not be KL divergence.</p>"},{"location":"books-and-courses/mfdnn/format/3/#prediction","title":"Prediction","text":"<p>Definition 3.17 : Estimation, Prediction</p> <p>Finding \\(f \\approx f_{\\star}\\) for unknown</p> \\[ f_{\\star}: \\mathcal{X} \\rightarrow \\mathcal{P}(\\mathcal{Y}) \\] <p>is called estimation. When we consider a parameterized model \\(f_{\\theta}\\), finding \\(\\theta\\) is the estimation. However, estimation is usually not the end goal.</p> <p>The end goal is prediction. It is to use \\(f_{\\theta} \\approx f_{\\star}\\) on new data \\(X_{1}^{\\prime}, \\ldots, X_{M}^{\\prime} \\in \\mathcal{X}\\) to find labels \\(Y_{1}^{\\prime}, \\ldots, Y_{M}^{\\prime} \\in \\mathcal{Y}\\).</p> <p>Concept 3.18 : Is prediction possible?</p> <p>In the worst hypotheticals, prediction is impossible.</p> <ul> <li>Even though smoking is harmful for every other human being, how can we be 100% sure that this one person is not a mutant who benefits from the chemicals of a cigarette?</li> <li>Water freezes at \\(0^{\\circ}\\), but will the same be true tomorrow? How can we be 100% sure that the laws of physics will not suddenly change tomorrow?</li> </ul> <p>Of course, prediction is possible in practice.</p> <p>Theoretically, prediction requires assumptions on the distribution of \\(X\\) and the model of \\(f_{\\star}\\) is needed. This is in the realm of statistics of statistical learning theory.</p> <p>For now, we will take the view that if we predict known labels of the training data, we can reasonably hope to do well on the new data. (We will discuss the issue of generalization and overfitting later.)</p> <p>Concept 3.19 : Training Data vs Test Data</p> <p>When testing a machine learning model, it is essential that one separates the training data with the test data.</p> <p>In other classical disciplines using data, one performs a statistical hypothesis test to obtain a \\(p\\)-value. In ML, people do not do that.</p> <p>The only sure way to ensure that the model is doing well is to assess its performance on new data.</p> <p>Usually, training data and test data is collected together. This ensures that they have the same statistical properties. The assumption is that this test data will be representative of the actual data one intends to use machine learning on.</p>"},{"location":"books-and-courses/mfdnn/format/3/#datasets","title":"Datasets","text":"<p>Concept 3.20 : MNIST</p> <p>Images of hand-written digits with \\(28 \\times 28=784\\) pixels and integervalued intensity between 0 and 255 . Every digit has a label in \\(\\{0,1, \\ldots, 8,9\\}\\).</p> <p>70,000 images (60,000 for training / 10,000 testing) of 10 almost balanced classes.</p> <p>One of the simplest data set used in machine learning.</p> <p> </p> <p>The USA government needed a standardized test to assess handwriting recognition software being sold to the government. So the NIST (National Institute of Standards and Technology) created the dataset in the 1990s. In 1990, NIST Special Database 1 distributed on CD-ROMs by mail. NIST SD 3 (1992) and SD 19 (1995) were improvements.</p> <p>Humans were instructed to fill out handwriting sample forms. However, humans cannot be trusted to follow instructions, so a lab technician performed \"human ground truth adjudication\".</p> <p>In 1998, Man LeCun, Corinna Cortes, Christopher J. C. Barges took the NIST dataset and modified it to create the MNIST dataset.</p> <p>Concept 3.21 : CIFAR10</p> <p>60,000 \\(32 \\times 32\\) color images in 10 (perfectly) balanced classes.</p> <p> </p> <p>(There is no overlap between automobiles and trucks. \u201cAutomobile\u201d includes sedans, SUVs, things of that sort. \u201cTruck\u201d includes only big trucks. Neither includes pickup trucks.)</p> <p>In 2008, a MIT and NYU team created the 80 million tiny images data set by searching on Google, Flickr, and Altavista for every non-abstract English noun and downscaled the images to \\(32 \\times 32\\). The search term provided an unreliable label for the image. This dataset was not very easy to use since the classes were too numerous.</p> <p>In 2009, Alex Krizhevsky published the CIFAR10, by distilling just a few classes and cleaning up the labels. Students were paid to verify the labels.</p> <p>The dataset was named CIFAR-10 after the funding agency Canadian Institute For Advanced Research.</p> <p>There is also a CIFAR-100 with 100 classes.</p> <p>Concept 3.22 : Roles of Datasets in ML Research</p> <p>An often underappreciated contribution.</p> <p>Good datasets play a crucial role in driving progress in ML research.</p> <p>Thinking about the dataset is the essential first step of understanding the feasibility of a ML task.</p> <p>Accounting for the cost of producing datasets and leveraging freely available data as much as possible (semi-supervised learning) is a recent trend in machine learning.</p>"},{"location":"books-and-courses/mfdnn/format/4/","title":"\u00a7 4. Deep Neural Networks","text":""},{"location":"books-and-courses/mfdnn/format/4/#deep-neural-networks","title":"Deep Neural Networks","text":"<p>Concept 4.1 : LR can be seen as 1-layer (shallow) neural network.</p> <p>In LR, we solve</p> \\[ \\underset{a \\in \\mathbb{R}^{p}, b \\in \\mathbb{R}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(f_{\\theta}\\left(X_{i}\\right), Y_{i}\\right) \\] <p>where \\(\\ell\\left(y_{1}, y_{2}\\right)=\\log \\left(1+e^{-y_{1} y_{2}}\\right)\\) and \\(f_{\\theta}\\) is linear.</p> <p>We can view \\(f_{\\theta}(x)=O=a^{\\top} x+b\\) as a 1-layer (shallow) neural network.</p> <p> </p> <p>Concept 4.2 : Deep Neural Networks with Nonlinearities</p> <p>What happens if we stack multiple linear layers?</p> <p>Problem: This is pointless because composition of linear functions is linear. ( \\(O = W_2 h = W_2(W_1 x) = (W_2 W_1) x \\leftarrow\\) linear in \\(x\\) )</p> <p>Solution: use a nonlinear activation function \\(\\sigma\\) to inject nonlinearities.</p> <p> </p> <p>Hidden layer \\(h=\\sigma(W_1 x)\\) (\\(\\sigma\\) : Nonlinear function) Output layer \\(O=W_2 h = W_2 \\sigma(W_1 x) \\leftarrow\\) nonlinear in \\(x\\)</p> <p>Definition 4.3 : Common Activation Functions</p> <ul> <li> <p>Rectified Linear Unit (ReLU)</p> \\[ \\operatorname{ReLU}(z)=\\max (z, 0) \\] <p> </p> </li> <li> <p>Sigmoid</p> \\[ \\operatorname{Sigmoid}(z)=\\frac{1}{1+e^{-z}} \\] <p> </p> </li> <li> <p>Hyperbolic Tangent</p> \\[ \\tanh (z)=\\frac{1-e^{-2 z}}{1+e^{-2 z}} \\] <p> </p> </li> </ul> <p>Definition 4.4 : Multilayer Perceptron (MLP)</p> <p>The multilayer perceptron, also called fully connected neural network, has the form</p> \\[ \\begin{aligned} y_{L}= &amp; W_{L} y_{L-1}+b_{L} \\\\ y_{L-1}= &amp; \\sigma\\left(W_{L-1} y_{L-2}+b_{L-1}\\right) \\\\ &amp; \\vdots \\\\ y_{2}= &amp; \\sigma\\left(W_{2} y_{1}+b_{2}\\right) \\\\ y_{1}= &amp; \\sigma\\left(W_{1} x+b_{1}\\right), \\end{aligned} \\] <p>where \\(x \\in \\mathbb{R}^{n_{0}}, W_{\\ell} \\in \\mathbb{R}^{n_{\\ell} \\times n_{\\ell-1}}, b_{\\ell} \\in \\mathbb{R}^{n_{\\ell}}\\), and \\(n_{L}=1\\). To clarify, \\(\\sigma\\) is applied element-wise.</p> <p>Definition 4.5 : Linear Layer (with Batches)</p> <ul> <li>Input tensor: \\(X \\in \\mathbb{R}^{B \\times n}, B\\) batch size, \\(n\\) number of indices.</li> <li>Output tensor: \\(Y \\in \\mathbb{R}^{B \\times m}, B\\) batch size, \\(m\\) number of indices.</li> </ul> <p>With weight \\(A \\in \\mathbb{R}^{m \\times n}\\), bias \\(b \\in \\mathbb{R}^{m}, k=1, \\ldots B\\), and \\(i=1, \\ldots, m\\) :</p> \\[ Y_{k, i}=\\sum_{j=1}^{n} A_{i, j} X_{k, j}+b_{i} \\] <p>Operation is independent across elements of the batch.</p> <p>If <code>bias=False</code>, then \\(b=0\\).</p>"},{"location":"books-and-courses/mfdnn/format/4/#multi-class-classification","title":"Multi-Class Classification","text":"<p>Definition 4.6 : Multi-Class Classification Problem</p> <p>Consider supervised learning with data \\(X_{1}, \\ldots, X_{N} \\in \\mathbb{R}^{n}\\) and labels \\(Y_{1}, \\ldots, Y_{N} \\in\\{1, \\ldots, k\\}\\). (A \\(k\\) class classification problem.) Assume there exists a function \\(f_{\\star}: \\mathbb{R}^{n} \\rightarrow \\Delta^{k}\\) mapping from data to label probabilities. Here, \\(\\Delta^{k} \\subset \\mathbb{R}^{k}\\) denotes the set of probability mass functions on \\(\\{1, \\ldots, k\\}\\).</p> <p>Define the empirical distribution \\(\\mathcal{P}(y) \\in \\mathbb{R}^{k}\\) as the one-hot vector:</p> \\[ (\\mathcal{P}(y))_{i}=\\left\\{\\begin{array}{cc} 1 &amp; \\text { if } y=i \\\\ 0 &amp; \\text { otherwise } \\end{array}\\right. \\] <p>for \\(i=1, \\ldots, k\\).</p> <p>Definition 4.7 : Softmax Function</p> <p>Softmax function \\(\\mu: \\mathbb{R}^{k} \\rightarrow \\Delta^{k}\\) is defined by</p> \\[ \\mu_{i}(z)=(\\mu(z))_{i}=\\frac{e^{z_{i}}}{\\sum_{j=1}^{k} e^{z_{j}}} \\] <p>for \\(i=1, \\ldots, k\\), where \\(z=\\left(z_{1}, \\ldots, z_{k}\\right) \\in \\mathbb{R}^{k}\\). Since</p> \\[ \\sum_{i=1}^{k} \\mu_{i}(z)=1, \\quad \\mu&gt;0 \\] <p>Name \"softmax\" is a misnomer. \"Softargmax\" would be more accurate</p> <ul> <li>\\(\\mu(z) \\not \\approx \\max (z)\\)</li> <li>\\(\\mu(z) \\approx \\operatorname{argmax}(z)\\)</li> </ul> <p>Examples:</p> \\[ \\mu\\left(\\left[\\begin{array}{l} 1 \\\\ 2 \\\\ 3 \\end{array}\\right]\\right)=\\left[\\begin{array}{l} 0.09 \\\\ 0.24 \\\\ 0.6 \\end{array}\\right], \\mu\\left(\\left[\\begin{array}{c} 999 \\\\ 0 \\\\ -2 \\end{array}\\right]\\right) \\approx\\left[\\begin{array}{l} 1 \\\\ 0 \\\\ 0 \\end{array}\\right], \\mu\\left(\\left[\\begin{array}{c} -2 \\\\ -2 \\\\ -99 \\end{array}\\right]\\right) \\approx\\left[\\begin{array}{c} 0.5 \\\\ 0.5 \\\\ 0 \\end{array}\\right] \\] <p>Definition 4.8 : Softmax Regression (SR)</p> <p>In softmax regression (SR):</p> <p>Choose the model</p> \\[ \\mu\\left(f_{A, b}(x)\\right)=\\frac{1}{\\sum_{i=1}^{k} e^{a_{i}^{\\top} x+b_{i}}}\\left[\\begin{array}{c} e^{a_{1}^{\\top} x+b_{1}} \\\\ e^{a_{2}^{\\top} x+b_{2}} \\\\ \\vdots \\\\ e^{a_{k}^{\\top} x+b_{k}} \\end{array}\\right], \\quad f_{A, b}(x)=A x+b, A=\\left[\\begin{array}{c} a_{1}^{\\top} \\\\ a_{2}^{\\top} \\\\ \\vdots \\\\ a_{k}^{\\top} \\end{array}\\right] \\in \\mathbb{R}^{k \\times n}, \\quad b=\\left[\\begin{array}{c} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{k} \\end{array}\\right] \\in \\mathbb{R}^{k} . \\] <p>Minimize KL-Divergence (or cross entropy) from the model \\(\\mu\\left(f_{A, b}\\left(X_{i}\\right)\\right)\\) output probabilities to the empirical distribution \\(\\mathcal{P}\\left(Y_{i}\\right)\\).</p> \\[ \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} \\sum_{i=1}^{N} D_{\\mathrm{KL}}\\left(\\mathcal{P}\\left(Y_{i}\\right) \\| \\mu\\left(f_{A, b}\\left(X_{i}\\right)\\right)\\right) \\Longleftrightarrow \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} \\sum_{i=1}^{N} H\\left(\\mathcal{P}\\left(Y_{i}\\right), \\mu\\left(f_{A, b}\\left(X_{i}\\right)\\right)\\right) \\] <p>Concept 4.9 : Other Expression of Softmax Regression</p> \\[ \\begin{gathered} \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} \\sum_{i=1}^{N} H\\left(\\mathcal{P}\\left(Y_{i}\\right), \\mu\\left(f_{A, b}\\left(X_{i}\\right)\\right)\\right) \\\\ \\mathbb{\\Updownarrow} \\\\ \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N}-\\log \\left(\\mu_{Y_{i}}\\left(f_{A, b}\\left(X_{i}\\right)\\right)\\right) \\\\ \\mathbb{\\Updownarrow} \\\\ \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N}-\\log \\left(\\frac{\\exp \\left(a_{Y_{i}}^{\\top} X_{i}+b_{Y_{i}}\\right)}{\\sum_{j=1}^{k} \\exp \\left(a_{j}^{\\top} X_{i}+b_{j}\\right)}\\right) \\\\ \\mathbb{\\Updownarrow} \\\\ \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N}\\left(-\\left(a_{Y_{i}}^{\\top} X_{i}+b_{Y_{i}}\\right)+\\log \\left(\\sum_{j=1}^{k} \\exp \\left(a_{j}^{\\top} X_{i}+b_{j}\\right)\\right)\\right) \\end{gathered} \\] <p>Definition 4.10 : Cross Entropy Loss</p> <p>Where \\(f \\in \\mathbb{R}^{k}, y \\in \\{1, 2, \\cdots, k\\}\\), the cross entropy loss is</p> \\[ \\ell^{\\mathrm{CE}}(f, y)=-\\log \\left(\\frac{\\exp \\left(f_{y}\\right)}{\\sum_{j=1}^{k} \\exp \\left(f_{j}\\right)}\\right) \\] <p>Concept 4.11 : SR uses cross entropy loss as loss function.</p> \\[ \\begin{gathered} \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} \\sum_{i=1}^{N} H\\left(\\mathcal{P}\\left(Y_{i}\\right), \\mu\\left(f_{A, b}\\left(X_{i}\\right)\\right)\\right) \\\\ \\mathbb{\\Updownarrow} \\\\ \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N}-\\log \\left(\\frac{\\exp \\left(a_{Y_{i}}^{\\top} X_{i}+b_{Y_{i}}\\right)}{\\sum_{j=1}^{k} \\exp \\left(a_{j}^{\\top} X_{i}+b_{j}\\right)}\\right) \\\\ \\mathbb{\\Updownarrow} \\\\ \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell^{\\mathrm{CE}}\\left(f_{A, b}\\left(X_{i}\\right), Y_{i}\\right) \\end{gathered} \\] <ul> <li>SR = linear model \\(f_{A, b}\\) with cross entropy loss:</li> </ul> \\[ \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell^{\\mathrm{CE}}\\left(f_{A, b}\\left(X_{i}\\right), Y_{i}\\right) \\Longleftrightarrow \\underset{A \\in \\mathbb{R}^{k \\times n}, b \\in \\mathbb{R}^{k}}{\\operatorname{minimize}} \\sum_{i=1}^{N} D_{\\mathrm{KL}}\\left(\\mathcal{P}\\left(Y_{i}\\right) \\| \\mu\\left(f_{A, b}\\left(X_{i}\\right)\\right)\\right) \\] <ul> <li> <p>The natural extension of SR is to consider</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell^{\\mathrm{CE}}\\left(f_{\\theta}\\left(X_{i}\\right), Y_{i}\\right) \\Leftrightarrow \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\quad \\sum_{i=1}^{N} D_{\\mathrm{KL}}\\left(\\mathcal{P}\\left(Y_{i}\\right) \\| \\mu\\left(f_{\\theta}\\left(X_{i}\\right)\\right)\\right) \\] <p>where \\(f_{\\theta}\\) is a deep neural network.</p> </li> </ul>"},{"location":"books-and-courses/mfdnn/format/4/#gpus-in-deep-learning","title":"GPUs in Deep Learning","text":"<p>Concept 4.12 : History of GPU Computing</p> <p>Rendering graphics involves computing many small tasks in parallel. Graphics cards provide many small processors to render graphics.</p> <p>In 1999, Nvidia released GeForce 256 and introduced programmability in the form of vertex and pixel shaders. Marketed as the first 'Graphical Processing Unit (GPU)'.</p> <p>Researchers quickly learned how to implement linear algebra by mapping matrix data into textures and applying shaders.</p> <p>In 2007, Nvidia released 'Compute Unified Device Architecture (CUDA)', which enabled general purpose computing on a CUDA-enabled GPUs.</p> <p>Unlike CPUs which provide fast serial processing, GPUs provide massive parallel computing with its numerous slower processors.</p> <p>The 2008 financial crisis hit Nvidia very hard as GPUs were luxury items used for games. This encouraged Nvidia to invest further in 'General Purpose GPUs (GPGPU)' and create a more stable consumer base.</p> <p>Concept 4.13 : CPU Computing Model</p> <p> </p> <p>Concept 4.14 : GPU Computing Model</p> <p> </p> <p>Concept 4.15 : GPUs for Machine Learning</p> <p>Raina et al.'s 2009* paper demonstrated that GPUs can be used to train large neural networks. (This was not the first to use of GPUs in machine learning, but it was one of the most influential.)</p> <p>Modern deep learning is driven by big data and big compute, respectively provided by the internet and GPUs.</p> <p>Krizhevsky et al.'s 2012 landmark paper introduced AlexNet trained on GPUs and kickstarted the modern deep learning boom.</p> <p>(R. Raina, A. Madhavan, and A. Y. Ng , Large-scale Deep Unsupervised Learning using Graphics Processors, ICML, 2009. /  A. Krizhevsky, I. Sutskever, G. E. Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NeurIPS, 2012.)</p> <p>Concept 4.16 : Deep Learning on GPUs</p> <p>Steps for training neural network on GPU:</p> <ol> <li> <p>Create the neural network on CPU and send it to GPU. Neural network parameters stay on GPU.</p> <ul> <li>Sometimes you load parameters from CPU to GPU.</li> </ul> </li> <li> <p>Select data batch (image, label) and send it to GPU every iteration</p> <ul> <li>Data for real-world setups is large, so keeping all data on GPU is infeasible.</li> </ul> </li> <li> <p>On GPU, compute network output (forward pass)</p> </li> <li>On GPU, compute gradients (backward pass)</li> <li>On GPU, perform gradient update</li> <li> <p>Once trained, perform prediction on GPU.</p> <ul> <li>Send test data to GPU.</li> <li>Compute network output.</li> <li>Retrieve output on CPU.</li> <li>Alternatively, neural network can be loaded on CPU and prediction can be done on CPU.</li> </ul> </li> </ol>"},{"location":"books-and-courses/mfdnn/format/5/","title":"\u00a7 5. Convolutional Neural Networks","text":""},{"location":"books-and-courses/mfdnn/format/5/#convolutional-layers","title":"Convolutional Layers","text":"<p>Concept 5.1 : Pros and Cons of Fully Connected Layers</p> <p>Advantages of fully connected layers:</p> <ul> <li>Simple.</li> <li>Very general, in theory. (Sufficiently large MLPs can learn any function, in theory.)</li> </ul> <p>Disadvantage of fully connected layers:</p> <ul> <li>Too many trainable parameters.</li> <li>Does not encode shift equivariance/invariance and therefore has poor inductive bias. (More on this later.)</li> </ul> <p>Concept 5.2 : Shift Equivarience/Invariance in Vision</p> <p>Many tasks in vision are equivariant/invariant with respect shifts/translations.</p> <p> </p> <p>Roughly speaking, equivariance/invariance means shifting the object does not change the meaning (it only changes the position).</p> <p>Logistic regression (with a single fully connected layer) does not encode shift invariance.</p> <p>Since convolution is equivariant with respect to translations, constructing neural network layers with them is a natural choice.</p> <p>Definition 5.3 : 2D Convolutional Layer</p> <ul> <li>\\(B\\) : batch size</li> <li>\\(C_{\\text{in}}\\) : # of input channels</li> <li>\\(C_{\\text{out}}\\) : # of output channels</li> <li>\\(m, n\\) : # of vertical and horizontal indices of input</li> <li>\\(f_1, f_2\\) : # of vertical and horizontal indices of filter</li> </ul> <ul> <li>Input tensor : \\(X \\in \\mathbb{R}^{B \\times C_{\\text {in }} \\times m \\times n}\\)</li> <li>Output tensor : \\(Y \\in \\mathbb{R}^{B \\times C_{\\text {out }} \\times\\left(m-f_{1}+1\\right) \\times\\left(n-f_{2}+1\\right)}\\)</li> <li>Filter : \\(w \\in \\mathbb{R}^{C_{\\text {out }} \\times C_{\\text {in }} \\times f_{1} \\times f_{2}}\\)</li> <li>Bias : \\(b \\in \\mathbb{R}^{C_{\\text{out}}}\\)</li> </ul> <p>For \\(k = 1, \\dots, B, \\quad \\ell = 1, \\dots, C_{\\text{out}}, \\quad i = 1, \\dots, m-f_1+1, \\quad j = 1, \\dots, n-f_2+1\\):</p> \\[ Y_{k, \\ell, i, j}=\\sum_{\\gamma=1}^{c_{\\text {in }}} \\sum_{\\alpha=1}^{f_{1}} \\sum_{\\beta=1}^{f_{2}} w_{\\ell, \\gamma, \\alpha, \\beta} X_{k, \\gamma, i+\\alpha-1, j+\\beta-1}+b_{\\ell} \\] <p>Operation is independent across elements of the batch. The vertical and horizontal indices are referred to as spatial dimensions. If <code>bias=False</code>, then \\(b=0\\).</p> <p>Convolve a filter with an image : slide the filter spatially over the image and compute dot products.</p> <p>Take a \\(C_{\\text {in }} \\times f_{1} \\times f_{2}\\) chunk of the image and take the inner product with \\(w\\) and add bias \\(b\\).</p> <p>Example 5.4 : Example of 2D Convolutional Layer</p> <p> </p> <ul> <li>\\(B = 1, C_{\\text{in}} = 3, C_{\\text{out}} = 4, m=n=32, f_1=f_2=5\\)</li> <li>Input tensor : \\(X \\in \\mathbb{R}^{1 \\times 3 \\times 32 \\times 32}\\)</li> <li>Output tensor : \\(Y \\in \\mathbb{R}^{1 \\times 4 \\times 28 \\times 28}\\)</li> <li>Filter : \\(w \\in \\mathbb{R}^{4 \\times 3 \\times 5 \\times 5}\\)</li> <li>Bias : \\(b \\in \\mathbb{R}^{4}\\)</li> </ul> <p>Concept 5.5 : Zero Padding</p> <ul> <li>Problem</li> </ul> <p>Spatial dimension is reduced when passed through convolutional layers.</p> <p> </p> <p>\\((C \\times 7 \\times 7\\) image\\() \\circledast(C \\times 5 \\times 5\\) filter\\()=(1 \\times 3 \\times 3\\) feature map\\()\\). Spatial dimension 7 reduced to 3.</p> <ul> <li>Solution</li> </ul> <p>Zero padding on boundaries can preserve spatial dimension through convolutional layers.</p> <p> </p> <p>\\((C \\times 7 \\times 7\\) image with zero padding \\(=2) \\circledast(C \\times 5 \\times 5\\) filter\\()=(1 \\times 7 \\times 7\\) feature map\\()\\). Spatial dimension is preserved.</p> <p>Concept 5.6 : Stride</p> <p>The horizontal/vertical distance of two adjacent inner product calculations with the filter when sliding it across the image is called stride. It is originally set to 1, but can be adjusted.</p> <ul> <li>\\((7 \\times 7\\) image\\() \\circledast(3 \\times 3\\) filter\\()\\) with stride \\(1=(\\)output \\(5 \\times 5)\\)</li> <li> <p>\\((7 \\times 7\\) image\\() \\circledast(3 \\times 3\\) filter\\()\\) with stride \\(2=(\\)output \\(3 \\times 3)\\)</p> <p> </p> </li> <li> <p>\\((7 \\times 7\\) image with zero padding \\(=1) \\circledast(3 \\times 3\\) filter\\()\\) with stride \\(3=(\\)output \\(3 \\times 3)\\)</p> </li> </ul> <p>Concept 5.7 : Convolution Summary</p> <ul> <li>Input tensor : \\(C_{\\text {in}} \\times W_{\\text {in}} \\times H_{\\text {in}}\\)</li> <li> <p>Convolution Layer parameters</p> <ul> <li>\\(C_{\\text {out}}\\) filters, each of \\(C_{\\text {in}} \\times F \\times F\\)</li> <li>Stride : \\(S\\)</li> <li>Padding : \\(P\\)</li> </ul> </li> <li> <p>Output tensor : \\(C_{\\text {out}} \\times W_{\\text {out}} \\times H_{\\text {out}}\\)</p> \\[ \\begin{aligned} &amp; W_{\\text{out}}=\\left\\lfloor\\frac{W_{\\text{in}}-F+2 P}{S}+1\\right\\rfloor \\\\ &amp; H_{\\text{out}}=\\left\\lfloor\\frac{H_{\\text{in}}-F+2 P}{S}+1\\right\\rfloor \\end{aligned} \\] <p>To avoid the complication of this floor operation, it is best to ensure the formula inside evaluates to an integer.</p> </li> <li> <p>Number of trainable parameters : \\(F^2 C_{\\text{in}} C_{\\text{out}}\\) (filters) \\(+ C_{\\text{out}}\\) (biases)</p> </li> </ul> <p>Concept 5.8 : Pooling</p> <p>Pooling is primarily used to reduce spatial dimension. Similar to convolution. Pooling operates over each channel independently.</p> <p> </p> <p> </p> <p>Concept 5.9 : Weight Sharing</p> <p>In neural networks, weight sharing is a way to reduce the number of parameters by reusing the same parameter in multiple operations. Convolutional layers are the primary example.</p> \\[ A_{w}=\\left[\\begin{array}{cccccccc} w_{1} &amp; \\cdots &amp; w_{r} &amp; 0 &amp; \\cdots &amp; &amp; &amp; 0 \\\\ 0 &amp; w_{1} &amp; \\cdots &amp; w_{r} &amp; 0 &amp; \\cdots &amp; &amp; 0 \\\\ 0 &amp; 0 &amp; w_{1} &amp; \\cdots &amp; w_{r} &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; &amp; &amp; \\ddots &amp; &amp; \\ddots &amp; &amp; \\vdots \\\\ 0 &amp; &amp; \\cdots &amp; 0 &amp; w_{1} &amp; \\cdots &amp; w_{r} &amp; 0 \\\\ 0 &amp; &amp; \\cdots &amp; 0 &amp; 0 &amp; w_{1} &amp; \\cdots &amp; w_{r} \\end{array}\\right] \\] <p>If we consider convolution with filter \\(w\\) as a linear operator, the components of \\(w\\) appear may times in the matrix representation. This is because the same \\(w\\) is reused for every patch in the convolution. Weight sharing in convolution may now seem obvious, but it was a key contribution back when the LeNet architecture was presented.</p> <p>Some models (not studied in this course) use weight sharing more explicitly in other ways.</p> <p>Concept 5.10 : Geometric Deep Learning</p> <p>More generally, given a group \\(\\mathcal{G}\\) encoding a symmetry or invariance, one can define operations \"equivariant\" with respect \\(\\mathcal{G}\\) and construct equivariant neural networks.</p> <p>This is the subject of geometric deep learning, and its formulation utilizes graph theory and group theory.</p> <p>Geometric deep learning is particularly useful for non-Euclidean data. Examples include as protein molecule data and social network service connections.</p>"},{"location":"books-and-courses/mfdnn/format/6/","title":"\u00a7 6. Foundations of Design and Training of Deep Neural Networks","text":""},{"location":"books-and-courses/mfdnn/format/6/#data-augmentation","title":"Data Augmentation","text":"<p>Definition 6.1 : Spurious Correlation</p> <p>Hypothetical: A photographer prefers to take pictures with cats looking to the left and dogs looking to the right. Neural network learns to distinguish cats from dogs by which direction it is facing. This learned correlation will not be useful for pictures taken by another photographer.</p> <p>This is a spurious correlation, a correlation between the data and labels that does not capture the \"true\" meaning. Spurious correlations are not robust in the sense that the spurious correlation will not be a useful predictor when the data changes slightly.</p> <p>Definition 6.2 : Data Augmentation (DA)</p> <p> </p> <p>Translation invariance are encoded in convolution, but other invariances are harder to encode (unless one uses geometric deep learning). Therefore encode invariances in data and have neural networks learn the invariance.</p> <p>Data augmentation (DA) applies transforms to the data while preserving meaning and label.</p> <ul> <li> <p>Option 1: Enlarge dataset itself.  </p> <p>Usually cumbersome and unnecessary.</p> </li> <li> <p>Option 2: Use randomly transformed data in training loop.  </p> <p>In PyTorch, we use <code>Torchvision.transforms</code>.</p> </li> </ul> <p>We use DA to :</p> <ul> <li>Inject our prior knowledge of the structure of the data and force the neural network to learn it.</li> <li>Remove spurious correlations.</li> <li>Increase the effective data size. In particular, we ensure neural network never encounters the exact same data again and thereby prevent the neural network from performing exact memorization. (Neural network can memorize quite well.)</li> </ul> <p>Effects of DA :</p> <ul> <li>DA usually worsens the training error (but we don't care about training error).</li> <li> <p>DA often, but not always, improves the test error.  </p> <p>If DA removes a spurious correlation, then the test error can be worsened.</p> </li> <li> <p>DA usually improves robustness.</p> </li> </ul>"},{"location":"books-and-courses/mfdnn/format/6/#overfitting-underfitting","title":"Overfitting &amp; Underfitting","text":"<p>Definition 6.3 : Classical Statistics - Overfitting vs Underfitting</p> <p> </p> <p> </p> <p>Given separate train and test data</p> <ul> <li>When (training loss) &lt;&lt; (testing loss) you are overfitting. What you have learned from the training data does not carry over to test data.</li> <li>When (training loss) \\(\\approx\\) (testing loss) you are underfitting. You have the potential to learn more from the training data.</li> </ul> <p>The goal of ML is to learn patterns that generalize to data you have not seen. From each datapoint, you want to learn enough (don't underfit) but if you learn too much you overcompensate for an observation specific to the single experience.</p> <p>In classical statistics, underfitting vs. overfitting (bias vs. variance tradeoff) is characterized rigorously.</p> <p>Definition 6.4 : Modern Deep Learning - Double Descent</p> <p>In modern deep learning, you can overfit, but the state-of-the art neural networks do not overfit (or \"benignly overfit\") despite having more model parameters than training data.</p> <p>We do not yet have clarity with this new phenomenon called double descent. When overfitting happens and when it does not is unclear.</p> <p> </p> <p>Example 6.5 : Double Descent on 2-Layer Neural Network on MNIST</p> <p>Belkin et al. experimentally demonstrates the double descent phenomenon with an MLP trained on the MNIST dataset.</p> <p> </p> <p>(M. Belkin, D. Hsu, S. Ma, and S. Mandal, Reconciling modern machine-learning practice and the classical bias-variance trade-off, PNAS, 2019.)</p> <p>Concept 6.6 : How to Avoid Overfitting</p> <p>Regularization is loosely defined as mechanisms to prevent overfitting.</p> <p>When you are overfitting, regularize with:</p> <ul> <li>Smaller NN (fewer parameters) or larger NN (more parameters).</li> <li> <p>Improve data by:</p> <ul> <li>using data augmentation</li> <li>acquiring better, more diverse, data</li> <li>acquiring more of the same data</li> </ul> </li> <li> <p>Weight decay</p> </li> <li>Dropout</li> <li>Early stopping on SGD or late stopping on SGD</li> </ul> <p>Concept 6.7 : How to Avoid Underfitting</p> <p>When you are underfitting, use:</p> <ul> <li>Larger NN (if computationally feasible)</li> <li>Less weight decay</li> <li>Less dropout</li> <li>Run SGD longer (if computationally feasible)</li> </ul> <p>Concept 6.8 : Summary of Overfitting vs Underfitting</p> <p>In modern deep learning, the double descent phenomenon has brought a conceptual and theoretical crisis regarding over and underfitting. Much of the machine learning practice is informed by classical statistics and learning theory, which do not take the double descent phenomenon into account.</p> <p>Double descent will bring fundamental changes to statistics, and researchers need more time to figure things out. Most researchers, practitioners and theoreticians, agree that not all classical wisdom is invalid, but what part do we keep, and what part do we replace?</p> <p>In the meantime, we will have to keep in mind the two contradictory viewpoints and move forward in the absence of clarity.</p>"},{"location":"books-and-courses/mfdnn/format/6/#weight-decay","title":"Weight Decay","text":"<p>Definition 6.9 : \\(\\ell^{2}\\) - Regularization</p> <p>\\(\\ell^{2}\\)-regularization augments the loss function with</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(f_{\\theta}\\left(x_{i}\\right), y_{i}\\right)+\\frac{\\lambda}{2}\\|\\theta\\|^{2} \\] <p>SGD on the augmented loss is usually implemented by changing SGD update rather than explicitly changing the loss since</p> \\[ \\begin{gathered} \\theta^{k+1}=\\theta^{k}-\\alpha\\left(g^{k}+\\lambda \\theta^{k}\\right) \\\\ =(1-\\alpha \\lambda) \\theta^{k}-\\alpha g^{k} \\end{gathered} \\] <p>Where \\(g^{k}\\) is stochastic gradient of original (unaugmented) loss.</p> <p>In classical statistics, this is called ridge regression or maximum a posteriori (MAP) estimation with Gaussian prior.</p> <p>Concept 6.10 : Weight Decay \\(\\cong \\ell^{2}\\) - Regularization</p> <p>In Pytorch, you can use SGD + weight decay by:</p> <p>augmenting the loss function</p> <pre><code>for param in model.parameters():\n    loss += (lamda/2)*param.pow(2.0).sum()\ntorch.optim.SGD(model.parameters(), lr=... , weight_decay=0)\n</code></pre> <p>or by using <code>weight_decay</code> in the optimizer</p> <pre><code>torch.optim.SGD(model.parameters(), lr=... , weight_decay=lamda)\n</code></pre> <p>For plain SGD, weight decay and \\(\\ell^{2}\\)-regularization are equivalent. For other optimizers, the two are similar but not the same. More on this later.</p>"},{"location":"books-and-courses/mfdnn/format/6/#dropout","title":"Dropout","text":"<p>Definition 6.11 : Dropout</p> <p>Dropout is a regularization technique that randomly disables neurons.</p> <p>Standard layer,</p> \\[ h_{2}=\\sigma\\left(W_{1} h_{1}+b_{1}\\right) \\] <p>Dropout with drop probability \\(p\\) defines</p> \\[ h_{2}=\\sigma\\left(W_{1} h_{1}^{\\prime}+b_{1}\\right) \\] <p>with \\(h_{1}^{\\prime}\\) defined as</p> \\[ \\left(h_{1}^{\\prime}\\right)_{j}= \\begin{cases}0 &amp; \\text { with probability } p \\\\ \\frac{\\left(h_{1}\\right)_{j}}{1-p} &amp; \\text { otherwise }\\end{cases} \\] <p>Note that \\(h_{1}^{\\prime}\\) is defined so that \\(\\mathbb{E}[h_{1}^{\\prime}]=h_1\\).</p> <p> </p> <p>During training, dropout masks are different in every forward pass due to their random nature.</p> <p>(N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov, Dropout: A Simple Way to Prevent Neural Networks from Overfitting, JMLR, 2014.)</p> <p>Concept 6.12 : Why is dropout helpful?</p> <p>\"A motivation for dropout comes from a theory of the role of sex in evolution (Livnat et al., 2010).\"</p> <p>Sexual reproduction, compared to asexual reproduction, creates the criterion for natural selection mix-ability of genes rather than individual fitness, since genes are mixed in a more haphazard manner.</p> <p>\"Since a gene cannot rely on a large set of partners to be present at all times, it must learn to do something useful on its own or in collaboration with a small number of other genes. ... Similarly, each hidden unit in a neural network trained with dropout must learn to work with a randomly chosen sample of other units. This should make each hidden unit more robust and drive it towards creating useful features on its own without relying on other hidden units to correct its mistakes.</p> <p>The analogy to evolution is very interesting, but it is ultimately a heuristic argument. It also shifts the burden to the question: \"why is sexual evolution more powerful than asexual evolution?\"</p> <p>However, dropout can be shown to be loosely equivalent to \\(\\ell^{2}\\)-regularization. However, we do not yet have a complete understanding of the mathematical reason behind dropout's performance.</p> <p>Concept 6.13 : Dropout in Pytorch</p> <p>Dropout simply multiplies the neurons with a random \\(0-\\frac{1}{1-p_{\\text {drop }}}\\) mask.</p> <p>A direct implementation in PyTorch:</p> <pre><code>def dropout_layer(X, p_drop):\n    mask = (torch.rand(X.shape) &gt; p_drop).float()\n    return mask * X / (1.0 - p_drop)\n</code></pre> <p>PyTorch provides an implementation of dropout through <code>torch.nn.Dropout</code>.</p> <p>Concept 6.14 : Dropout in Training vs Test</p> <p>Typically, dropout is used during training and turned off during prediction/testing. (Dropout should be viewed as an additional onus imposed during training to make training more difficult and thereby effective, but it is something that should be turned off later.)</p> <p>In PyTorch, activate the training mode with</p> <pre><code>model.train()\n</code></pre> <p>and activate evaluation mode with</p> <pre><code>model.eval()\n</code></pre> <p>dropout (and batchnorm) will behave differently in these two modes.</p> <p>Concept 6.15 : When to Use Dropout</p> <p>Dropout is usually used on linear layers but not on convolutional layers.</p> <ul> <li>Linear layers have many weights and each weight is used only once per forward pass. (If \\(y=\\operatorname{Linear}_{A, b}(x)\\), then \\(A_{i j}\\) only affect \\(y_{i}\\).) So regularization seems more necessary.</li> <li>A convolutional filter has fewer weights and each weight is used multiple times in each forward pass. (If \\(y=\\operatorname{Conv} 2 \\mathrm{D}_{w, b}(x)\\), then \\(w_{i j k t}\\) affects \\(\\left.y_{i, .,:}.\\right)\\) So regularization seems less necessary.</li> </ul> <p>Dropout seems to be going out of fashion:</p> <ul> <li>Dropout's effect is somehow subsumed by batchnorm. (This is poorly understood.)</li> <li>Linear layers are less common due to their large number of trainable parameters.</li> </ul> <p>There is no consensus on whether dropout should be applied before or after the activation function. However, Dropout- \\(\\sigma\\) and \\(\\sigma\\)-Dropout are equivalent when \\(\\sigma\\) is \\(\\operatorname{ReLU}\\) or leaky \\(\\operatorname{ReLU}\\), or, more generally, when \\(\\sigma\\) is nonnegative homogeneous.</p>"},{"location":"books-and-courses/mfdnn/format/6/#sgd-early-late-stopping","title":"SGD Early / Late Stopping","text":"<p>Definition 6.16 : SGD Early Stopping</p> <p>Early stopping of SGD refers to stopping the training early even if you have time for more iterations.</p> <p>The rationale is that SGD fits data, so too many iterations lead to overfitting.</p> <p>A similar phenomenon (too many iterations hurt) is observed in classical algorithms for inverse problems.</p> <p> </p> <p>Definition 6.17 : Epochwise Double Descent</p> <p>Recently, however, an epochwise double descent has been observed.</p> <p>So perhaps one should stop SGD early or very late.</p> <p>We do not yet have clarity with this new phenomenon.</p> <p> </p> <p>(P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak, and I. Sutskever, Deep double descent: Where bigger models and more data hurt, ICLR, 2020.)</p>"},{"location":"books-and-courses/mfdnn/format/6/#more-data","title":"More Data","text":"<p>Concept 6.18 : More Data (by Data Augmentation)</p> <p>With all else fixed, using more data usually leads to less overfitting.</p> <p>However, collecting more data is often expensive.</p> <p>Think of data augmentation (DA) as a mechanism to create more data for free. You can view DA as a form of regularization.</p>"},{"location":"books-and-courses/mfdnn/format/6/#sgd-optimizer","title":"SGD Optimizer","text":"<p>Definition 6.19 : SGD with Momentum</p> <p>SGD:</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha g^{k} \\] <p>SGD with momentum:</p> \\[ \\begin{gathered} v^{k+1}=g^{k}+\\beta v^{k} \\\\ \\theta^{k+1}=\\theta^{k}-\\alpha v^{k+1} \\end{gathered} \\] <p>\\(\\beta=0.9\\) is a common choice.</p> <p> </p> <p>When different coordinates (parameters) have very different scalings (i.e., when the problem is ill-conditioned, momentum can help find a good direction of progress.</p> <p>(I. Sutskever, J. Martens, G. Dahl, and G. Hinton, On the importance of initialization and momentum in deep learning, ICML, 2013.)</p> <p>Definition 6.20 : RMSProp</p> <p>RMSProp:</p> \\[ \\begin{gathered} m_{2}^{k+1}=\\beta_{2} m_{2}^{k}+\\left(1-\\beta_{2}\\right)\\left(g^{k} \\circledast g^{k}\\right) \\\\ \\theta^{k+1}=\\theta^{k}-\\alpha g^{k} \\oslash \\sqrt{m_{2}^{k+1}+\\epsilon} \\end{gathered} \\] <p>\\(\\beta_{2}=0.99\\) and \\(\\epsilon=10^{-8}\\) are common values. \\(\\circledast\\) and \\(\\oslash\\) are elementwise mult. and div.</p> <p>\\(m_{2}^{k}\\) is a running estimate of the \\(2^{\\text {nd }}\\) moment of the stochastic gradients, i.e., \\(\\left(m_{2}^{k}\\right)_{i} \\approx \\mathbb{E}\\left(g^{k}\\right)_{i}^{2}\\).</p> <p>\\(\\alpha \\oslash \\sqrt{m_{2}^{k+1}+\\epsilon}\\) is the learning rate scaled elementwise. Progress along steep and noisy directions are dampened while progress along flat and non-noisy directions are accelerated.</p> <p>(T. Tieleman, and G. Hinton, Lecture 6.5 - RMSProp, COURSERA: Neural Networks for Machine Learning, 2012.)</p> <p>Definition 6.21 : Adam (Adaptive Moment Estimation)</p> <p>Adam:</p> \\[ \\begin{gathered} m_{1}^{k+1}=\\beta_{1} m_{1}^{k}+\\left(1-\\beta_{1}\\right) g^{k}, m_{2}^{k+1}=\\beta_{2} m_{2}^{k}+\\left(1-\\beta_{2}\\right)\\left(g^{k} \\circledast g^{k}\\right) \\\\ \\tilde{m}_{1}^{k+1}=\\frac{m_{1}^{k+1}}{1-\\beta_{1}^{k+1}}, \\quad \\widetilde{m}_{2}^{k+1}=\\frac{m_{2}^{k+1}}{1-\\beta_{2}^{k+1}} \\\\ \\theta^{k+1}=\\theta^{k}-\\alpha \\widetilde{m}_{1}^{k+1} \\oslash \\sqrt{\\widetilde{m}_{2}^{k+1}+\\epsilon} \\end{gathered} \\] <ul> <li>\\(\\beta_{1}^{k+1}\\) means \\(\\beta_{1}\\) to the \\((k+1)\\) th power.</li> <li>\\(\\beta_{1}=0.9, \\beta_{2}=0.999\\), and \\(\\epsilon=10^{-8}\\) are common values. Initialize with \\(m_{1}^{0}=m_{2}^{0}=0\\).</li> <li>\\(m_{1}^{k}\\) and \\(m_{2}^{k}\\) are running estimates of the \\(1^{\\text {st }}\\) and \\(2^{\\text {nd }}\\) moments of \\(g^{k}\\).</li> <li>\\(\\tilde{m}_{1}^{k}\\) and \\(\\tilde{m}_{2}^{k}\\) are bias-corrected estimates of \\(m_{1}^{k}\\) and \\(m_{2}^{k}\\).</li> <li>Using \\(\\widetilde{m}_{1}^{k}\\) instead of \\(g^{k}\\) adds the effect of momentum.</li> </ul> <p>(D. P. Kingma and J. Ba, Adam: A method for stochastic optimization, ICLR, 2015.)</p> <p>Concept 6.22 : Bias correction of Adam</p> <p>To understand the bias correction, consider the hypothetical \\(g^{k}=g\\) for \\(k=0,1, \\ldots\\). Then</p> \\[ \\begin{gathered} m_{1}^{k}=\\left(1-\\beta_{1}^{k}\\right) g \\\\ m_{2}^{k}=\\left(1-\\beta_{2}^{k}\\right)(g \\circledast g) \\end{gathered} \\] <p>Even though \\(m_{1}^{k} \\rightarrow g\\) and \\(m_{2}^{k} \\rightarrow(g \\circledast g)\\) as \\(k \\rightarrow \\infty\\), the estimators are not exact despite there being no variation in \\(g^{k}\\).</p> <p>On the other hand, the bias-corrected estimators are exact:</p> \\[ \\begin{gathered} \\widetilde{m}_{1}^{k}=g \\\\ \\widetilde{m}_{2}^{k}=(g \\circledast g) \\end{gathered} \\] <p>Concept 6.23 : The Cautionary Tale of Adam</p> <p>Adam's original 2015 paper justified the effectiveness of the algorithm through experiments training deep neural networks with Adam. After all, this non-convex optimization is what Adam was proposed to do.</p> <p>However, the paper also provided a convergence proof under the assumption of convexity. This was perhaps unnecessary in an applied paper focusing on non-convex optimization.</p> <p>The proof was later shown to be incorrect! Adam does not always converge in the convex setup, i.e., the algorithm, rather than the proof, is wrong.</p> <p>Reddi and Kale presented the AMSGrad optimizer, which does come with a correct convergence proof, but AMSGrad tends to perform worse than Adam, empirically.</p> <p>(S. J. Reddi, S. Kale, and S. Kumar, On the convergence of Adam and beyond, ICLR, 2018.)</p> <p>Concept 6.24 : How to Choose Optimizer</p> <p>Extensive research has gone into finding the \"best\" optimizer. Schmidt et al.\\({ }^{\\star}\\) reports that, roughly speaking, that Adam works well most of the time.</p> <p>So, Adam is a good default choice. Currently, it seems to be the best default choice.</p> <p>However, Adam does not always work. For example, it seems to be that the widely used EfficientNet model can only be trained \\({ }^{\\dagger}\\) with RMSProp.</p> <p>However, there are some setups where the LR of SGD is harder to tune, but SGD outperforms Adam when properly tuned.\\({ }^{\\#}\\)</p> <p>(\\({ }^{\\star}\\) R. M. Schmidt, F. Schneider, and P. Hennig, Descending through a crowded valley \u2014 benchmarking deep learning optimizers, ICML, 2021. \\({ }^{\\dagger}\\) M. Tan and Q. V. Le, EfficientNet: Rethinking model scaling for convolutional neural networks, ICML, 2019. \\({ }^{\\#}\\) A. C. Wilson, R. Roelofs, M. Stern, N. Srebro, and B. Recht, The marginal value of adaptive gradient methods in machine learning, NeurlPS, 2017.)</p> <p>Concept 6.25 : How to Tune Parameters</p> <p>Everything should be chosen by trial and error. The weight parameters and \\(\\beta, \\beta_{1}, \\beta_{2}\\) and the weight decay parameter \\(\\lambda\\), and the optimizers should be chosen based on trial and error.</p> <p>The LR (the stepsize \\(\\alpha\\) ) of different optimizers are not really comparable between the different optimizers. When you change the optimizer, the LR should be tuned again.</p> <p>Roughly, large stepsize, large momentum, small weight decay is faster but less stable, while small stepsize, small momentum, and large weight decay is slower but more stable.</p> <p>Concept 6.26 : Using Different Optimizers in Pytorch</p> <p>In PyTorch, the <code>torch.optim</code> module implements the commonly used optimizers.</p> <ul> <li>Using SGD:</li> </ul> <pre><code>torch.optim.SGD(model.parameters(), lr=X)\n</code></pre> <ul> <li>Using SGD with momentum:</li> </ul> <pre><code>torch.optim.SGD(model.parameters(), momentum=0.9, lr=X)\n</code></pre> <ul> <li>Using RMSprop:</li> </ul> <pre><code>torch.optim.RMSprop(model.parameters(), lr=X)\n</code></pre> <ul> <li>Using Adam:</li> </ul> <pre><code>torch.optim.Adam(model.parameters(), lr=X)\n</code></pre> <p>Concept 6.27 : Learning Rate Scheduling</p> <p>Sometimes, it is helpful to change (usually reduce) the learning rate as the training progresses. PyTorch provides learning rate schedulers to do this.</p> <pre><code>optimizer = SGD(model.parameters(), lr=0.1)\nscheduler = ExponentialLR(optimizer, gamma=0.9) # lr = 0.9*lr\nfor _ in range(...):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler.step() # .step() call updates (changes) the learning rate\n</code></pre> <p>One common choice is to specify a diminishing learning rate via a function (a lambda expression). Choices like <code>C/epoch</code> or <code>C / sqrt(iteration)</code>, where <code>C</code> is an appropriately chosen constant, are common.</p> <pre><code># lr_lambda allows us to set lr with a function\nscheduler = LambdaLR(optimizer, lr_lambda = lambda ep: 1e-2/ep)\nfor epoch in range(...):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler.step() # lr=0.01/epoch\n</code></pre> <p>Concept 6.28 : Cosine Learning Rate</p> <p>The cosine learning rate scheduler, which sets the learning rate with the cosine function, is also commonly used.</p> <p>The \\(2^{\\text {nd }}\\) case in the specification means \\(k\\) and its purpose is to prevent the learning rate from becoming 0 .</p> <p>It is also common to use only a half-period of the cosine rather than having the learning rate oscillate.</p> <p> </p> <p>(I. Loshchilov and F. Hutter, SGDR: Stochastic gradient descent with warm restarts, ICLR, 2017)</p> <p>Concept 6.29 : Wide vs Sharp Minima</p> <ul> <li>Large step makes large and rough progress towards regions with small loss.</li> <li>Small steps refines the model by finding sharper minima.</li> </ul> <p>Also small steps better suppress the effect of noise. Mathematically, one can show that SGD with small steps becomes very similar to GD with small steps.\\({ }^{\\#}\\)</p> <p>However, using small steps to converge to sharp minima may not always be optimal. There is some empirical evidence that wide minima have better test error than sharp minima.\\({ }^{\\star}\\)</p> <p>(\\({ }^{\\#}\\) D. Davis, D. Drusvyatskiy, S. Kakade and J. D. Lee, Stochastic subgradient method converges on tame functions, Found. Comput. Math., 2020. \\({ }^{\\star}\\) Y. Jiang, B. Neyshabur, H. Mobahi, D. Krishnan, and S. Bengio, Fantastic generalization measures and where to find them, ICLR, 2020.)</p>"},{"location":"books-and-courses/mfdnn/format/6/#weight-initialization","title":"Weight Initialization","text":"<p>Concept 6.30 : Importance of Weight Initialization</p> <p>Remember, SGD is</p> \\[ \\theta^{k+1}=\\theta^{k}-\\alpha g^{k} \\] <p>where \\(\\theta^{0} \\in \\mathbb{R}^{p}\\) is an initial point. Using a good initial point is important in NN training.</p> <p>Prescription by LeCun et al.: \"Weights should be chosen randomly but in such a way that the [tanh] is primarily activated in its linear region. If weights are all very large then the [tanh] will saturate resulting in small gradients that make learning slow. If weights are very small then gradients will also be very small.\" (Cf. Vanishing gradient)</p> <p>\"Intermediate weights that range over the [tanh's] linear region have the advantage that (1) the gradients are large enough that learning can proceed and (2) the network will learn the linear part of the mapping before the more difficult nonlinear part.\"</p> <p>(Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. M\u00fcller. Efficient BackProp, In: G. Montavon, G. B. Orr, and K.-R. M\u00fcller. (eds), Neural Networks: Tricks of the Trade, 1998.)</p> <p>Concept 6.31 : Mathematics Review</p> <ul> <li> <p>Using the \\(1^{\\text {st }}\\) order Taylor approximation,</p> \\[ \\tanh (z) \\approx z \\] </li> <li> <p>Write \\(X \\sim \\mathcal{N}\\left(\\mu, \\sigma^{2}\\right)\\) to denote that \\(X\\) is a Gaussian (normal) random variable with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).</p> </li> <li> <p>If \\(X\\) and \\(Y\\) are random variables, with expected values \\(\\mu_X, \\mu_Y\\) and standard deviations \\(\\sigma_X, \\sigma_Y\\), the following properties hold.</p> \\[ \\text{Cov}(X, Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] = \\mathbb{E}[(X-\\mu_X)(Y-\\mu_Y)] \\] \\[ \\text{Corr}(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y} \\] \\[ \\mathbb{E}[X+Y] = \\mathbb{E}[X] + \\mathbb{E}[Y] \\] \\[ \\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\text{Cov}(X, Y) \\] \\[ \\mathbb{E}[aX+b] = a\\mathbb{E}[X] + b \\] \\[ \\text{Var}[aX+b] = a^2 \\text{Var}[X] \\] </li> <li> <p>If \\(X\\) and \\(Y\\) are random variables, such that</p> \\[ \\text{Cov}(X, Y)=\\text{Corr}(X, Y)=0 \\] <p>\\(X\\) and \\(Y\\) are uncorrelated random variables, and following properties hold.</p> \\[ \\mathbb{E}[XY] = \\mathbb{E}[X] \\mathbb{E}[Y] \\] \\[ \\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y) \\] </li> <li> <p>If \\(X\\) and \\(Y\\) are random variables, with probability density function \\(f_X(x), f_Y(y)\\) and joint probability density funciton \\(f_{X, Y}(x, y)\\), such that</p> \\[ f_{X, Y}(x, y) = f_X(x)f_Y(y) \\] <p>\\(X\\) and \\(Y\\) are independent random variables, and following properties hold.</p> \\[ \\mathbb{E}[X^n Y^m] = \\mathbb{E}[X^n] \\mathbb{E}[Y^m] \\] \\[ \\text{Cov}(X, Y) = \\text{Corr}(X, Y) = 0 \\] \\[ \\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y) \\] \\[ \\text{Var}(XY) = \\text{Var}(X)\\text{Var}(Y) + \\text{Var}(X)\\mathbb{E}[Y]^2 + \\text{Var}(Y)\\mathbb{E}[X]^2 \\] \\[ \\text{Var}(XY) = \\text{Var}(X)\\text{Var}(Y) \\quad (\\text{if} \\ \\mathbb{E}[X]=\\mathbb{E}[Y]=0) \\] </li> <li> <p>If \\(X\\) and \\(Y\\) are independent, then \\(X\\) and \\(Y\\) are uncorrelated. The converse does not hold.</p> </li> <li>IID means, \"independent and identically distributed\" random variables.</li> </ul> <p>Definition 6.32 : LeCun Initialization</p> <p>Consider the layer</p> \\[ \\begin{gathered} \\tilde{y}=A x+b \\\\ y=\\tanh (\\tilde{y}) \\end{gathered} \\] <p>where \\(x \\in \\mathbb{R}^{n_{\\text {in }}}\\) and \\(y, \\tilde{y} \\in \\mathbb{R}^{n_{\\text {out }}}\\). Assume \\(x_{j}\\) have mean \\(=0\\), variance \\(=1\\) and are uncorrelated. If we initialize \\(A_{i j} \\sim \\mathcal{N}\\left(0, \\sigma_{A}^{2}\\right)\\) and \\(b_{i} \\sim \\mathcal{N}\\left(0, \\sigma_{b}^{2}\\right)\\), IID, then</p> \\[ \\begin{aligned} &amp; \\tilde{y}_{i}=\\sum_{j=1}^{n_{\\mathrm{in}}} A_{i j} x_{j}+b_{i} \\quad \\text { has mean }=0 \\text {, variance }=n_{\\mathrm{in}} \\sigma_{A}^{2}+\\sigma_{b}^{2} \\\\ &amp; y_{i}=\\tanh \\left(\\tilde{y}_{i}\\right) \\approx \\tilde{y}_{i} \\quad \\text { has mean } \\approx 0 \\text {, variance } \\approx n_{\\mathrm{in}} \\sigma_{A}^{2}+\\sigma_{b}^{2} \\end{aligned} \\] <p>If we choose</p> \\[ \\sigma_{A}^{2}=\\frac{1}{n_{\\text {in }}}, \\quad \\sigma_{b}^{2}=0, \\] <p>(so \\(b=0\\) ) then we have \\(y_{i}\\) mean \\(\\approx 0\\) variance \\(\\approx 1\\) and are uncorrelated.</p> <p>By induction, with an \\(L\\)-layer MLP,</p> <ul> <li>if the input to has mean \\(=0\\) variance \\(=1\\) and uncorrelated elements,</li> <li>the weights and biases are initialized with \\(A_{i j} \\sim \\mathcal{N}\\left(0, \\frac{1}{n_{\\text {in }}}\\right)\\) and \\(b_{i}=0\\), and</li> <li>the linear approximations \\(\\tanh (z) \\approx z\\) are valid,</li> </ul> <p>then we can expect the output layer to have mean \\(\\approx 0\\), variance \\(\\approx 1\\).</p> <p>(Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. M\u00fcller. Efficient BackProp, In: G. Montavon, G. B. Orr, and K.-R. M\u00fcller. (eds), Neural Networks: Tricks of the Trade, 1998.)</p> <p>Definition 6.33 : Xavier Initialization</p> <p>Consider the layer</p> \\[ \\begin{gathered} \\tilde{y}=A x+b \\\\ y=\\tanh (\\tilde{y}) \\end{gathered} \\] <p>where \\(x \\in \\mathbb{R}^{n_{\\text {in }}}\\) and \\(y, \\tilde{y} \\in \\mathbb{R}^{n_{\\text {out }}}\\). Consider the gradient with respect to some loss \\(\\ell(y)\\). Assume \\(\\left(\\frac{\\partial \\ell}{\\partial y}\\right)_{i}\\) have mean \\(=0\\), variance \\(=1\\) and are uncorrelated. Then</p> \\[ \\frac{\\partial y}{\\partial x}=\\operatorname{diag}\\left(\\tanh ^{\\prime}(A x+b)\\right) A \\approx A \\] <p>if \\(\\tanh (\\tilde{y}) \\approx \\tilde{y}\\) and</p> \\[ \\frac{\\partial \\ell}{\\partial x}=\\frac{\\partial \\ell}{\\partial y} A \\] <p>If we initialize \\(A_{i j} \\sim \\mathcal{N}\\left(0, \\sigma_{A}^{2}\\right)\\) and \\(b_{i} \\sim \\mathcal{N}\\left(0, \\sigma_{b}^{2}\\right)\\), IID, and assume that \\(\\frac{\\partial \\ell}{\\partial y}\\) and \\(A\\) are independent, then</p> \\[ \\left(\\frac{\\partial \\ell}{\\partial x}\\right)_{j}=\\sum_{i=1}^{n_{\\text {out }}}\\left(\\frac{\\partial \\ell}{\\partial y}\\right)_{i} A_{i j} \\text { has mean } \\approx 0 \\text { and variance } \\approx n_{\\text {out }} \\sigma_{A}^{2} \\] <p>If we choose</p> \\[ \\sigma_{A}^{2}=\\frac{1}{n_{\\mathrm{out}}} \\] <p>then \\(\\left(\\frac{\\partial \\ell}{\\partial x}\\right)_{j}\\) have mean \\(\\approx 0\\), variance \\(\\approx 1\\) and are uncorrelated.</p> <p>\\(\\frac{\\partial \\ell}{\\partial y}\\) and \\(A\\) are not independent; \\(\\frac{\\partial \\ell}{\\partial y}\\) depends on the forward evaluation, which in turn depends on \\(A\\). Nevertheless, the calculation is an informative exercise and its result seems to be representative of common behavior.</p> <p>If \\(y=\\tanh (A x+b)\\) is an early layer (close to input) in a deep neural network, then the randomness of \\(A\\) is diluted through the forward and backward propagation and \\(\\frac{\\partial \\ell}{\\partial y}\\) and \\(A\\) will be nearly independent.</p> <p>If \\(y=\\tanh (A x+b)\\) is an later layer (close to output) in a deep neural network, then \\(\\frac{\\partial \\ell}{\\partial y}\\) and \\(A\\) will have strong dependency.</p> <p>Consideration of forward and backward passes result in different prescriptions.</p> <p>The Xavier initialization uses the harmonic mean of the two:</p> \\[ \\sigma_{A}^{2}=\\frac{2}{n_{\\mathrm{in}}+n_{\\mathrm{out}}}, \\quad \\sigma_{b}^{2}=0 \\] <p>In the literature, the alternate notation \\(\\text{fan}_{\\text {in }}\\) and \\(\\text{fan}_{\\text {out }}\\) are often used instead of \\(n_{\\text {in }}\\) and \\(n_{\\text {out }}\\). The fan-in and fan-out terminology originally refers to the number of electric connections entering and exiting a circuit or an electronic device.</p> <p>(Xavier Glorot and Y. Bengio, Understanding the difficulty of training deep feedforward neural networks, AISTATS, 2010.)</p> <p>Definition 6.34 : (Kaiming) He Initialization</p> <p>Consider the layer</p> \\[ y=\\operatorname{ReLU}(A x+b) \\] <p>We cannot use the Taylor expansion with ReLU.</p> <p>However, a similar line of reasoning with the forward pass gives rise to</p> \\[ \\sigma_{A}^{2}=\\frac{2}{n_{\\mathrm{in}}} \\] <p>And a similar consideration with backprop gives rise to</p> \\[ \\sigma_{A}^{2}=\\frac{2}{n_{\\text {out }}} \\] <p>In PyTorch, use <code>mode='fan_in'</code> and <code>mode='fan_out'</code> to toggle between the two modes.</p> <p>(Kaiming He, X. Zhang, S. Ren, and J. Sun, Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification, ICCV, 2015.)</p> <p>Concept 6.35 : Discussions on Initializations</p> <p>In the original description of the Xavier and He initializations, the biases are all initialized to 0 . However, the default initialization of Linear\\({ }^{\\star}\\) and Conv2d \\({ }^{\\#}\\) layers in PyTorch uses initialize the biases randomly. A documented reasoning behind this choice (in the form of papers or GitHub discussions) do not seem to exist.</p> <p>Initializing weights with the proper scaling is sometimes necessary to get the network to train, as you will see with the VGG network. However, so long as the network gets trained, the choice of initialization does not seem to affect the final performance.</p> <p>Since initializations rely on the assumption that the input to each layer has roughly unit variance, it is important that the data is scaled properly. This is why PyTorch dataloader scales pixel intensity values to be in \\([0,1]\\), rather than \\([0,255]\\).</p> <p>(\\({ }^{\\star}\\) https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html \\({ }^{\\#}\\) https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html)</p> <p>Definition 6.36 : Initialization for Convolutional Layer</p> <p>Consider the layer</p> \\[ \\begin{aligned} &amp; \\tilde{y}=\\operatorname{Conv} 2 \\mathrm{D}_{w, b}(x) \\\\ &amp; y=\\tanh (\\tilde{y}) \\\\ \\end{aligned} \\] <p>where \\(w \\in \\mathbb{R}^{C_{\\text {out }} \\times C_{\\text {in }} \\times f_{1} \\times f_{2}}\\) and \\(b \\in \\mathbb{R}^{C_{\\text {out }}}\\). Assume \\(x_{j}\\) have mean \\(=0\\) variance \\(=1\\) and are uncorrelated. If we initialize \\(w_{i j k \\ell} \\sim \\mathcal{N}\\left(0, \\sigma_{w}^{2}\\right)\\) and \\(b_{i} \\sim \\mathcal{N}\\left(0, \\sigma_{b}^{2}\\right)\\), IID, then</p> \\[ \\begin{aligned} &amp; \\tilde{y}_{i} \\quad \\text { has mean }=0 \\text { variance }=\\left(C_{\\text {in }} f_{1} f_{2}\\right) \\sigma_{w}^{2}+\\sigma_{b}^{2} \\\\ &amp; y_{i} \\approx \\tilde{y}_{i} \\text { has mean } \\approx 0 \\text { variance } \\approx\\left(C_{\\text {in }} f_{1} f_{2}\\right) \\sigma_{w}^{2}+\\sigma_{b}^{2} \\end{aligned} \\] <p>If we choose</p> \\[ \\sigma_{w}^{2}=\\frac{1}{c_{\\text {in }} f_{1} f_{2}}, \\quad \\sigma_{b}^{2}=0 \\] <p>(so \\(b=0\\) ) then we have \\(y_{i}\\) mean \\(\\approx 0\\) variance \\(\\approx 1\\) and are correlated.</p> <p>Outputs from a convolutional layer are correlated. The uncorrelated assumption is false. Nevertheless, the calculation is an informative exercise and its result seems to be representative of common behavior.</p> <p>Xavier and He initialization is usually used with</p> \\[ n_{\\mathrm{in}}=C_{\\mathrm{in}} f_{1} f_{2} \\] <p>and</p> \\[ n_{\\text {out }}=C_{\\text {out }} f_{1} f_{2} \\] <p>Justification of \\(n_{\\text {out }}=C_{\\text {out }} f_{1} f_{2}\\) requires working through the complex indexing or considering the \"transpose convolution\". We will return to it later.</p>"},{"location":"books-and-courses/mfdnn/format/6/#automatic-differentation","title":"Automatic Differentation","text":"<p>Definition 6.37 : Automatic Differentation</p> <p>Autodiff (automatic differentiation) is an algorithm that automates gradient computation. In deep learning libraries, you only need to specify how to evaluate the function.</p> <p>Backprop (back propagation) is an instance of autodiff. (backprop \\(\\subseteq\\) autodiff)</p> <p>Gradient computation costs roughly \\(5 \\times\\) the computation cost of forward evaluation.</p> <p>To clarify, backprop and autodiff are not</p> <ul> <li>finite difference (numerical differentation) or</li> <li>symbolic differentiation.</li> </ul> <p>Autodiff \\(\\approx\\) chain rule of vector calculus</p> <p>Autodiff is an essential yet often an underappreciated feature of the deep learning libraries. It allows deep learning researchers to use complicated neural networks, while avoiding the burden of performing derivative calculations by hand.</p> <p>Most deep learning libraries support \\(2^{\\text {nd }}\\) and higher order derivative computation, but we will only use \\(1^{\\text {st }}\\) order derivatives (gradients) in this class.</p> <p>Autodiff includes forward-mode, reverse-mode (backprop), and other orders. In deep learning, reverse-mode is most commonly used.</p> <p>Concept 6.38 : Autodiff by Jacobial Multiplication</p> <p>Consider \\(g=f_{L} \\circ f_{L-1} \\circ \\cdots \\circ f_{2} \\circ f_{1}\\), where \\(f_{\\ell}: \\mathbb{R}^{n_{\\ell-1}} \\rightarrow \\mathbb{R}^{n_{\\ell}}\\) for \\(\\ell=1, \\cdots, L\\).</p> <p>Chain rule: \\(D g=D f_{L} \\quad D f_{L-1} \\quad \\cdots \\quad D f_{2} \\quad D f_{1}\\)</p> <p>Forward-mode: \\(D f_{L}\\left(D f_{L-1}\\left(\\cdots\\left(D f_{2} D f_{1}\\right) \\cdots\\right)\\right)\\)</p> <p>Reverse-mode (back propagation): \\(\\left(\\left(\\left(D f_{L} D f_{L-1}\\right) D f_{L-2}\\right) \\cdots\\right) D f_{1}\\)</p> <p>Reverse mode is optimal (can be proved using DP) when \\(n_{L} \\leq n_{L-1} \\leq \\cdots \\leq n_{1} \\leq n_{0}\\). The number of neurons in each layer tends to decrease in deep neural networks for classification. So reverse-mode is often close to the most efficient mode of autodiff in deep learning.</p> <p>Definition 6.39 : General Backprop</p> <p>Backprop in PyTorch:</p> <ol> <li>When the loss function is evaluated, a computation graph is constructed.</li> <li>The computation graph is a directed acyclic graph (DAG) that encodes dependencies of the individual computational components.</li> <li>A topological sort is performed on the DAG and the backprop is performed on the reversed order of this topological sort. (The topological sort ensures that nodes ahead in the DAG are processed first.)</li> </ol> <p>The general form combines a graph theoretic formulation with the principles of backprop.</p> <p>Definition 6.40 : Computation Graph</p> <p>Let \\(y_{1}, \\ldots, y_{L}\\) be the output values (neurons) of the computational nodes. Assume \\(y_{1}, \\ldots, y_{L}\\) follow a linear topological ordering, i.e., the computation of \\(y_{\\ell}\\) depends on \\(y_{1}, \\ldots, y_{\\ell-1}\\) and does not depend on \\(y_{\\ell+1}, \\ldots, y_{L}\\).</p> <p>Define the graph \\(G=(V, E)\\), where \\(V=\\{1, \\ldots, L\\}\\) and \\((i, \\ell) \\in E\\), i.e., \\(i \\rightarrow \\ell\\), if the computation of \\(y_{\\ell}\\) directly depends on \\(y_{i}\\). Write the computation of \\(y_{1}, \\ldots, y_{L}\\) as</p> \\[ y_{\\ell}=f_{\\ell}\\left(\\left[y_{i}: \\text { for } i \\rightarrow \\ell\\right]\\right) \\] <p>Definition 6.41 : Forward Pass on Computation Graph</p> <p>In the forward pass, sequentially compute \\(y_{1}, \\ldots, y_{L}\\) via</p> \\[ y_{\\ell}=f_{\\ell}\\left(\\left[y_{i}: \\text { for } i \\rightarrow \\ell\\right]\\right) \\] <pre><code># Use 1-based indexing\n# y[1] given\nfor l = 2,...,L\n    inputs = [y[i] for j such that (i-&gt;l)]\n    y[l] = f[l].eval(inputs)\nend\n</code></pre> <p>Example 6.42 : Forward Pass &amp; Forward-mode Autodiff</p> <p>Consider \\(f(x, y)=y \\log x+\\sqrt{y \\log x}\\). Evaluate \\(f\\) with the computation graph:</p> <p> </p> <ul> <li> <p>Step 0 :</p> \\[ \\begin{gathered} x=3, y=2 \\\\ \\frac{\\partial x}{\\partial x}=1, \\frac{\\partial x}{\\partial y}=0, \\frac{\\partial y}{\\partial x}=0, \\frac{\\partial y}{\\partial y}=1 \\end{gathered} \\] </li> <li> <p>Step 1 :</p> \\[ \\begin{gathered} a=\\log x=\\log 3 \\\\ \\frac{\\partial a}{\\partial x}=\\frac{1}{x} \\cdot \\frac{\\partial x}{\\partial x}=\\frac{1}{3}, \\frac{\\partial a}{\\partial y}=0 \\end{gathered} \\] </li> <li> <p>Step 2 :</p> \\[ \\begin{gathered} b=y a=2 \\log 3 \\\\ \\frac{\\partial b}{\\partial x}=\\frac{\\partial y}{\\partial x} a+y \\frac{\\partial a}{\\partial x}=\\frac{2}{3}, \\frac{\\partial b}{\\partial y}=\\frac{\\partial y}{\\partial y} a+y \\frac{\\partial a}{\\partial y}=a=\\log 3 \\end{gathered} \\] </li> <li> <p>Step 3 :</p> \\[ \\begin{gathered} c=\\sqrt{b}=\\sqrt{2 \\log 3} \\\\ \\frac{\\partial c}{\\partial x}=\\frac{1}{2 \\sqrt{b}} \\frac{\\partial b}{\\partial x}=\\frac{1}{3 \\sqrt{2 \\log 3}}, \\frac{\\partial c}{\\partial y}=\\frac{1}{\\sqrt{b}} \\frac{\\partial b}{\\partial y}=\\frac{1}{2} \\sqrt{\\frac{\\log 3}{2}} \\end{gathered} \\] </li> <li> <p>Step 4 :</p> \\[ \\begin{gathered} f=c+b=\\sqrt{2 \\log 3}+2 \\log 3 \\\\ \\frac{\\partial f}{\\partial x}=\\frac{\\partial c}{\\partial x}+\\frac{\\partial b}{\\partial x}=\\frac{1}{3}\\left(2+\\frac{1}{3 \\sqrt{2 \\log 3}}\\right), \\frac{\\partial f}{\\partial y}=\\frac{\\partial c}{\\partial y}+\\frac{\\partial b}{\\partial y}=\\frac{1}{2} \\sqrt{\\frac{\\log 3}{2}}+\\log 3 \\end{gathered} \\] </li> </ul> <p>Definition 6.42 : Backprop on Computation Graph</p> <p>To perform backprop, use</p> \\[ \\frac{\\partial y_{L}}{\\partial y_{i}}=\\sum_{\\ell: i \\rightarrow \\ell} \\frac{\\partial y_{L}}{\\partial y_{\\ell}} \\frac{\\partial f_{\\ell}}{\\partial y_{i}} \\] <p>to sequentially compute \\(\\frac{\\partial y_{L}}{\\partial y_{L}}, \\frac{\\partial y_{L}}{\\partial y_{L-1}}, \\ldots, \\frac{\\partial y_{L}}{\\partial y_{1}}\\).     </p> <pre><code># Use 1-based indexing\n# y[1],...,y[L] already computed\ng[:] = 0 // .zero_grad()\ng[L] = 1 // dy[L]/dy[L]=1\nfor l = L,...,2\n    for i such that (i-&gt;l)\n        g[i] += g[l]*f[l].grad(i)\n    end\nend\n</code></pre> <p>Example 6.43 : Reverse-mode Autodiff (Backprop)</p> <p>Consider \\(f(x, y)=y \\log x+\\sqrt{y \\log x}\\). Evaluate \\(f\\) with the computation graph:</p> <p> </p> <ul> <li> <p>Step 0 :</p> \\[ x=3, y=2 \\] </li> <li> <p>Step 1 :</p> \\[ a=\\log 3 \\] </li> <li> <p>Step 2 :</p> \\[ b=2 \\log 3 \\] </li> <li> <p>Step 3 :</p> \\[ c=\\sqrt{2 \\log 3} \\] </li> <li> <p>Step 4 :</p> \\[ f=\\sqrt{2 \\log 3}+2 \\log 3 \\] </li> </ul> <ul> <li> <p>Step 0' :</p> \\[ \\frac{\\partial f}{\\partial f}=1 \\] </li> <li> <p>Step 1' :</p> \\[ \\frac{\\partial f}{\\partial c}=\\frac{\\partial f}{\\partial f} \\frac{\\partial f}{\\partial c}=\\frac{\\partial f}{\\partial f} 1=1 \\] </li> <li> <p>Step 2' :</p> \\[ \\frac{\\partial f}{\\partial b}=\\frac{\\partial f}{\\partial c} \\frac{\\partial c}{\\partial b}+\\frac{\\partial f}{\\partial f} \\frac{\\partial f}{\\partial c}=\\frac{1}{2 \\sqrt{b}} 1+1=\\frac{1}{2 \\sqrt{2 \\log 3}}+1 \\] </li> <li> <p>Step 3' :</p> \\[ \\frac{\\partial f}{\\partial a}=\\frac{\\partial f}{\\partial b} \\frac{\\partial b}{\\partial a}=\\frac{\\partial f}{\\partial b} y=2+\\frac{1}{\\sqrt{2 \\log 3}} \\] </li> <li> <p>Step 4' :</p> \\[ \\begin{gathered} \\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial a} \\frac{\\partial a}{\\partial x}=\\frac{\\partial f}{\\partial a} \\frac{1}{x}=\\frac{1}{3}\\left(2+\\frac{1}{\\sqrt{2 \\log 3}}\\right) \\\\ \\frac{\\partial f}{\\partial y}=\\frac{\\partial f}{\\partial b} \\frac{\\partial b}{\\partial y}=\\frac{\\partial f}{\\partial b} a=\\frac{1}{2} \\sqrt{\\frac{\\log 3}{2}}+\\log 3 \\end{gathered} \\] </li> </ul> <p>Concept 6.44 : Backprop in Pytorch</p> <p> </p> <p>In NN training, parameters (shown blue in the image) and fixed inputs are distinguished. In PyTorch, you (1) clear the existing gradient with <code>.zero_grad()</code> (2) forward-evaluate the loss function by providing the input and label and (3) perform backprop with <code>.backward()</code>.</p> <p>The forward pass stores the intermediate neuron values so that they can later be used in backprop. In the test loop, however, we don't compute gradients so the intermediate neuron values are unnecessary. The <code>torch.no_grad()</code> context manager allows intermediate node values to discarded or not be stored. This saves memory and can accelerate the test loop.</p>"},{"location":"books-and-courses/mfdnn/format/6/#batch-normalization","title":"Batch Normalization","text":"<p>Concept 6.45 : Idea of Batch Normalization</p> <p>The first step of many data processing algorithms is often to normalize data to have zero mean and unit variance.</p> <ul> <li>Step 1. Compute \\(\\hat{\\mu}=\\frac{1}{N} \\sum_{i=1}^{N} X_{i}, \\widehat{\\sigma^{2}}=\\frac{1}{N} \\sum_{i=1}^{N}\\left(X_{i}-\\hat{\\mu}\\right)^{2}\\)</li> </ul> \\[ \\hat{X}_{i}=\\frac{X_{i}-\\widehat{\\mu}}{\\sqrt{\\sigma^{2}}+\\varepsilon} \\] <ul> <li>Step 2. Run method with data \\(\\hat{X}_{1}, \\ldots, \\hat{X}_{N}\\)</li> </ul> <p>Batch normalization (BN) (sort of) enforces this normalization layer-by-layer. BN is an indispensable tool for training very deep neural networks. Theoretical justification is weak.</p> <p>(S. Ioffe and C. Szegedy, Batch normalization: Accelerating deep network training by reducing internal covariate shift, ICML, 2015.)</p> <p>Definition 6.46 : BN for Linear Layers</p> <p>Underlying assumption: Each element of the batch is an IID sample.</p> <p>Input: \\(X\\), \\(\\text{shape}(X) = \\text{(batch size)} \\times \\text{(# entries)}\\)</p> <p>Output: \\(\\mathrm{BN}_{\\beta, \\gamma}(X)\\), \\(\\text{shape} \\left(\\mathrm{BN}_{\\beta, \\gamma}(X)\\right)=\\operatorname{shape}(X)\\)</p> <p>\\(\\mathrm{BN}_{\\beta, \\gamma}\\) for linear layers acts independently over neurons.</p> \\[ \\begin{gathered} \\hat{\\mu}[:]=\\frac{1}{B} \\sum_{b=1}^{B} X[b,:]\\\\ \\hat{\\sigma}^{2}[:]=\\frac{1}{B} \\sum_{b=1}^{B}(X[b,:]-\\hat{\\mu}[:])^{2} \\\\ \\mathrm{BN}_{\\gamma, \\beta}(X)[b,:]=\\gamma[:] \\frac{X[b,:]-\\hat{\\mu}[:]}{\\sqrt{\\hat{\\sigma}^{2}[:]+\\varepsilon}}+\\beta[:] \\quad b=1, \\ldots, B \\end{gathered} \\] <p>where operations are elementwise. BN normalizes each output neuron. The mean and variance are explicitly controlled through learned parameters \\(\\beta\\) and \\(\\gamma\\). In Pytorch, <code>nn.BatchNorm1d</code>.</p> <p>Definition 6.47 : BN for Convolutional Layers</p> <p>Underlying assumption: Each element of the batch, horizontal pixel, and vertical pixel is an IID sample.</p> <p>Input: \\(X\\), \\(\\text{shape}(X) =  \\text{(batch size)} \\times \\text{(channels)} \\times \\text{(vertical dim)} \\times \\text{(horizontal dim)}\\)</p> <p>Output: \\(\\mathrm{BN}_{\\beta, \\gamma}(X)\\), \\(\\text{shape} \\left(\\mathrm{BN}_{\\beta, \\gamma}(X)\\right)=\\operatorname{shape}(X)\\)</p> <p>\\(\\mathrm{BN}_{\\beta, \\gamma}\\) for conv. layers acts independently over channels.</p> \\[ \\begin{gathered} \\hat{\\mu}[:]=\\frac{1}{B P Q} \\sum_{b=1}^{B} \\sum_{i=1}^{P} \\sum_{j=1}^{Q} X[b,:, i, j] \\\\ \\hat{\\sigma}^{2}[:]=\\frac{1}{B P Q} \\sum_{b=1}^{B} \\sum_{i=1}^{P} \\sum_{j=1}^{Q}(X[b,:, i, j]-\\hat{\\mu}[:])^{2} \\\\ \\operatorname{BN}_{\\gamma, \\beta}(X)[b,:, i, j]=\\gamma[:] \\frac{X[b,:, i, j]-\\hat{\\mu}[:]}{\\sqrt{\\hat{\\sigma}^{2}[:]+\\varepsilon}}+\\beta[:] \\quad \\begin{array}{l} b=1, \\ldots, B \\\\ i=1, \\ldots, P \\\\ j=1, \\ldots, Q \\end{array} \\end{gathered} \\] <p>BN normalizes over each convolutional filter. The mean and variance are explicitly controlled through learned parameters \\(\\beta\\) and \\(\\gamma\\). In Pytorch, <code>nn.BatchNorm2d</code>.</p> <p>Definition 6.48 : BN during Testing</p> <p>\\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}\\) are estimated from batches during training. During testing, we don't update the NN, and we may only have a single input (so no batch).</p> <p>There are 2 strategies for computing final values of \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}\\) :</p> <ol> <li>After training, fix all parameters and evaluate NN on full training set to compute \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}\\) layer-by-layer. Store this computed value. (Computation of \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}\\) must be done sequentially layer-by-layer. Why?)</li> <li>During training, compute running average of \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}\\). This is the default behavior of PyTorch.</li> </ol> <p>In PyTorch, use <code>model.train()</code> and <code>model.eval()</code> to switch BN behavior between training and testing.</p> <p>Concept 6.49 : Efficiency of BN</p> <p>BN does not change the representation power of NN ; since \\(\\beta\\) and \\(\\gamma\\) are trained, the output of each layer can have any mean and variance. However, controlling the mean and variance as explicit trainable parameters makes training easier.</p> <p>With BN, the choice of batch size becomes a more important hyperparameter to tune.</p> <p>BN is indispensable in practice. Training of VGGNet and GoogLeNet becomes much easier with BN. Training of ResNet requires BN.</p> <p>Concept 6.51 : BN and Internal Covariate Shift</p> <p>BN has insufficient theoretical justification. The original paper by loffe and Szegedy hypothesized that BN mitigates internal covariate shift (ICS), the shift in the mean and variance of the intermediate layer neurons throughout the training, and that this mitigation leads to improved training.</p> \\[ \\mathrm{BN} \\Rightarrow(\\text { reduced ICS }) \\Rightarrow \\text { (improved training }) \\] <p>However, Santukar et al. demonstrated that when experimentally measured, BN does not mitigate ICS, but nevertheless improves the training.</p> \\[ \\mathrm{BN} \\nRightarrow \\text { (reduced ICS) } \\] <p>Nevertheless</p> \\[ \\mathrm{BN} \\Rightarrow \\text { (improved training performance) } \\] <p>Santukar et al. argues that</p> \\[ \\mathrm{BN} \\Rightarrow \\text { (smoother loss landscape) } \\Rightarrow \\text { (improved training performance) } \\] <p>While this claim is more evidence-based than that of loffe and Szegedy, it is still not conclusive. It is also unclear why BN makes the loss landscape smoother, and it is not clear whether the smoother loss landscape fully explains the improved training performance.</p> <p>This story is a cautionary tale: we should carefully distinguish between speculative hypotheses and evidence-based claims, even in a primarily empirical subject.</p> <p>(S. Ioffe and C. Szegedy, Batch normalization: Accelerating deep network training by reducing internal covariate shift, ICML, 2015. S. Santurkar, D. Tsipras, A. Ilyas, and A. M\u0105dry, How does batch normalization help optimization?, NeurIPS, 2018.)</p> <p>Concept 6.50 : BN has trainable parameters.</p> <p>BN is usually not considered a trainable layer, much like pooling or dropout, and they are usually excluded when counting the \"depth\" of a NN. However, BN does have trainable parameters. Interestingly, if one randomly initializes a CNN, freezes all other parameters, and only train BN parameters, the performance is surprisingly good.</p> <p> </p> <p>(J. Frankle, D. J. Schwab, and A. S. Morcos, Training BatchNorm and only BatchNorm: On the expressive power of random features in CNNs, NeurIPS SEDL Workshop, 2019.)</p> <p>Concept 6.51 : Discussion of BN</p> <p>BN seems to also act as a regularizer, and for some reason subsumes effect Dropout. (Using dropout together with BN seems to worsen performance.) Since BN has been popularized, Dropout is used less often.</p> <p>After training, functionality of BN can be absorbed into the previous layer when the previous layer is a linear layer or a conv layer.</p> <p>The use of batch norm makes the scaling of weight initialization less important irrelevant.</p> <p>Use <code>bias=false</code> on layers preceding BN , since \\(\\beta\\) subsumes the bias.</p> <p>(X. Li, S. Chen, X. Hu and J. Yang, Understanding the disharmony between dropout and batch normalization by variance shift, CVPR, 2019.)</p>"},{"location":"books-and-courses/mfdnn/format/7/","title":"\u00a7 7. ImageNet Challenge","text":"<p>Definition 7.1 : ImageNet Dataset</p> <p>ImageNet contains more 14 million hand-annotated images in more than 20,000 categories. Many classes, higher resolution, non-uniform image size, multiple objects per image.</p> <p> </p> <p>History</p> <ul> <li>Fei-Fei Li started the ImageNet project in 2006 with the goal of expanding and improving the data available for training Al algorithms.</li> <li>Images were annotated with Amazon Mechanical Turk.</li> <li>The ImageNet team first presented their dataset in the 2009 Conference on Computer Vision and Pattern Recognition (CVPR).</li> <li>From 2010 to 2017, the ImageNet project ran the ImageNet Large Scale Visual Recognition Challenge (ILSVRC).</li> <li>In the 2012 ILSVRC challenge, 150,000 images of 1000 classes were used.</li> <li>In 2017, 29 teams achieved above 95\\% accuracy. The organizers deemed task complete and ended the ILSVRC competition.</li> </ul> <p>ImageNet-1k</p> <p>Commonly referred to as \"the ImageNet dataset\". Also called ImageNet2012. However, ImageNet-1k is really a subset of full ImageNet dataset. ImageNet-1k has 150,000 images of 1000 roughly balanced classes.</p> <p> </p> <p>Top-1 vs Top-5 Accuracy</p> <p>Classifiers on ImageNet-1k are often assessed by their top-5 accuracy, which requires the 5 categories with the highest confidence to contain the label. In contrast, the top-1 accuracy simply measures whether the network's single prediction is the label.</p> <p>For example, AlexNet had a top-5 accuracy of 84.6% and a top-1 accuracy of 63.3%. Nowadays, accuracies of classifiers has improved, so the top 1 accuracy is becoming the more common metric.</p>"},{"location":"books-and-courses/mfdnn/format/7/#lenet","title":"LeNet","text":"<p>Definition 7.2 : LeNet5</p> <p> </p> <p>(Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, Gradient-based learning applied to document recognition, Proceedings of the IEEE, 1998.)</p> <p>Concept 7.3 : Architectural Contribution</p> <p>One of the earliest demonstration of using a deep CNN to learn a nontrivial task.</p> <p>Laid the foundation of the modern CNN architecture.</p>"},{"location":"books-and-courses/mfdnn/format/7/#alexnet","title":"AlexNet","text":"<p>Definition 7.4 : AlexNet</p> <p>Won the 2012 ImageNet challenge by a large margin: top-5 error rate \\(15.3 \\%\\) vs. \\(26.2 \\%\\) second place.</p> <p>Started the era of deep neural networks and their training via GPU computing.</p> <p>AlexNet was split into 2 as GPU memory was limited. (A single modern GPU can easily hold AlexNet.)</p> <p> </p> <p>(A. Krizhevsky, I. Sutskever, and G. E. Hinton, ImageNet classification with deep convolutional neural networks, NeurIPS, 2012.)</p> <p>Definition 7.5 : AlexNet for ImageNet</p> <p> </p> <p>Definition 7.6 : AlexNet for Cifar10</p> <p> </p> <p>Concept 7.7 : Architectural Contribution</p> <p>A scaled-up version of LeNet.</p> <p>Demonstrated that deep CNNs can learn significantly complex tasks. (Some thought CNNs could only learn simple, toy tasks like MNIST.)</p> <p>Demonstrated GPU computing to be an essential component of deep learning.</p> <p>Demonstrated effectiveness of ReLU over sigmoid or tanh in deep CNNs for classification.</p>"},{"location":"books-and-courses/mfdnn/format/7/#vggnet","title":"VGGNet","text":"<p>Definition 7.8 : VGGNet</p> <p> </p> <p> </p> <p>(K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale image recognition, ICLR, 2015.)</p> <p>Definition 7.9 : VGGNet for Cifar10</p> <p> </p> <p>Concept 7.10 : Architectural Contribution</p> <p>Demonstrated simple deep CNNs can significantly improve upon AlexNet.</p> <p>In a sense, VGGNet represents the upper limit of the simple CNN architecture. (It is the best simple model.) Future architectures make gains through more complex constructions.</p> <p>Demonstrated effectiveness of stacked \\(3 \\times 3\\) convolutions over larger \\(5 \\times 5\\) or \\(11 \\times 11\\) convolutions. Large convolutions (larger than \\(5 \\times 5\\) ) are now uncommon.</p> <p>Due to its simplicity, VGGNet is one of the most common test subjects for testing something on deep CNNs.</p>"},{"location":"books-and-courses/mfdnn/format/7/#nin-network","title":"NiN Network","text":"<p>Concept 7.11 : Linear layers have too many parameters.</p> <p>Linear layers have too many parameters.</p> <ul> <li> <p>AlexNet:</p> <p>Conv layer params: 2,469,696 (4%) Linear layer params: 58,631,144 (96%) Total params: 61,100,840</p> </li> <li> <p>VGG19:</p> <p>Conv layer params: 20,024,384 (14%) Linear layer params: 123,642,856 (86%) Total params: 143,667,240</p> </li> </ul> <p>Definition 7.12 : Network in Network (NiN)</p> <p> </p> <p>(M. Lin, Q. Chen, and S. Yan, Network In Network, arXiv, 2013.)</p> <p>Concept 7.13 : \\(1 \\times 1\\) Convolution</p> <p>A \\(1 \\times 1\\) convolution is like a fully connected layer acting independently and identically on each spatial location.</p> <p> </p> <ul> <li>96 filters act on 192 channels separately for each pixel</li> <li>\\(96 \\times 192+96\\) parameters for weights and biases</li> </ul> <p>Concept 7.14 : Regular Convolution Layer vs Network in Network</p> <p>Regular Convolution Layer</p> <p>Input: \\(X \\in \\mathbb{R}^{C_{0} \\times m \\times n}\\)</p> <ul> <li>Select an \\(f \\times f\\) patch \\(\\tilde{X}=X[:, i: i+f, j: j+f]\\).</li> <li>Inner product \\(\\tilde{X}\\) and \\(w_{1}, \\ldots, w_{C_{1}} \\in \\mathbb{R}^{C_{0} \\times f \\times f}\\) and add bias \\(b_{1} \\in \\mathbb{R}^{C_{1}}\\).</li> <li>Apply \\(\\sigma\\). (Output in \\(\\mathbb{R}^{C_{1}}\\).)</li> </ul> <p>Repeat this for all patches. Output in \\(X \\in \\mathbb{R}^{C_{1} \\times(m-f+1) \\times(n-f+1)}\\). Repeat this for all batch elements.</p> <p>Network in Network</p> <p>Input: \\(X \\in \\mathbb{R}^{c_{0} \\times m \\times n}\\)</p> <ul> <li>Select an \\(f \\times f\\) patch \\(\\tilde{X}=X[:, i: i+f, j: j+f]\\).</li> <li>Inner product \\(\\tilde{X}\\) and \\(w_{1}, \\ldots, w_{C_{1}} \\in \\mathbb{R}^{C_{0} \\times f \\times f}\\) and add bias \\(b_{1} \\in \\mathbb{R}^{C_{1}}\\).</li> <li>Apply \\(\\sigma\\). (Output in \\(\\mathbb{R}^{C_{1}}\\).)</li> <li>Apply Linear \\(A_{A_{2}, b_{2}}(x)\\) where \\(A_{2} \\in \\mathbb{R}^{C_{2} \\times C_{1}}\\) and \\(b_{2} \\in \\mathbb{R}^{C_{2}}\\).</li> <li>Apply \\(\\sigma\\). (Output in \\(\\mathbb{R}^{C_{2}}\\).)</li> <li>Apply Linear \\(A_{A_{3}, b_{3}}(x)\\) where \\(A_{3} \\in \\mathbb{R}^{C_{3} \\times C_{2}}\\) and \\(b_{3} \\in \\mathbb{R}^{C_{3}}\\).</li> <li>Apply \\(\\sigma\\). (Output in \\(\\mathbb{R}^{C_{3}}\\).)</li> </ul> <p>Repeat this for all patches. Output in \\(X \\in \\mathbb{R}^{C_{3} \\times(m-f+1) \\times(n-f+1)}\\). Repeat this for all batch elements. Why is this equivalent to (\\(3 \\times 3\\) conv)-(\\(1 \\times 1\\) conv)-(\\(1 \\times 1\\) conv)?</p> <p>Concept 7.15 : Global Average Pool</p> <p>When using CNNs for classification, position of object is not important.</p> <p>The global average pool has no trainable parameters (linear layers have many) and it is translation invariant. Global average pool removes the spatial dependency.</p> <p>Concept 7.16 : Architectural Contribution</p> <p>Used \\(1 \\times 1\\) convolutions to increase the representation power of the convolutional modules.</p> <p>Replaced linear layer with average pool to reduce number of trainable parameters.</p> <p>First step in the trend of architectures becoming more abstract. Modern CNNs are built with smaller building blocks.</p>"},{"location":"books-and-courses/mfdnn/format/7/#googlenet","title":"GoogLeNet","text":"<p>Definition 7.17 : GoogLeNet (Inception v1)</p> <p>Utilizes the inception module. Structure inspired by NiN and name inspired by 2010 Inception movie meme.</p> <p>Used \\(1 \\times 1\\) convolutions.</p> <ul> <li>Increased depth adds representation power (improves ability to represent nonlinear functions).</li> <li>Reduce the number of channels before the expensive \\(3 \\times 3\\) and \\(5 \\times 5\\) convolutions, and thereby reduce number of trainable weights and computation time.</li> </ul> <p>The name GoogLeNet is a reference to the authors' Google affiliation and is an homage to LeNet.</p> <p> </p> <p>(C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, Going deeper with convolutions, CVPR, 2015)</p> <p> </p> <p>Definition 7.18 : GoogLeNet for Cifar10</p> <p> </p> <p>Concept 7.19 : Architectural Contribution</p> <p>Demonstrated that more complex modular neural network designs can outperform VGGNet's straightforward design.</p> <p>Together with VGGNet, demonstrated the importance of depth.</p> <p>Kickstarted the research into deep neural network architecture design.</p>"},{"location":"books-and-courses/mfdnn/format/7/#resnet","title":"ResNet","text":""},{"location":"books-and-courses/mfdnn/format/8/","title":"\u00a7 8. CNNs for Other Supervised Learning Tasks","text":""},{"location":"books-and-courses/mfdnn/format/8/#inverse-problem","title":"Inverse Problem","text":"<p>Definition 8.1 : Inverse Problem Model</p> <p>In inverse problems, we wish to recover a signal \\(X_{\\text {true }}\\) given measurements \\(Y\\). The unknown and the measurements are related through</p> \\[ \\mathcal{A}\\left[X_{\\text {true }}\\right]+\\varepsilon=Y, \\] <p>where \\(\\mathcal{A}\\) is often, but not always, linear, and \\(\\varepsilon\\) represents small error.</p> <p>The forward model \\(\\mathcal{A}\\) may or may not be known. In other words, the goal of an inverse problem is to find an approximation of \\(\\mathcal{A}^{-1}\\).</p> <p>In many cases, \\(\\mathcal{A}\\) is not even be invertible. In such cases, we can still hope to find an mapping that serves as an approximate inverse in practice.</p> <p>Concept 8.2 : Inverse Problems via Deep Learning</p> <p>In deep learning, we use a neural network to approximate the inverse mapping</p> \\[ f_{\\theta} \\approx \\mathcal{A}^{-1} \\] <p>i.e., we want \\(f_{\\theta}(Y) \\approx X_{\\text {true }}\\) for the measurements \\(X\\) that we care about.</p> <p>If we have \\(X_{1}, \\ldots, X_{N}\\) and \\(Y_{1}, \\ldots, Y_{N}\\) (but no direct knowledge of \\(\\mathcal{A}\\) ), we can solve</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\sum_{i=1}^{N}\\left\\|f_{\\theta}\\left(Y_{i}\\right)-X_{i}\\right\\| \\] <p>If we have \\(X_{1}, \\ldots, X_{N}\\) and knowledge of \\(\\mathcal{A}\\), we can solve</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\sum_{i=1}^{N}\\left\\|f_{\\theta}\\left[\\mathcal{A}\\left(X_{i}\\right)\\right]-X_{i}\\right\\| \\] <p>If we have \\(Y_{1}, \\ldots, Y_{N}\\) and knowledge of \\(\\mathcal{A}\\), we can solve</p> \\[ \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{minimize}} \\sum_{i=1}^{N}\\left\\|\\mathcal{A}\\left[f_{\\theta}\\left(Y_{i}\\right)\\right]-Y_{i}\\right\\| \\]"},{"location":"books-and-courses/mfdnn/format/8/#gaussian-denoising","title":"Gaussian Denoising","text":"<p>Definition 8.3 : Gaussian Denoising</p> <p>Given \\(X_{\\text {true }} \\in \\mathbb{R}^{w \\times h}\\), we measure</p> \\[ Y=X_{\\text {true }}+\\varepsilon \\] <p>where \\(\\varepsilon_{i j} \\sim \\mathcal{N}\\left(0, \\sigma^{2}\\right)\\) is IID Gaussian noise. For the sake of simplicity, assume we know \\(\\sigma\\). Goal is to recover \\(X_{\\text {true }}\\) from \\(Y\\).</p> <p>Gaussian denoising is the simplest setup in which the goal is to remove noise from the image. In more realistic setups, the noise model will be more complicated and the noise level \\(\\sigma\\) will be unknown.</p> <p>Definition 8.4 : DnCNN</p> <p>In 2017, Zhang et al. presented the denoising convolutional neural networks (DnCNNs). They trained a 17-layer CNN \\(f_{\\theta}\\) to learn the noise with the loss</p> \\[ \\mathcal{L}(\\theta)=\\sum_{i=1}^{N}\\left\\|f_{\\theta}\\left(Y_{i}\\right)-\\left(Y_{i}-X_{i}\\right)\\right\\|^{2} \\] <p>so that the clean recovery can be obtained with \\(Y_{i}-f_{\\theta}\\left(Y_{i}\\right)\\). (This is equivalent to using a residual connection from beginning to end.)</p> <p> </p> <p>Image denoising is was an area with a large body of prior work. DnCNN dominated all prior approaches that were not based on deep learning.</p> <p>Nowadays, all state-of-the-art denoising algorithms are based on deep learning.</p> <p> </p> <p>(K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang, Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising, IEEE TIP, 2017.)</p>"},{"location":"books-and-courses/mfdnn/format/8/#image-super-resolution","title":"Image Super-Resolution","text":"<p>Definition 8.5 : Image Super-Resolution</p> <p>Given \\(X_{\\text {true }} \\in \\mathbb{R}^{w \\times h}\\), we measure</p> \\[ Y=\\mathcal{A}\\left(X_{\\text {true }}\\right) \\] <p>where \\(\\mathcal{A}\\) is a \"downsampling\" operator. So \\(Y \\in \\mathbb{R}^{w_{2} \\times h_{2}}\\) with \\(w_{2}&lt;w\\) and \\(h_{2}&lt;h\\). Goal is to recover \\(X_{\\text {true }}\\) from \\(Y\\).</p> <p>In the simplest setup, \\(\\mathcal{A}\\) is an average pool operator with \\(r \\times r\\) kernel and a stride \\(r\\).</p> <p>Definition 8.6 : SRCNN</p> <p>In 2015, Dong et al. presented super-resolution convolutional neural network (SRCNN). They trained a 3-layer \\(\\operatorname{CNN} f_{\\theta}\\) to learn the high-resolution reconstruction with the loss</p> \\[ \\mathcal{L}(\\theta)=\\sum_{i=1}^{N}\\left\\|f_{\\theta}\\left(\\tilde{Y}_{i}\\right)-X_{i}\\right\\|^{2} \\] <p>where \\(\\tilde{Y}_{i} \\in \\mathbb{R}^{w \\times h}\\) is an upsampled version of \\(Y_{i} \\in \\mathbb{R}^{(w / r) \\times(h / r)}\\), i.e., \\(\\tilde{Y}_{i}\\) has the same number of pixels as \\(X_{i}\\), but the image is pixelated or blurry. The goal is to have \\(f_{\\theta}\\left(\\tilde{Y}_{i}\\right)\\) be a sharp reconstruction.</p> <p> </p> <p>SRCNN showed that simple learning based approaches can match the state-of the art performances of superresolution task.</p> <p> </p> <p>(C. Dong, C. C. Loy, K. He, and X. Tang, Image super-resolution using deep convolutional networks, IEEE TPAMI, 2015.)</p> <p>Definition 8.7 : VDSR</p> <p>In 2016, Kim et al. presented VDSR. They trained a 20-layer CNN with a residual connection \\(f_{\\theta}\\) to learn the high-resolution reconstruction with the loss</p> \\[ \\mathcal{L}(\\theta)=\\sum_{i=1}^{N}\\left\\|f_{\\theta}\\left(\\tilde{Y}_{i}\\right)-X_{i}\\right\\|^{2} \\] <p>The residual connection was the key insight that enabled the training of much deeper CNNs.</p> <p> </p> <p>VDSR dominated all prior approaches not based on deep learning. Showed that simple learning based approaches can batch the state-of theart performances of super-resolution task.</p> <p> </p> <p>(J. Kim, J. K. Lee, and K. M. Lee, Accurate image super-resolution using very deep convolutional networks, CVPR, 2016.)</p>"},{"location":"books-and-courses/mfdnn/format/8/#other-examples","title":"Other Examples","text":"<p>Example 8.8 : SRGAN</p> <p> </p> <p> </p> <p> </p> <p>(C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi, Photo-realistic single image super-resolution using a generative adversarial network, CVPR, 2017.)</p> <p>Example 8.9 : Image Colorization</p> <p> </p> <p>(R. Zhang, P. Isola, and A. A. Efros, Colorful image colorization, ECCV, 2016.)</p> <p>Example 8.10 : Image Inpainting</p> <p> </p> <p> </p> <p>(J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang, Generative image inpainting with contextual attention, CVPR, 2018.)</p>"},{"location":"books-and-courses/mfdnn/format/8/#operations-increasing-spatial-dimensions","title":"Operations Increasing Spatial Dimensions","text":"<p>Concept 8.11 : Operations Increasing Spatial Dimensions</p> <p>In image classification tasks, the spatial dimensions of neural networks often decrease as the depth progresses.</p> <p>This is because we are trying to forget location information. (In classification, we care about what is in the image, but we do not where it is in the image.)</p> <p>However, there are many networks for which we want to increase the spatial dimension:</p> <ul> <li>Linear layers</li> <li>Upsampling</li> <li>Transposed convolution</li> </ul>"},{"location":"books-and-courses/mfdnn/format/8/#transposed-convolution","title":"Transposed convolution","text":"<p>Concept 8.12 : Linear Operator \\(\\cong\\) Matrix</p> <p>Core tenet of linear algebra: matrices are linear operators and linear operators are matrices.</p> <p>Let \\(f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}\\) be linear, i.e.,</p> \\[ f(x+y)=f(x)+f(y) \\text { and } f(\\alpha x)=\\alpha f(x) \\] <p>for all \\(x, y \\in \\mathbb{R}^{n}\\) and \\(\\alpha \\in \\mathbb{R}\\).</p> <p>There exists a matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) that represents \\(f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}\\), i.e.,</p> \\[ f(x)=A x \\] <p>for all \\(x \\in \\mathbb{R}^{n}\\).</p> <p>Let \\(e_{i}\\) be the \\(i\\)-th unit vector, i.e., \\(e_{i}\\) has all zeros elements except entry 1 in the \\(i\\)-th coordinate.</p> <p>Given a linear \\(f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}\\), we can find the matrix</p> \\[ A=\\left[\\begin{array}{llll} A_{;, 1} &amp; A_{;, 2} &amp; \\cdots &amp; A_{;, n} \\end{array}\\right] \\in \\mathbb{R}^{m \\times n} \\] <p>representing \\(f\\) with</p> \\[ f\\left(e_{j}\\right)=A e_{j}=A_{;, j} \\] <p>for all \\(j=1, \\ldots, n\\), or with</p> \\[ e_{i}^{\\top} f\\left(e_{j}\\right)=e_{i}^{\\top} A e_{j}=A_{i, j} \\] <p>for all \\(i=1, \\ldots, m\\) and \\(j=1, \\ldots, n\\).</p> <p>Concept 8.13 : Linear Operator \\(\\ncong\\) Matrix</p> <p>In applied mathematics and machine learning, there are many setups where explicitly forming the matrix representation \\(A \\in \\mathbb{R}^{m \\times n}\\) is costly, even though the matrix-vector products \\(A x\\) and \\(A^{\\top} y\\) are efficient to evaluate.</p> <p>In machine learning, convolutions are the primary example. Other areas, linear operators based on FFTs are the primary example.</p> <p>In such setups, the matrix representation is still a useful conceptual tool, even if we never intend to form the matrix.</p> <p>Given a matrix \\(A\\), the transpose \\(A^{\\top}\\) is obtained by flipping the row and column dimensions, i.e., \\(\\left(A^{\\top}\\right)_{i j}=(A)_{j i}\\). However, using this definition is not always the most effective when understanding the action of \\(A^{\\top}\\).</p> <p>Another approach is to use the adjoint view. Since</p> \\[ y^{\\top}(A x)=\\left(A^{\\top} y\\right)^{\\top} x \\] <p>for any \\(x \\in \\mathbb{R}^{n}\\) and \\(y \\in \\mathbb{R}^{m}\\), understand the action of \\(A^{\\top}\\) by finding an expression of the form</p> \\[ y^{\\top} A x=\\sum_{j=1}^{n}(\\text { something })_{j} x_{j}=\\left(A^{\\top} y\\right)^{\\top} x \\] <p>Example 8.14 : 1D Transpose Convolution</p> <p>Consider the 1D convolution represented by \\(A \\in \\mathbb{R}^{(n-f+1) \\times n}\\) defined with a given \\(w \\in \\mathbb{R}^{f}\\) and</p> \\[ A=\\left[\\begin{array}{cccccccc} w_{1} &amp; \\cdots &amp; w_{f} &amp; 0 &amp; \\cdots &amp; &amp; &amp; 0 \\\\ 0 &amp; w_{1} &amp; \\cdots &amp; w_{f} &amp; 0 &amp; \\cdots &amp; &amp; 0 \\\\ 0 &amp; 0 &amp; w_{1} &amp; \\cdots &amp; w_{f} &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; &amp; &amp; \\ddots &amp; &amp; \\ddots &amp; &amp; \\vdots \\\\ 0 &amp; &amp; \\cdots &amp; 0 &amp; w_{1} &amp; \\cdots &amp; w_{f} &amp; 0 \\\\ 0 &amp; &amp; \\cdots &amp; 0 &amp; 0 &amp; w_{1} &amp; \\cdots &amp; w_{f} \\end{array}\\right] \\] <p>Then we have</p> \\[ (A x)_{j}=\\sum_{i=1}^{f} w_{i} x_{j+i-1} \\] <p>and we have the following formula which coincides with transposing the matrix \\(A\\).</p> \\[ \\begin{aligned} y^{\\top} A x &amp; =\\sum_{j=1}^{n-f+1} y_{j} \\sum_{i=1}^{f} w_{i} x_{j+i-1} \\\\ &amp; =\\sum_{j=1}^{n-f+1} \\sum_{i=1}^{f} y_{j} w_{i} x_{j+i-1} \\sum_{k=1}^{n} \\mathbf{1}_{\\{k=j+i-1\\}} \\\\ &amp; =\\sum_{k=1}^{n} \\sum_{j=1}^{n-f+1} \\sum_{i=1}^{f} y_{j} w_{i} x_{k} \\mathbf{1}_{\\{k-j+1=i\\}} \\\\ &amp; =\\sum_{k=1}^{n} x_{k} \\sum_{j=1}^{n-f+1} \\sum_{i=1}^{f} w_{k-j+1} y_{j} \\mathbf{1}_{\\{k-j+1=i\\}} \\\\ &amp; =\\sum_{k=1}^{n} x_{k} \\sum_{j=1}^{n-f+1} w_{k-j+1} y_{j} \\sum_{i=1}^{f} \\mathbf{1}_{\\{k-j+1=i\\}} \\\\ &amp; =\\sum_{k=1}^{n} x_{k} \\sum_{j=1}^{n-f+1} w_{k-j+1} y_{j} \\mathbf{1}_{\\{1 \\leq k-j+1 \\leq f\\}} \\\\ &amp; =\\sum_{k=1}^{n} x_{k} \\sum_{j=1}^{n-f+1} w_{k-j+1} y_{j} \\mathbf{1}_{\\{j \\leq k\\}} \\mathbf{1}_{\\{k-f+1 \\leq j\\}} \\\\ &amp; =\\sum_{k=1}^{n} x_{k} \\sum_{j=\\max (k-f+1,1)}^{\\min (n-f+1, k)} w_{k-j+1} y_{j}=\\left(A^{\\top} y\\right)^{\\top} x \\\\ \\end{aligned} \\] <p>Definition 8.15 : Transposed Convolution</p> <p>In transposed convolution, input neurons additively distribute values to the output via the kernel. Before people noticed that this is the transpose of convolution, the names backwards convolution and deconvolution were used.</p> <p>For each input neuron, multiply the kernel and add (accumulate) the value in the output. Can accommodate strides, padding, and multiple channels.</p> <p> </p> <p> </p> <ul> <li>Convolution Visualized</li> </ul>  ![](.././assets/8.15.gif){: width=\"50%\"}  <ul> <li>Transpose Convolution Visualized</li> </ul>  ![](.././assets/8.16.gif){: width=\"100%\"}  <p>Definition 8.16 : 2D Transpose Convolution Layer (Formal Definition)</p> <ul> <li>\\(B\\) : batch size</li> <li>\\(C_{\\text{in}}\\) : # of input channels</li> <li>\\(C_{\\text{out}}\\) : # of output channels</li> <li>\\(m, n\\) : # of vertical and horizontal indices of input</li> <li>\\(f_1, f_2\\) : # of vertical and horizontal indices of filter</li> </ul> <ul> <li>Input tensor: \\(Y \\in \\mathbb{R}^{B \\times C_{\\mathrm{in}} \\times m \\times n}\\)</li> <li>Output tensor: \\(X \\in \\mathbb{R}^{B \\times C_{\\text {out }} \\times\\left(m+f_{1}-1\\right) \\times\\left(n+f_{2}-1\\right)}\\)</li> <li>Filter \\(w \\in \\mathbb{R}^{C_{\\text {in }} \\times C_{\\text {out }} \\times f_{1} \\times f_{2}}\\)</li> <li>Bias \\(b \\in \\mathbb{R}^{C_{\\text {out }}}\\) (If <code>bias=False</code>, then \\(b=0\\).) </li> </ul> <pre><code>def trans_conv(Y, w, b):\n    c_in, c_out, f1, f2 = w.shape\n    batch, c_in, m, n = Y.shape\n    X = torch.zeros(batch, c_out, m + f1 - 1, n + f2 - 1)\n    for k in range(c_in):\n        for i in range(Y.shape[2]):\n            for j in range(Y.shape[3]):\n                X[:, :, i:i+f1, j:j+f2] += Y[:, k, i, j].view(-1,1,1,1)*w[k, :, :, :].unsqueeze(0)\n    return X + b.view(1,-1,1,1)\n</code></pre> <p>In a matrix representation \\(A\\) of convolution, the dependencies of the inputs and outputs are represented by the non-zeros of \\(A\\), i.e., the sparsity pattern of \\(A\\). If \\(A_{i j}=0\\), then input neuron \\(j\\) does not affect the output neuron \\(i\\). If \\(A_{i j} \\neq 0\\), then \\(\\left(A^{\\top}\\right)_{j i} \\neq 0\\). So if input neuron \\(j\\) affects output neuron \\(i\\) in convolution, then input neuron \\(i\\) affects output neuron \\(j\\) in transposed convolution.</p>  ![](.././assets/8.17.png){: width=\"50%\"}  <p>We can combine this reasoning with our visual understanding of convolution. The diagram simultaneously illustrates the dependencies for both convolution and transposed convolution.</p>"},{"location":"books-and-courses/mfdnn/format/8/#upsampling","title":"Upsampling","text":"<p>Concept 8.17 : Upsampling</p> <p><code>torch.nn.Upsample</code> with <code>mode='nearest'</code></p> <p> </p> <p>Concept 8.18 : Upsampling</p> <p><code>Torch.nn.Upsample</code> with <code>mode='bilinear'</code> <code>linear</code> interpolation is available for 1D data <code>trilinear</code> interpolation is available for 3D data (We won't pay attention to the interpolation formula.)</p> <p> </p>"},{"location":"books-and-courses/mfdnn/format/8/#semantic-segmentation","title":"Semantic Segmentation","text":"<p>Definition 8.19 : Semantic Segmentation</p> <p>In semantic segmentation, the goal is to segment the image into semantically meaningful regions by classifying each pixel.</p> <p> </p> <p>Definition 8.20 : Object Localization</p> <p>Object localization localizes a single object usually via a bounding box.</p> <p> </p> <p>Definition 8.21 : Object Detection</p> <p>Object detection detects many objects, with the same class often repeated, usually via bounding boxes.</p> <p> </p> <p>Definition 8.22 : Image Segmentation</p> <p>Instance segmentation distinguishes multiple instances of the same object type.</p> <p> </p> <p>We will focus on semantic segmentation.</p> <p>Definition 8.23 : Pascal VOC</p> <p>We will use PASCAL Visual Object Classes (VOC) dataset for semantic segmentation. (Dataset also contains labels for object detection.)</p> <p>There are 21 classes: 20 main classes and 1 \"unlabeled\" class.</p> <p>Data \\(X_{1}, \\ldots, X_{N} \\in \\mathbb{R}^{3 \\times m \\times n}\\) and labels \\(Y_{1}, \\ldots, Y_{N} \\in\\{0,1, \\ldots, 20\\}^{m \\times n}\\), i.e., \\(Y_{i}\\) provides a class label for every pixel of \\(X_{i}\\).</p> <p> </p> <p>Concept 8.24 : Loss for Semantic Segmentation</p> <p>Consider the neural network</p> \\[ f_{\\theta}: \\mathbb{R}^{3 \\times m \\times n} \\rightarrow \\mathbb{R}^{k \\times m \\times n} \\] <p>such that \\(\\mu\\left(f_{\\theta}(X)\\right)_{i j} \\in \\Delta^{k}\\) is the probabilities for the \\(k\\) classes for pixel \\((i, j)\\).</p> <p>We minimize the sum of pixel-wise cross-entropy losses</p> \\[ \\mathcal{L}(\\theta)=\\sum_{l=1}^{N} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\ell^{\\mathrm{CE}}\\left(f_{\\theta}\\left(X_{l}\\right)_{i j},\\left(Y_{l}\\right)_{i j}\\right) \\] <p>where \\(\\ell^{C E}\\) is the cross entropy loss.</p> <p>Definition 8.25 : U-Net</p> <p>The U-Net architecture:</p> <ul> <li>Reduce the spatial dimension to obtain high-level (coarse scale) features</li> <li>Upsample or transpose convolution to restore spatial dimension.</li> <li>Use residual connections across each dimension reduction stage.</li> </ul> <p> </p> <p>Definition 8.26 : Magnetic Resonance Imaging (MRI)</p> <p>Magnetic resonance imaging (MRI) is an inverse problem in which we partially measure the Fourier transform of the patient and the goal is to reconstruct the patient's image.</p> <p>So \\(X_{\\text {true }} \\in \\mathbb{R}^{n}\\) is the true original image (reshaped into a vector) with \\(n\\) pixels or voxels and \\(\\mathcal{A}\\left[X_{\\text {true }}\\right] \\in \\mathbb{C}^{k}\\) with \\(k \\ll n\\). (If \\(k=n\\), MRI scan can take hours.)</p> <p>Classical reconstruction algorithms rely on Fourier analysis, total variation regularization, compressed sensing, and optimization.</p> <p>Recent state-of-the-art use deep neural networks.</p> <p>Definition 8.27 : FastMRI Dataset</p> <p>A team of researchers from Facebook AI Research and NYU released a large MRI dataset to stimulate datadriven deep learning research for MRI reconstruction.</p> <p> </p> <p>(J. Zbontar, F. Knoll, A. Sriram, T. Murrell, Z. Huang, M. J. Muckley, A. Defazio, R. Stern, P. Johnson, M. Bruno, M. Parente, K. J. Geras, J. Katsnelson, H. Chandarana, Z. Zhang, M. Drozdzal, A. Romero, M. Rabbat, P. Vincent, N. Yakubova, J. Pinkerton, D. Wang, E. Owens, C. L. Zitnick, M. P. Recht, D. K. Sodickson, and Y. W. Lui, fastMRI: An open dataset and benchmarks for accelerated MRI, arXiv, 2019.)</p> <p>Definition 8.28 : Computational Tomography (CT)</p> <p>Computational tomography (CT) is an inverse problem in which we partially measure the Radon transform of the patient and the goal is to reconstruct the patient's image.</p> <p>So \\(X_{\\text {true }} \\in \\mathbb{R}^{n}\\) is the true original image (reshaped into a vector) with \\(n\\) pixels or voxels and \\(\\mathcal{A}\\left[X_{\\text {true }}\\right] \\in \\mathbb{R}^{k}\\) with \\(k \\ll n\\). (If \\(k=n\\), the X -ray exposure to perform the CT scan can be harmful.)</p> <p>Recent state-of-the-art use deep neural networks.</p> <p>Concept 8.29 : U-Net is used for inverse problems.</p> <p>Although U-Net was originally proposed as an architecture for semantic segmentation, it is also being used widely as one of the default architectures in inverse problems, including MRI reconstruction.</p> <p> </p> <p>(J. Zbontar, F. Knoll, A. Sriram, T. Murrell, Z. Huang, M. J. Muckley, A. Defazio, R. Stern, P. Johnson, M. Bruno, M. Parente, K. J. Geras, J. Katsnelson, H. Chandarana, Z. Zhang, M. Drozdzal, A. Romero, M. Rabbat, P. Vincent, N. Yakubova, J. Pinkerton, D. Wang, E. Owens, C. L. Zitnick, M. P. Recht, D. K. Sodickson, and Y. W. Lui, fastMRI: An open dataset and benchmarks for accelerated MRI, arXiv, 2019.)</p> <p>U-Net is also used as one of the default architectures in CT reconstruction.</p> <p> </p> <p> </p> <p>(K. H. Jin, M. T. McCann, E. Froustey, and M. Unser, Deep convolutional neural network for inverse problems in imaging, IEEE TIP, 2017.)</p>"},{"location":"books-and-courses/mfdnn/format/9/","title":"\u00a7 9. Autoencoder","text":""},{"location":"books-and-courses/mfdnn/format/9/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>Definition 9.1 : Unsupervised Learning</p> <p>Unsupervised learning utilizes data \\(X_{1}, \\ldots, X_{N}\\) to learn the \"structure\" of the data. No labels are utilized.</p> <p>There are a wide range of unsupervised learning tasks. In this class, we discuss just a few.</p> <p>Generally, unsupervised learning tasks tend to have more mathematical complexity.</p> <p>Concept 9.2 : Many data has low-dimensional latent representation, and the task is to find it.</p> <p>Many high-dimensional data has some underlying low-dimensional structure. (One can model this assumption as data residing in a low dimensional manifold and utilize ideas from differential geometry. We won\u2019t pursue this direction)</p> <p>If you randomly generate the pixels of a color image \\(X \\in \\mathbb{R}^{3 \\times m \\times n}\\), it will likely make no sense. Only a very small subset of pixel values correspond to meaningful images.</p> <p>In machine learning, especially in unsupervised learning, finding a \"meaningful\" low dimensional latent representation is of interest.</p> <p>A good lower-dimensional representation of the data implies you have a good understanding of the data.</p>"},{"location":"books-and-courses/mfdnn/format/9/#definition-of-autoencoder","title":"Definition of Autoencoder","text":"<p>Definition 9.3 : Autoencoder</p> <p>An autoencoder (AE) has encoder \\(E_{\\theta}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{r}\\) and decoder \\(D_{\\varphi}: \\mathbb{R}^{r} \\rightarrow \\mathbb{R}^{n}\\) networks, where \\(r \\ll n\\). (If \\(r \\geq n\\), AE learns identity mapping, so pointless.) The two networks are trained through the loss</p> \\[ \\mathcal{L}(\\theta, \\varphi)=\\sum_{i=1}^{N}\\left\\|X_{i}-D_{\\varphi}\\left(E_{\\theta}\\left(X_{i}\\right)\\right)\\right\\|^{2} \\] <p>The low-dimensional output \\(E_{\\theta}(X)\\) is the latent vector. The encoder performs dimensionality reduction.</p> <p>The autoencoder can be thought of as a deep non-linear generalization of the principle component analysis (PCA).</p> <p> </p> <p>(G. E. Hinton and R. R. Salakhutdinov, Reducing the dimensionality of data with neural networks, Science, 2006.)</p>"},{"location":"books-and-courses/mfdnn/format/9/#applications-of-autoencoder","title":"Applications of Autoencoder","text":"<p>Concept 9.4 : Applications of AE</p> <p>Autoencoders can be used to denoise or reconstruct corrupted images.</p> <p> </p> <p> </p> <p>(P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol, Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion, JMLR, 2010. G. Nishad, Reconstruct corrupted data using Denoising Autoencoder, Medium, 2020.)</p> <p>Concept 9.5 : Applications of AE</p> <p>Once an AE has been trained, storing the latent variable representation, rather than the original image can be used as a compression mechanism.</p> <p>More generally, latent variable representations can be used for video compression. (link)</p> <p>Concept 9.6 : Applications of AE</p> <p>Train an AE and then perform clustering on the latent variables. For the clustering algorithm, one can use things like k-means, which groups together.</p> <p>Clustering is also referred to as unsupervised classification. Without labels, we want the group \"similar\" data.</p> <p> </p> <p> </p> <p>(J. Xie, R. Girshick, and A. Farhadi, Unsupervised deep embedding for clustering analysis, ICML, 2016.)</p> <p>Concept 9.7 : Anomaly/Outlier Detection</p> <p>Problem: detecting data that is significantly different from the data seen during training.</p> <p>Insight: AE should not be able to faithfully reconstruct novel data.</p> <p>Solution: Train an AE and define the score function to be the reconstruction loss:</p> \\[ s(X)=\\left\\|X-D_{\\varphi}\\left(E_{\\theta}(X)\\right)\\right\\|^{2} \\] <p>If score is high, determine the datapoint to be an outliner.</p> <p>(S. Hawkins, H. He, G. Williams, and R. Baxter, Outlier detection using replicator neural networks, DaWaK, 2002.)</p>"}]}